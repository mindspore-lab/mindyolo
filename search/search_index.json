{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MindYOLO","text":"<p>MindYOLO implements state-of-the-art YOLO series algorithms based on MindSpore. The following is the corresponding <code>mindyolo</code> versions and supported <code>mindspore</code> versions.</p> mindyolo mindspore master master 0.5 2.5.0 0.4 2.3.0/2.3.1 0.3 2.2.10 0.2 2.0 0.1 1.8 <p></p>"},{"location":"#benchmark-and-model-zoo","title":"Benchmark and Model Zoo","text":"<p>See Benchmark Results.</p>"},{"location":"#supported-model-list","title":"supported model list","text":"<ul> <li> YOLOv10 (welcome to contribute)</li> <li> YOLOv9 (welcome to contribute)</li> <li> YOLOv8</li> <li> YOLOv7</li> <li> YOLOX</li> <li> YOLOv5</li> <li> YOLOv4</li> <li> YOLOv3</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>See INSTALLATION for details.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>See QUICK START for details.</p>"},{"location":"#notes","title":"Notes","text":"<p>\u26a0\ufe0f The current version is based on the static shape of GRAPH.  The dynamic shape will be supported later. Please look forward to it.</p>"},{"location":"#how-to-contribute","title":"How to Contribute","text":"<p>We appreciate all contributions including issues and PRs to make MindYOLO better. </p> <p>Please refer to CONTRIBUTING for the contributing guideline.</p>"},{"location":"#license","title":"License","text":"<p>MindYOLO is released under the Apache License 2.0.</p>"},{"location":"#acknowledgement","title":"Acknowledgement","text":"<p>MindYOLO is an open source project that welcome any contribution and feedback. We wish that the toolbox and benchmark could support the growing research community, reimplement existing methods, and develop their own new real-time object detection methods by providing a flexible and standardized toolkit.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you find this project useful in your research, please consider cite:</p> <pre><code>@misc{MindSpore Object Detection YOLO 2023,\n    title={{MindSpore Object Detection YOLO}:MindSpore Object Detection YOLO Toolbox and Benchmark},\n    author={MindSpore YOLO Contributors},\n    howpublished = {\\url{https://github.com/mindspore-lab/mindyolo}},\n    year={2023}\n}\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#dependency","title":"Dependency","text":"<ul> <li>mindspore &gt;= 2.3</li> <li>numpy &gt;= 1.17.0</li> <li>pyyaml &gt;= 5.3</li> <li>openmpi 4.0.3 (for distributed mode)</li> </ul> <p>To install the dependency, please run</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>MindSpore can be easily installed by following the official instructions where you can select your hardware platform for the best fit. To run in distributed mode, openmpi is required to install.</p> <p>\u26a0\ufe0f The current version only supports the Ascend platform, and the GPU platform will be supported later.</p>"},{"location":"installation/#install-with-pypi","title":"Install with PyPI","text":"<p>MindYOLO is published as a <code>Python package</code> and can be installed with <code>pip</code>, ideally by using a <code>virtual environment</code>. Open up a terminal and install MindYOLO with:</p> <pre><code>pip install mindyolo\n</code></pre>"},{"location":"installation/#install-from-source-bleeding-edge-version","title":"Install from Source (Bleeding Edge Version)","text":""},{"location":"installation/#from-vcs","title":"from VCS","text":"<pre><code>pip install git+https://github.com/mindspore-lab/mindyolo.git\n</code></pre>"},{"location":"installation/#from-local-src","title":"from local src","text":"<p>As this project is in active development, if you are a developer or contributor, please prefer this installation!</p> <p>MindYOLO can be directly used from <code>GitHub</code> by cloning the repository into a local folder which might be useful if you want to use the very latest version:</p> <pre><code>git clone https://github.com/mindspore-lab/mindyolo.git\n</code></pre> <p>After cloning from <code>git</code>, it is recommended that you install using \"editable\" mode, which can help resolve potential module import issues:</p> <pre><code>cd mindyolo\npip install -e .\n</code></pre> <p>In addition, we provide an optional fast coco api to improve eval speed. The code is provided in C++, and you can try compiling with the following commands (This operation is optional) :</p> <pre><code>cd mindyolo/csrc\nsh build.sh\n</code></pre> <p>We also provide fused GPU operators which are built upon MindSpore ops.Custom API. The fused GPU operators are able to improve train speed. The source code is provided in C++ and CUDA and is in the folder <code>examples/custom_gpu_op/</code>. To enable this feature in the GPU training process, you shall modify the method <code>bbox_iou</code> in <code>mindyolo/models/losses/iou_loss.py</code> by referring to the demo script <code>examples/custom_gpu_op/iou_loss_fused.py</code>. Before runing <code>iou_loss_fused.py</code>, you shall compile the C++ and CUDA source code to dynamic link libraries with the following commands (This operation is optional) :</p> <pre><code>bash examples/custom_gpu_op/fused_op/build.sh\n</code></pre>"},{"location":"how_to_guides/callback/","title":"Usage of MindYOLO callback function","text":"<p>Callback function: When the program runs to a certain mount point, all methods registered to the mount point at runtime will be automatically called. The flexibility and extensibility of the program can be increased by using the callback function, because users can register custom methods to the mount point to be called without modifying the code in the program.</p> <p>In MindYOLO, the callback function is specifically implemented in the mindyolo/utils/callback.py file. <pre><code>#mindyolo/utils/callback.py\n@CALLBACK_REGISTRY.registry_module()\nclass callback_class_name(BaseCallback):\n    def __init__(self, **kwargs):\n        super().__init__()\n...\ndef callback_fn_name(self, run_context: RunContext):\n    pass\n</code></pre></p> <p>Add a dictionary list under the callback field of the model's yaml file to implement the call <pre><code>#Callback function configuration dictionary:\ncallback:\n- { name: callback_class_name, args: xx }\n- { name: callback_class_name2, args: xx }\n</code></pre> For example, take YOLOX as an example:</p> <p>Add logic to the on_train_step_begin method in the YoloxSwitchTrain class in the mindyolo/utils/callback.py file to print \"train step begin\u201d log <pre><code>@CALLBACK_REGISTRY.registry_module()\nclass YoloxSwitchTrain(BaseCallback):\n\n    def on_train_step_begin(self, run_context: RunContext):\n        # Custom logic\n        logger.info(\"train step begin\")\n        pass\n</code></pre> Add the callback function under the callback field of the YOLOX corresponding yaml file configs/yolox/hyp.scratch.yaml <pre><code>callback:\n- { name: YoloxSwitchTrain, switch_epoch_num: 285 }\n</code></pre> Then the logger.info(\"train step begin\") statement will be executed before each training step is executed.</p> <p>With the help of the callback function, users can customize the logic that needs to be executed at a certain mount point without having to understand the code of the complete training process.</p>"},{"location":"how_to_guides/data_preparation/","title":"Data preparation","text":""},{"location":"how_to_guides/data_preparation/#dataset-format-introduction","title":"Dataset format introduction","text":"<p>Download coco2017 YOLO format coco2017labels-segments and coco2017 original images train2017 , val2017 , then put the coco2017 original images into the coco2017 YOLO format images directory: <pre><code>\u2514\u2500 coco2017_yolo\n    \u251c\u2500 annotations\n        \u2514\u2500 instances_val2017.json\n    \u251c\u2500 images\n        \u251c\u2500 train2017   # coco2017 \u539f\u59cb\u56fe\u7247\n        \u2514\u2500 val2017     # coco2017 \u539f\u59cb\u56fe\u7247\n    \u251c\u2500 labels\n        \u251c\u2500 train2017\n        \u2514\u2500 val2017\n    \u251c\u2500 train2017.txt\n    \u251c\u2500 val2017.txt\n    \u2514\u2500 test-dev2017.txt\n</code></pre> Each line of the train.txt file corresponds to the relative path of a single image, for example: <pre><code>./images/train2017/00000000.jpg\n./images/train2017/00000001.jpg\n./images/train2017/00000002.jpg\n./images/train2017/00000003.jpg\n./images/train2017/00000004.jpg\n./images/train2017/00000005.jpg\n</code></pre> The txt files in the train2017 folder under labels are the annotation information of the corresponding images, supporting both detect and segment formats.</p> <p>Detect format: Usually each row has 5 columns, corresponding to the category id and the center coordinates xy and width and height wh after normalization of the annotation box <pre><code>62 0.417040 0.206280 0.403600 0.412560\n62 0.818810 0.197933 0.174740 0.189680\n39 0.684540 0.277773 0.086240 0.358960\n0 0.620220 0.725853 0.751680 0.525840\n63 0.197190 0.364053 0.394380 0.669653\n39 0.932330 0.226240 0.034820 0.076640\n</code></pre> segment format: the first data in each line is the category id, followed by pairs of normalized coordinate points x, y</p> <p><pre><code>45 0.782016 0.986521 0.937078 0.874167 0.957297 0.782021 0.950562 0.739333 0.825844 0.561792 0.714609 0.420229 0.657297 0.391021 0.608422 0.4 0.0303438 0.750562 0.0016875 0.811229 0.003375 0.889896 0.0320156 0.986521\n45 0.557859 0.143813 0.487078 0.0314583 0.859547 0.00897917 0.985953 0.130333 0.984266 0.184271 0.930344 0.386521 0.80225 0.480896 0.763484 0.485396 0.684266 0.39775 0.670781 0.3955 0.679219 0.310104 0.642141 0.253937 0.561234 0.155063 0.559547 0.137083\n50 0.39 0.727063 0.418234 0.649417 0.455297 0.614125 0.476469 0.614125 0.51 0.590583 0.54 0.569417 0.575297 0.562354 0.601766 0.56 0.607062 0.536479 0.614125 0.522354 0.637063 0.501167 0.665297 0.48 0.69 0.477646 0.698828 0.494125 0.698828 0.534125 0.712938 0.529417 0.742938 0.548229 0.760594 0.564708 0.774703 0.550583 0.778234 0.536479 0.781766 0.531771 0.792359 0.541167 0.802937 0.555292 0.802937 0.569417 0.802937 0.576479 0.822359 0.576479 0.822359 0.597646 0.811766 0.607062 0.811766 0.618833 0.818828 0.637646 0.820594 0.656479 0.827641 0.687063 0.827641 0.703521 0.829406 0.727063 0.838234 0.708229 0.852359 0.729417 0.868234 0.750583 0.871766 0.792938 0.877063 0.821167 0.884125 0.861167 0.817062 0.92 0.734125 0.976479 0.711172 0.988229 0.48 0.988229 0.494125 0.967063 0.517062 0.912937 0.508234 0.832937 0.485297 0.788229 0.471172 0.774125 0.395297 0.729417\n45 0.375219 0.0678333 0.375219 0.0590833 0.386828 0.0503542 0.424156 0.0315208 0.440797 0.0281458 0.464 0.0389167 0.525531 0.115583 0.611797 0.222521 0.676359 0.306583 0.678875 0.317354 0.677359 0.385271 0.66475 0.394687 0.588594 0.407458 0.417094 0.517771 0.280906 0.604521 0.0806562 0.722208 0.0256719 0.763917 0.00296875 0.809646 0 0.786104 0 0.745083 0 0.612583 0.03525 0.613271 0.0877187 0.626708 0.130594 0.626708 0.170437 0.6025 0.273844 0.548708 0.338906 0.507 0.509906 0.4115 0.604734 0.359042 0.596156 0.338188 0.595141 0.306583 0.595141 0.291792 0.579516 0.213104 0.516969 0.129042 0.498297 0.100792 0.466516 0.0987708 0.448875 0.0786042 0.405484 0.0705208 0.375219 0.0678333 0.28675 0.108375 0.282719 0.123167 0.267078 0.162854 0.266062 0.189083 0.245391 0.199833 0.203516 0.251625 0.187375 0.269771 0.159641 0.240188 0.101125 0.249604 0 0.287271 0 0.250271 0 0.245563 0.0975938 0.202521 0.203516 0.145354 0.251953 0.123167 0.28675 0.108375\n49 0.587812 0.128229 0.612281 0.0965625 0.663391 0.0840833 0.690031 0.0908125 0.700109 0.10425 0.705859 0.133042 0.700109 0.143604 0.686422 0.146479 0.664828 0.153188 0.644672 0.157042 0.629563 0.175271 0.605797 0.181021 0.595 0.147437\n49 0.7405 0.178417 0.733719 0.173896 0.727781 0.162583 0.729484 0.150167 0.738812 0.124146 0.747281 0.0981458 0.776109 0.0811875 0.804094 0.0845833 0.814266 0.102667 0.818516 0.115104 0.812578 0.133208 0.782906 0.151292 0.754063 0.172771\n49 0.602656 0.178854 0.636125 0.167875 0.655172 0.165125 0.6665 0.162375 0.680391 0.155521 0.691719 0.153458 0.703047 0.154146 0.713859 0.162375 0.724156 0.174729 0.730844 0.193271 0.733422 0.217979 0.733938 0.244063 0.733422 0.281813 0.732391 0.295542 0.728266 0.300354 0.702016 0.294854 0.682969 0.28525 0.672156 0.270146\n49 0.716891 0.0519583 0.683766 0.0103958 0.611688 0.0051875 0.568828 0.116875 0.590266 0.15325 0.590266 0.116875 0.613641 0.0857083 0.631172 0.0857083 0.6565 0.083125 0.679875 0.0883125 0.691563 0.0961042 0.711031 0.0649375\n</code></pre> instances_val2017.json is the verification set annotation in coco format, which can directly call coco api for map calculation.</p> <p>During training &amp; reasoning, you need to modify <code>train_set</code>, <code>val_set</code>, <code>test_set</code> in <code>configs/coco.yaml</code> to the actual data path</p> <p>For actual examples of using MindYOLO kit to complete custom dataset finetune, please refer to Finetune</p>"},{"location":"how_to_guides/write_a_new_model/","title":"Model Writing Guide","text":"<p>This document provides a tutorial for writing custom models for MindYOLO.  It is divided into three parts:</p> <ul> <li>Model definition: We can define a network directly or use a yaml file to define a network.</li> <li>Register model: Optional. After registration, you can use the file name in the create_model interface to create a custom model</li> <li>Verification: Verify whether the model is operational</li> </ul>"},{"location":"how_to_guides/write_a_new_model/#model-definition","title":"Model definition","text":""},{"location":"how_to_guides/write_a_new_model/#1-use-python-code-directly-to-write-the-network","title":"1. Use python code directly to write the network","text":""},{"location":"how_to_guides/write_a_new_model/#module-import","title":"Module import","text":"<p>Import the nn module and ops module in the MindSpore framework to define the components and operations of the neural network. <pre><code>import mindspore.nn as nn\nimport mindspore.ops.operations as ops\n</code></pre></p>"},{"location":"how_to_guides/write_a_new_model/#create-a-model","title":"Create a model","text":"<p>Define a model class MyModel that inherits from nn.Cell. In the constructor init, define the various components of the model:</p> <pre><code>class MyModel(nn.Cell):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        #conv1\u662f\u4e00\u4e2a2D\u5377\u79ef\u5c42\uff0c\u8f93\u5165\u901a\u9053\u6570\u4e3a3\uff0c\u8f93\u51fa\u901a\u9053\u6570\u4e3a16\uff0c\u5377\u79ef\u6838\u5927\u5c0f\u4e3a3x3\uff0c\u6b65\u957f\u4e3a1\uff0c\u586b\u5145\u4e3a1\u3002\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n        #relu\u662f\u4e00\u4e2aReLU\u6fc0\u6d3b\u51fd\u6570\u64cd\u4f5c\u3002\n        self.relu = ops.ReLU()\n        #axpool\u662f\u4e00\u4e2a2D\u6700\u5927\u6c60\u5316\u5c42\uff0c\u6c60\u5316\u7a97\u53e3\u5927\u5c0f\u4e3a2x2\uff0c\u6b65\u957f\u4e3a2\u3002\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n        #conv2\u662f\u53e6\u4e00\u4e2a2D\u5377\u79ef\u5c42\uff0c\u8f93\u5165\u901a\u9053\u6570\u4e3a16\uff0c\u8f93\u51fa\u901a\u9053\u6570\u4e3a32\uff0c\u5377\u79ef\u6838\u5927\u5c0f\u4e3a3x3\uff0c\u6b65\u957f\u4e3a1\uff0c\u586b\u5145\u4e3a1\u3002\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n        #fc\u662f\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\uff0c\u8f93\u5165\u7279\u5f81\u7ef4\u5ea6\u4e3a32x8x8\uff0c\u8f93\u51fa\u7279\u5f81\u7ef4\u5ea6\u4e3a10\u3002\n        self.fc = nn.Dense(32 * 8 * 8, 10)\n\n    #\u5728construct\u65b9\u6cd5\u4e2d\uff0c\u5b9a\u4e49\u4e86\u6a21\u578b\u7684\u524d\u5411\u4f20\u64ad\u8fc7\u7a0b\u3002\u8f93\u5165x\u7ecf\u8fc7\u5377\u79ef\u3001\u6fc0\u6d3b\u51fd\u6570\u3001\u6c60\u5316\u7b49\u64cd\u4f5c\u540e\uff0c\u901a\u8fc7\u5c55\u5e73\u64cd\u4f5c\u5c06\u7279\u5f81\u5f20\u91cf\u53d8\u4e3a\u4e00\u7ef4\u5411\u91cf\uff0c\u7136\u540e\u901a\u8fc7\u5168\u8fde\u63a5\u5c42\u5f97\u5230\u6700\u7ec8\u7684\u8f93\u51fa\u7ed3\u679c\u3002    \n    def construct(self, x): \n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.conv2(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = x.view(x.shape[0], -1)\n        x = self.fc(x)\n        return x\n</code></pre>"},{"location":"how_to_guides/write_a_new_model/#create-a-model-instance","title":"Create a model instance","text":"<p>By instantiating the MyModel class, create a model instance model, which can be used for model training and reasoning later. <pre><code>model = MyModel()\n</code></pre></p>"},{"location":"how_to_guides/write_a_new_model/#2-use-yaml-file-to-write-network","title":"2. Use yaml file to write network","text":"<p>Usually need the following three steps:</p> <ul> <li>Create a new mymodel.yaml file</li> <li>Create a corresponding mymodel.py file</li> <li>Introduce the model in the mindyolo/models/init.py file</li> </ul> <p>Here is a detailed guide to writing mymodel.yaml file: Take writing a simple network as an example: Write the necessary parameters in yaml format, and then use these parameters in the mymodel.py file. The network part is the model network  [[from, number, module, args], ...]: Each element represents the configuration of a network layer.  <pre><code># The yaml in __BASE__ indicates the base configuration file for inheritance. Repeated parameters will be overwritten by the current file;\n__BASE__:\n-'../coco.yaml'\n-'./hyp.scratch-high.yaml'\n\nper_batch_size: 32\nimg_size: 640\nsync_bn: False\n\nnetwork:\nmodel_name: mymodel\ndepth_multiple: 1.0 # model depth multiple\nwidth_multiple: 1.0 # layer channel multiple\nstride: [ 8, 16, 32 ]\n\n# Configuration of the backbone network. The meaning of each layer is\n# [from, number, module, args]\n# Take the first layer as an example, [-1, 1, ConvNormAct, [32, 3, 1]], which means the input comes from `-1` (the previous layer), the number of repetitions is 1, and the module name is ConvNormAct, module input parameters are [32, 3, 1];\nbackbone:\n[[-1, 1, ConvNormAct, [32, 3, 1]], # 0\n[-1, 1, ConvNormAct, [64, 3, 2]], # 1-P1/2\n[-1, 1, Bottleneck, [64]],\n[-1, 1, ConvNormAct, [128, 3, 2]], # 3-P2/4\n[-1, 2, Bottleneck, [128]],\n[-1, 1, ConvNormAct, [256, 3, 2]], # 5-P3/8\n[-1, 8, Bottleneck, [256]],\n]\n\n#head part configuration\nhead:\n[\n[ -1, 1, ConvNormAct, [ 512, 3, 2 ] ], # 7-P4/16\n[ -1, 8, Bottleneck, [ 512 ] ],\n[ -1, 1, ConvNormAct, [ 1024, 3, 2 ] ], # 9-P5/32\n[ -1, 4, Bottleneck, [ 1024 ] ], # 10\n]\n</code></pre></p> <p>Write mymodel.py file:</p>"},{"location":"how_to_guides/write_a_new_model/#module-import_1","title":"Module import","text":"<p>It is necessary to import modules in the package. For example, <code>from .registry import register_model</code>, etc.</p> <pre><code>import numpy as np\n\nimport mindspore as ms\nfrom mindspore import Tensor, nn\n\nfrom .initializer import initialize_defult #Used to initialize the default parameters of the model, including weight initialization method, BN layer parameters, etc.\nfrom .model_factory import build_model_from_cfg #Used to build a target detection model according to the parameters in the YAML configuration file and return an instance of the model.\nfrom .registry import register_model #Used to register a custom model in Mindyolo for use in the YAML configuration file.\n\n#Visibility declaration\n__all__ = [\"MYmodel\", \"mymodel\"]\n</code></pre>"},{"location":"how_to_guides/write_a_new_model/#create-a-configuration-dictionary","title":"Create a configuration dictionary","text":"<p>The _cfg function is an auxiliary function used to create a configuration dictionary. It accepts a url parameter and other keyword parameters and returns a dictionary containing the url and other parameters.  default_cfgs is a dictionary used to store default configurations. Here, mymodel is used as the key to create a configuration dictionary using the _cfg function. <pre><code>def _cfg(url=\"\", **kwargs):\n    return {\"url\": url, **kwargs}\n\ndefault_cfgs = {\"mymodel\": _cfg(url=\"\")}\n</code></pre></p>"},{"location":"how_to_guides/write_a_new_model/#create-a-model_1","title":"Create a model","text":"<p>In <code>MindSpore</code>, the model class inherits from <code>nn.Cell</code>. Generally, the following two functions need to be overloaded:</p> <ul> <li>In the <code>__init__</code> function, the module layer needed in the model should be defined.</li> <li>In the <code>construct</code> function, define the model forward logic. </li> </ul> <pre><code>class MYmodel(nn.Cell):\n\n    def __init__(self, cfg, in_channels=3, num_classes=None, sync_bn=False):\n        super(MYmodel, self).__init__()\n        self.cfg = cfg\n        self.stride = Tensor(np.array(cfg.stride), ms.int32)\n        self.stride_max = int(max(self.cfg.stride))\n        ch, nc = in_channels, num_classes\n\n        self.nc = nc  # override yaml value\n        self.model = build_model_from_cfg(model_cfg=cfg, in_channels=ch, num_classes=nc, sync_bn=sync_bn)\n        self.names = [str(i) for i in range(nc)]  # default names\n\n        initialize_defult()  # \u53ef\u9009\uff0c\u4f60\u53ef\u80fd\u9700\u8981initialize_defult\u65b9\u6cd5\u4ee5\u83b7\u5f97\u548cpytorch\u4e00\u6837\u7684conv2d\u3001dense\u5c42\u7684\u521d\u59cb\u5316\u65b9\u5f0f\uff1b\n\n    def construct(self, x):\n        return self.model(x)\n</code></pre>"},{"location":"how_to_guides/write_a_new_model/#register-model-optional","title":"Register model (optional)","text":"<p>If you need to use the mindyolo interface to initialize a custom model, you need to first register and import the model</p> <p>Model registration <pre><code>@register_model #The registered model can be accessed by the create_model interface as a model name;\ndef mymodel(cfg, in_channels=3, num_classes=None, **kwargs) -&gt; MYmodel:\n\"\"\"Get GoogLeNet model.\n    Refer to the base class `models.GoogLeNet` for more details.\"\"\"\n    model = MYmodel(cfg=cfg, in_channels=in_channels, num_classes=num_classes, **kwargs)\n    return model\n</code></pre> Model import </p> <pre><code>#Add the following code to the mindyolo/models/_init_.py file\n\nfrom . import mymodel #mymodel.py files are usually placed in the mindyolo/models/directory\n__all__.extend(mymodel.__all__)\nfrom .mymodel import *\n</code></pre>"},{"location":"how_to_guides/write_a_new_model/#verify-main","title":"Verify main","text":"<p>The initial writing phase should ensure that the model is runnable. Basic verification can be performed through the following code block: First import the required modules and functions. Then, parse the configuration object.</p> <p><pre><code>if __name__ == \"__main__\":\n    from mindyolo.models.model_factory import create_model\n    from mindyolo.utils.config import parse_config\n\n    opt = parse_config()\n</code></pre> Create a model and specify related parameters. Note: If you want to use the file name to create a custom model in create_model, you need to register it using the register @register_model first. Please refer to the above Register model (optional) section <pre><code>    model = create_model(\n        model_name=\"mymodel\",\n        model_cfg=opt.net,\n        num_classes=opt.data.nc,\n        sync_bn=opt.sync_bn if hasattr(opt, \"sync_bn\") else False,\n    )\n</code></pre></p> <p>Otherwise, please use import to introduce the model</p> <p><pre><code>    from mindyolo.models.mymodel import MYmodel\n    model = MYmodel(\n        model_name=\"mymodel\",\n        model_cfg=opt.net,\n        num_classes=opt.data.nc,\n        sync_bn=opt.sync_bn if hasattr(opt, \"sync_bn\") else False,\n    ) \n</code></pre> Finally, create an input tensor x and pass it to the model for forward computation. <pre><code>    x = Tensor(np.random.randn(1, 3, 640, 640), ms.float32)\n    out = model(x)\n    out = out[0] if isinstance(out, (list, tuple)) else out\n    print(f\"Output shape is {[o.shape for o in out]}\")\n</code></pre></p>"},{"location":"modelzoo/benchmark/","title":"Benchmark","text":""},{"location":"modelzoo/benchmark/#detection","title":"Detection","text":"performance tested on Ascend 910(8p) with graph mode Name Scale BatchSize ImageSize Dataset Box mAP (%) Params Recipe Download YOLOv8 N 16 * 8 640 MS COCO 2017 37.2 3.2M yaml weights YOLOv8 S 16 * 8 640 MS COCO 2017 44.6 11.2M yaml weights YOLOv8 M 16 * 8 640 MS COCO 2017 50.5 25.9M yaml weights YOLOv8 L 16 * 8 640 MS COCO 2017 52.8 43.7M yaml weights YOLOv8 X 16 * 8 640 MS COCO 2017 53.7 68.2M yaml weights YOLOv7 Tiny 16 * 8 640 MS COCO 2017 37.5 6.2M yaml weights YOLOv7 L 16 * 8 640 MS COCO 2017 50.8 36.9M yaml weights YOLOv7 X 12 * 8 640 MS COCO 2017 52.4 71.3M yaml weights YOLOv5 N 32 * 8 640 MS COCO 2017 27.3 1.9M yaml weights YOLOv5 S 32 * 8 640 MS COCO 2017 37.6 7.2M yaml weights YOLOv5 M 32 * 8 640 MS COCO 2017 44.9 21.2M yaml weights YOLOv5 L 32 * 8 640 MS COCO 2017 48.5 46.5M yaml weights YOLOv5 X 16 * 8 640 MS COCO 2017 50.5 86.7M yaml weights YOLOv4 CSPDarknet53 16 * 8 608 MS COCO 2017 45.4 27.6M yaml weights YOLOv4 CSPDarknet53(silu) 16 * 8 608 MS COCO 2017 45.8 27.6M yaml weights YOLOv3 Darknet53 16 * 8 640 MS COCO 2017 45.5 61.9M yaml weights YOLOX N 8 * 8 416 MS COCO 2017 24.1 0.9M yaml weights YOLOX Tiny 8 * 8 416 MS COCO 2017 33.3 5.1M yaml weights YOLOX S 8 * 8 640 MS COCO 2017 40.7 9.0M yaml weights YOLOX M 8 * 8 640 MS COCO 2017 46.7 25.3M yaml weights YOLOX L 8 * 8 640 MS COCO 2017 49.2 54.2M yaml weights YOLOX X 8 * 8 640 MS COCO 2017 51.6 99.1M yaml weights YOLOX Darknet53 8 * 8 640 MS COCO 2017 47.7 63.7M yaml weights performance tested on Ascend 910*(8p) Name Scale BatchSize ImageSize Dataset Box mAP (%) ms/step Params Recipe Download YOLOv8 N 16 * 8 640 MS COCO 2017 37.3 373.55 3.2M yaml weights YOLOv8 S 16 * 8 640 MS COCO 2017 44.7 365.53 11.2M yaml weights YOLOv7 Tiny 16 * 8 640 MS COCO 2017 37.5 496.21 6.2M yaml weights YOLOv5 N 32 * 8 640 MS COCO 2017 27.4 736.08 1.9M yaml weights YOLOv5 S 32 * 8 640 MS COCO 2017 37.6 787.34 7.2M yaml weights YOLOv4 CSPDarknet53 16 * 8 608 MS COCO 2017 46.1 337.25 27.6M yaml weights YOLOv3 Darknet53 16 * 8 640 MS COCO 2017 46.6 396.60 61.9M yaml weights YOLOX S 8 * 8 640 MS COCO 2017 41.0 242.15 9.0M yaml weights"},{"location":"modelzoo/benchmark/#segmentation","title":"Segmentation","text":"performance tested on Ascend 910(8p) with graph mode Name Scale BatchSize ImageSize Dataset Box mAP (%) Mask mAP (%) Params Recipe Download YOLOv8-seg X 16 * 8 640 MS COCO 2017 52.5 42.9 71.8M yaml weights"},{"location":"modelzoo/benchmark/#deploy-inference","title":"Deploy inference","text":"<ul> <li>See deployment</li> </ul>"},{"location":"modelzoo/benchmark/#notes","title":"Notes","text":"<ul> <li>Box mAP: Accuracy reported on the validation set.</li> </ul>"},{"location":"modelzoo/yolov3/","title":"YOLOv3","text":"<p>YOLOv3: An Incremental Improvement</p>"},{"location":"modelzoo/yolov3/#abstract","title":"Abstract","text":"<p>We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. </p>"},{"location":"modelzoo/yolov3/#results","title":"Results","text":"performance tested on Ascend 910(8p) with graph mode Name Scale BatchSize ImageSize Dataset Box mAP (%) Params Recipe Download YOLOv3 Darknet53 16 * 8 640 MS COCO 2017 45.5 61.9M yaml weights performance tested on Ascend 910*(8p) Name Scale BatchSize ImageSize Dataset Box mAP (%) ms/step Params Recipe Download YOLOv3 Darknet53 16 * 8 640 MS COCO 2017 46.6 396.60 61.9M yaml weights"},{"location":"modelzoo/yolov3/#notes","title":"Notes","text":"<ul> <li>Box mAP: Accuracy reported on the validation set.</li> <li>We referred to a commonly used third-party YOLOv3 implementation.</li> </ul>"},{"location":"modelzoo/yolov3/#quick-start","title":"Quick Start","text":"<p>Please refer to the QUICK START in MindYOLO for details.</p>"},{"location":"modelzoo/yolov3/#training","title":"Training","text":""},{"location":"modelzoo/yolov3/#-pretraining-model","title":"- Pretraining Model","text":"<p>You can get the pre-training model from here.</p> <p>To convert it to a loadable ckpt file for mindyolo, please put it in the root directory then run it <pre><code>python mindyolo/utils/convert_weight_darknet53.py\n</code></pre></p>"},{"location":"modelzoo/yolov3/#-distributed-training","title":"- Distributed Training","text":"<p>It is easy to reproduce the reported results with the pre-defined training recipe. For distributed training on multiple Ascend 910 devices, please run <pre><code># distributed training on multiple Ascend devices\nmsrun --worker_num=8 --local_worker_num=8 --bind_core=True --log_dir=./yolov3_log python train.py --config ./configs/yolov3/yolov3.yaml --device_target Ascend --is_parallel True\n</code></pre></p> <p>Note: For more information about msrun configuration, please refer to here.</p> <p>For detailed illustration of all hyper-parameters, please refer to config.py.</p> <p>Note:  As the global batch size  (batch_size x num_devices) is an important hyper-parameter, it is recommended to keep the global batch size unchanged for reproduction or adjust the learning rate linearly to a new global batch size.</p>"},{"location":"modelzoo/yolov3/#-standalone-training","title":"- Standalone Training","text":"<p>If you want to train or finetune the model on a smaller dataset without distributed training, please run:</p> <pre><code># standalone training on a CPU/Ascend device\npython train.py --config ./configs/yolov3/yolov3.yaml --device_target Ascend\n</code></pre>"},{"location":"modelzoo/yolov3/#validation-and-test","title":"Validation and Test","text":"<p>To validate the accuracy of the trained model, you can use <code>test.py</code> and parse the checkpoint path with <code>--weight</code>.</p> <pre><code>python test.py --config ./configs/yolov3/yolov3.yaml --device_target Ascend --weight /PATH/TO/WEIGHT.ckpt\n</code></pre>"},{"location":"modelzoo/yolov3/#deployment","title":"Deployment","text":"<p>See here.</p>"},{"location":"modelzoo/yolov3/#references","title":"References","text":"<p>[1] Jocher Glenn. YOLOv3 release v9.1. https://github.com/ultralytics/yolov3/releases/tag/v9.1, 2021. [2] Joseph Redmon and Ali Farhadi. YOLOv3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018.</p>"},{"location":"modelzoo/yolov4/","title":"YOLOv4","text":"<p>YOLOv4: Optimal Speed and Accuracy of Object Detection</p>"},{"location":"modelzoo/yolov4/#abstract","title":"Abstract","text":"<p>There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5% AP (65.7% AP50) for the MS COCO dataset at a realtime speed of 65 FPS on Tesla V100.</p>"},{"location":"modelzoo/yolov4/#results","title":"Results","text":"performance tested on Ascend 910(8p) with graph mode Name Scale BatchSize ImageSize Dataset Box mAP (%) Params Recipe Download YOLOv4 CSPDarknet53 16 * 8 608 MS COCO 2017 45.4 27.6M yaml weights YOLOv4 CSPDarknet53(silu) 16 * 8 608 MS COCO 2017 45.8 27.6M yaml weights performance tested on Ascend 910*(8p) Name Scale BatchSize ImageSize Dataset Box mAP (%) ms/step Params Recipe Download YOLOv4 CSPDarknet53 16 * 8 608 MS COCO 2017 46.1 337.25 27.6M yaml weights"},{"location":"modelzoo/yolov4/#notes","title":"Notes","text":"<ul> <li>Box mAP: Accuracy reported on the validation set.</li> </ul>"},{"location":"modelzoo/yolov4/#quick-start","title":"Quick Start","text":"<p>Please refer to the QUICK START in MindYOLO for details.</p>"},{"location":"modelzoo/yolov4/#training","title":"Training","text":""},{"location":"modelzoo/yolov4/#-pretraining-model","title":"- Pretraining Model","text":"<p>You can get the pre-training model trained on ImageNet2012 from here.</p> <p>To convert it to a loadable ckpt file for mindyolo, please put it in the root directory then run it <pre><code>python mindyolo/utils/convert_weight_cspdarknet53.py\n</code></pre></p>"},{"location":"modelzoo/yolov4/#-distributed-training","title":"- Distributed Training","text":"<p>It is easy to reproduce the reported results with the pre-defined training recipe. For distributed training on multiple Ascend 910 devices, please run <pre><code># distributed training on multiple Ascend devices\nmsrun --worker_num=8 --local_worker_num=8 --bind_core=True --log_dir=./yolov4_log python train.py --config ./configs/yolov4/yolov4-silu.yaml --device_target Ascend --is_parallel True --epochs 320\n</code></pre></p> <p>Note: For more information about msrun configuration, please refer to here.</p> <p>For detailed illustration of all hyper-parameters, please refer to config.py.</p>"},{"location":"modelzoo/yolov4/#notes_1","title":"Notes","text":"<ul> <li>As the global batch size  (batch_size x num_devices) is an important hyper-parameter, it is recommended to keep the global batch size unchanged for reproduction or adjust the learning rate linearly to a new global batch size.</li> <li>If the following warning occurs, setting the environment variable PYTHONWARNINGS='ignore:semaphore_tracker:UserWarning' will fix it. <pre><code>multiprocessing/semaphore_tracker.py: 144 UserWarning: semaphore_tracker: There appear to be 235 leaked semaphores to clean up at shutdown len(cache))\n</code></pre></li> </ul>"},{"location":"modelzoo/yolov4/#-standalone-training","title":"- Standalone Training","text":"<p>If you want to train or finetune the model on a smaller dataset without distributed training, please run:</p> <pre><code># standalone training on a CPU/Ascend device\npython train.py --config ./configs/yolov4/yolov4-silu.yaml --device_target Ascend --epochs 320\n</code></pre>"},{"location":"modelzoo/yolov4/#validation-and-test","title":"Validation and Test","text":"<p>To validate the accuracy of the trained model, you can use <code>test.py</code> and parse the checkpoint path with <code>--weight</code>.</p> <pre><code>python test.py --config ./configs/yolov4/yolov4-silu.yaml --device_target Ascend --iou_thres 0.6 --weight /PATH/TO/WEIGHT.ckpt\n</code></pre>"},{"location":"modelzoo/yolov4/#deployment","title":"Deployment","text":"<p>See here.</p>"},{"location":"modelzoo/yolov4/#references","title":"References","text":"<p>[1] Alexey Bochkovskiy, Chien-Yao Wang and Ali Farhadi. YOLOv4: Optimal Speed and Accuracy of Object Detection. arXiv preprint arXiv:2004.10934, 2020.</p>"},{"location":"modelzoo/yolov5/","title":"YOLOv5","text":""},{"location":"modelzoo/yolov5/#abstract","title":"Abstract","text":"<p>YOLOv5 is a family of object detection architectures and models pretrained on the COCO dataset, representing Ultralytics open-source research into future vision AI methods, incorporating lessons learned and best practices evolved over thousands of hours of research and development.</p>"},{"location":"modelzoo/yolov5/#results","title":"Results","text":"performance tested on Ascend 910(8p) with graph mode Name Scale BatchSize ImageSize Dataset Box mAP (%) Params Recipe Download YOLOv5 N 32 * 8 640 MS COCO 2017 27.3 1.9M yaml weights YOLOv5 S 32 * 8 640 MS COCO 2017 37.6 7.2M yaml weights YOLOv5 M 32 * 8 640 MS COCO 2017 44.9 21.2M yaml weights YOLOv5 L 32 * 8 640 MS COCO 2017 48.5 46.5M yaml weights YOLOv5 X 16 * 8 640 MS COCO 2017 50.5 86.7M yaml weights performance tested on Ascend 910*(8p) Name Scale BatchSize ImageSize Dataset Box mAP (%) ms/step Params Recipe Download YOLOv5 N 32 * 8 640 MS COCO 2017 27.4 736.08 1.9M yaml weights YOLOv5 S 32 * 8 640 MS COCO 2017 37.6 787.34 7.2M yaml weights"},{"location":"modelzoo/yolov5/#notes","title":"Notes","text":"<ul> <li>Box mAP: Accuracy reported on the validation set.</li> <li>We refer to the official YOLOV5 to reproduce the P5 series model, and the differences are as follows: We use 8x NPU(Ascend910) for training, and the single-NPU batch size is 32. This is different from the official code.</li> </ul>"},{"location":"modelzoo/yolov5/#quick-start","title":"Quick Start","text":"<p>Please refer to the QUICK START in MindYOLO for details.</p>"},{"location":"modelzoo/yolov5/#training","title":"Training","text":""},{"location":"modelzoo/yolov5/#-distributed-training","title":"- Distributed Training","text":"<p>It is easy to reproduce the reported results with the pre-defined training recipe. For distributed training on multiple Ascend 910 devices, please run <pre><code># distributed training on multiple Ascend devices\nmsrun --worker_num=8 --local_worker_num=8 --bind_core=True --log_dir=./yolov5_log python train.py --config ./configs/yolov5/yolov5n.yaml --device_target Ascend --is_parallel True\n</code></pre></p> <p>Note: For more information about msrun configuration, please refer to here.</p> <p>For detailed illustration of all hyper-parameters, please refer to config.py.</p> <p>Note:  As the global batch size  (batch_size x num_devices) is an important hyper-parameter, it is recommended to keep the global batch size unchanged for reproduction or adjust the learning rate linearly to a new global batch size.</p>"},{"location":"modelzoo/yolov5/#-standalone-training","title":"- Standalone Training","text":"<p>If you want to train or finetune the model on a smaller dataset without distributed training, please run:</p> <pre><code># standalone training on a CPU/Ascend device\npython train.py --config ./configs/yolov5/yolov5n.yaml --device_target Ascend\n</code></pre>"},{"location":"modelzoo/yolov5/#validation-and-test","title":"Validation and Test","text":"<p>To validate the accuracy of the trained model, you can use <code>test.py</code> and parse the checkpoint path with <code>--weight</code>.</p> <pre><code>python test.py --config ./configs/yolov5/yolov5n.yaml --device_target Ascend --weight /PATH/TO/WEIGHT.ckpt\n</code></pre>"},{"location":"modelzoo/yolov5/#deployment","title":"Deployment","text":"<p>See here.</p>"},{"location":"modelzoo/yolov5/#references","title":"References","text":"<p>[1] Jocher Glenn. YOLOv5 release v6.1. https://github.com/ultralytics/yolov5/releases/tag/v6.1, 2022.</p>"},{"location":"modelzoo/yolov7/","title":"YOLOv7","text":"<p>YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</p>"},{"location":"modelzoo/yolov7/#abstract","title":"Abstract","text":"<p>YOLOv7 surpasses all known object detectors in both speed and accuracy in the range from 5 FPS to 160 FPS and has the highest accuracy 56.8% AP among all known real-time object detectors with 30 FPS or higher on GPU V100. YOLOv7-E6 object detector (56 FPS V100, 55.9% AP) outperforms both transformer-based detector SWIN-L Cascade-Mask R-CNN (9.2 FPS A100, 53.9% AP) by 509% in speed and 2% in accuracy, and convolutional-based detector ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2% AP) by 551% in speed and 0.7% AP in accuracy, as well as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR, Deformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors in speed and accuracy. Moreover, we train YOLOv7 only on MS COCO dataset from scratch without using any other datasets or pre-trained weights.</p>"},{"location":"modelzoo/yolov7/#results","title":"Results","text":"performance tested on Ascend 910(8p) with graph mode Name Scale BatchSize ImageSize Dataset Box mAP (%) Params Recipe Download YOLOv7 Tiny 16 * 8 640 MS COCO 2017 37.5 6.2M yaml weights YOLOv7 L 16 * 8 640 MS COCO 2017 50.8 36.9M yaml weights YOLOv7 X 12 * 8 640 MS COCO 2017 52.4 71.3M yaml weights performance tested on Ascend 910*(8p) Name Scale BatchSize ImageSize Dataset Box mAP (%) ms/step Params Recipe Download YOLOv7 Tiny 16 * 8 640 MS COCO 2017 37.5 496.21 6.2M yaml weights"},{"location":"modelzoo/yolov7/#notes","title":"Notes","text":"<ul> <li>Context: Training context denoted as {device}x{pieces}-{MS mode}, where mindspore mode can be G - graph mode or F - pynative mode with ms function. For example, D910x8-G is for training on 8 pieces of Ascend 910 NPU using graph mode.</li> <li>Box mAP: Accuracy reported on the validation set.</li> <li>We refer to the official YOLOV7 to reproduce the P5 series model, and the differences are as follows: We use 8x NPU(Ascend910) for training, and the single-NPU batch size for tiny/l/x is 16/16/12. This is different from the official code.</li> </ul>"},{"location":"modelzoo/yolov7/#quick-start","title":"Quick Start","text":"<p>Please refer to the QUICK START in MindYOLO for details.</p>"},{"location":"modelzoo/yolov7/#training","title":"Training","text":""},{"location":"modelzoo/yolov7/#-distributed-training","title":"- Distributed Training","text":"<p>It is easy to reproduce the reported results with the pre-defined training recipe. For distributed training on multiple Ascend 910 devices, please run <pre><code># distributed training on multiple Ascend devices\nmsrun --worker_num=8 --local_worker_num=8 --bind_core=True --log_dir=./yolov7_log python train.py --config ./configs/yolov7/yolov7.yaml --device_target Ascend --is_parallel True\n</code></pre></p> <p>Note: For more information about msrun configuration, please refer to here.</p> <p>For detailed illustration of all hyper-parameters, please refer to config.py.</p> <p>Note:  As the global batch size  (batch_size x num_devices) is an important hyper-parameter, it is recommended to keep the global batch size unchanged for reproduction or adjust the learning rate linearly to a new global batch size.</p>"},{"location":"modelzoo/yolov7/#-standalone-training","title":"- Standalone Training","text":"<p>If you want to train or finetune the model on a smaller dataset without distributed training, please run:</p> <pre><code># standalone training on a CPU/Ascend device\npython train.py --config ./configs/yolov7/yolov7.yaml --device_target Ascend\n</code></pre>"},{"location":"modelzoo/yolov7/#validation-and-test","title":"Validation and Test","text":"<p>To validate the accuracy of the trained model, you can use <code>test.py</code> and parse the checkpoint path with <code>--weight</code>.</p> <pre><code>python test.py --config ./configs/yolov7/yolov7.yaml --device_target Ascend --weight /PATH/TO/WEIGHT.ckpt\n</code></pre>"},{"location":"modelzoo/yolov7/#deployment","title":"Deployment","text":"<p>See here.</p>"},{"location":"modelzoo/yolov7/#references","title":"References","text":"<p>[1] Chien-Yao Wang, Alexey Bochkovskiy, and HongYuan Mark Liao. Yolov7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. arXiv preprint arXiv:2207.02696, 2022.</p>"},{"location":"modelzoo/yolov8/","title":"YOLOv8","text":""},{"location":"modelzoo/yolov8/#abstract","title":"Abstract","text":"<p>Ultralytics YOLOv8, developed by Ultralytics, is a cutting-edge, state-of-the-art (SOTA) model that builds upon the success of previous YOLO versions and introduces new features and improvements to further boost performance and flexibility. YOLOv8 is designed to be fast, accurate, and easy to use, making it an excellent choice for a wide range of object detection, image segmentation and image classification tasks.</p>"},{"location":"modelzoo/yolov8/#results","title":"Results","text":""},{"location":"modelzoo/yolov8/#detection","title":"Detection","text":"performance tested on Ascend 910(8p) with graph mode Name Scale BatchSize ImageSize Dataset Box mAP (%) Params Recipe Download YOLOv8 N 16 * 8 640 MS COCO 2017 37.2 3.2M yaml weights YOLOv8 S 16 * 8 640 MS COCO 2017 44.6 11.2M yaml weights YOLOv8 M 16 * 8 640 MS COCO 2017 50.5 25.9M yaml weights YOLOv8 L 16 * 8 640 MS COCO 2017 52.8 43.7M yaml weights YOLOv8 X 16 * 8 640 MS COCO 2017 53.7 68.2M yaml weights performance tested on Ascend 910*(8p) Name Scale BatchSize ImageSize Dataset Box mAP (%) ms/step Params Recipe Download YOLOv8 N 16 * 8 640 MS COCO 2017 37.3 373.55 3.2M yaml weights YOLOv8 S 16 * 8 640 MS COCO 2017 44.7 365.53 11.2M yaml weights"},{"location":"modelzoo/yolov8/#segmentation","title":"Segmentation","text":"performance tested on Ascend 910(8p) with graph mode Name Scale BatchSize ImageSize Dataset Box mAP (%) Mask mAP (%) Params Recipe Download YOLOv8-seg X 16 * 8 640 MS COCO 2017 52.5 42.9 71.8M yaml weights"},{"location":"modelzoo/yolov8/#notes","title":"Notes","text":"<ul> <li>Box mAP: Accuracy reported on the validation set.</li> <li>We refer to the official YOLOV8 to reproduce the P5 series model.</li> </ul>"},{"location":"modelzoo/yolov8/#quick-start","title":"Quick Start","text":"<p>Please refer to the QUICK START in MindYOLO for details.</p>"},{"location":"modelzoo/yolov8/#training","title":"Training","text":""},{"location":"modelzoo/yolov8/#-distributed-training","title":"- Distributed Training","text":"<p>It is easy to reproduce the reported results with the pre-defined training recipe. For distributed training on multiple Ascend 910 devices, please run <pre><code># distributed training on multiple Ascend devices\nmsrun --worker_num=8 --local_worker_num=8 --bind_core=True --log_dir=./yolov8_log python train.py --config ./configs/yolov8/yolov8n.yaml --device_target Ascend --is_parallel True\n</code></pre></p> <p>Note: For more information about msrun configuration, please refer to here.</p> <p>For detailed illustration of all hyper-parameters, please refer to config.py.</p> <p>Note:  As the global batch size  (batch_size x num_devices) is an important hyper-parameter, it is recommended to keep the global batch size unchanged for reproduction or adjust the learning rate linearly to a new global batch size.</p>"},{"location":"modelzoo/yolov8/#-standalone-training","title":"- Standalone Training","text":"<p>If you want to train or finetune the model on a smaller dataset without distributed training, please run:</p> <pre><code># standalone training on a CPU/Ascend device\npython train.py --config ./configs/yolov8/yolov8n.yaml --device_target Ascend\n</code></pre>"},{"location":"modelzoo/yolov8/#validation-and-test","title":"Validation and Test","text":"<p>To validate the accuracy of the trained model, you can use <code>test.py</code> and parse the checkpoint path with <code>--weight</code>.</p> <pre><code>python test.py --config ./configs/yolov8/yolov8n.yaml --device_target Ascend --weight /PATH/TO/WEIGHT.ckpt\n</code></pre>"},{"location":"modelzoo/yolov8/#deployment","title":"Deployment","text":"<p>See here.</p>"},{"location":"modelzoo/yolov8/#references","title":"References","text":"<p>[1] Jocher Glenn. Ultralytics YOLOv8. https://github.com/ultralytics/ultralytics, 2023.</p>"},{"location":"modelzoo/yolox/","title":"YOLOX","text":""},{"location":"modelzoo/yolox/#abstract","title":"Abstract","text":"<p>YOLOX is a new high-performance detector with some experienced improvements to YOLO series. We switch the YOLO detector to an anchor-free manner and conduct other advanced detection techniques, i.e., a decoupled head and the leading label assignment strategy SimOTA to achieve state-of-the-art results across a large scale range of models: For YOLO-Nano with only 0.91M parameters and 1.08G FLOPs, we get 25.3% AP on COCO, surpassing NanoDet by 1.8% AP; for YOLOv3, one of the most widely used detectors in industry, we boost it to 47.3% AP on COCO, outperforming the current best practice by 3.0% AP; for YOLOX-L with roughly the same amount of parameters as YOLOv4-CSP, YOLOv5-L, we achieve 50.0% AP on COCO at a speed of 68.9 FPS on Tesla V100, exceeding YOLOv5-L by 1.8% AP. Further, we won the 1st Place on Streaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021) using a single YOLOX-L model.</p>"},{"location":"modelzoo/yolox/#results","title":"Results","text":"performance tested on Ascend 910(8p) with graph mode Name Scale BatchSize ImageSize Dataset Box mAP (%) Params Recipe Download YOLOX N 8 * 8 416 MS COCO 2017 24.1 0.9M yaml weights YOLOX Tiny 8 * 8 416 MS COCO 2017 33.3 5.1M yaml weights YOLOX S 8 * 8 640 MS COCO 2017 40.7 9.0M yaml weights YOLOX M 8 * 8 640 MS COCO 2017 46.7 25.3M yaml weights YOLOX L 8 * 8 640 MS COCO 2017 49.2 54.2M yaml weights YOLOX X 8 * 8 640 MS COCO 2017 51.6 99.1M yaml weights YOLOX Darknet53 8 * 8 640 MS COCO 2017 47.7 63.7M yaml weights performance tested on Ascend 910*(8p) Name Scale BatchSize ImageSize Dataset Box mAP (%) ms/step Params Recipe Download YOLOX S 8 * 8 640 MS COCO 2017 41.0 242.15 9.0M yaml weights"},{"location":"modelzoo/yolox/#notes","title":"Notes","text":"<ul> <li>Box mAP: Accuracy reported on the validation set.</li> <li>We refer to the official YOLOX to reproduce the results.</li> </ul>"},{"location":"modelzoo/yolox/#quick-start","title":"Quick Start","text":"<p>Please refer to the QUICK START in MindYOLO for details.</p>"},{"location":"modelzoo/yolox/#training","title":"Training","text":""},{"location":"modelzoo/yolox/#-distributed-training","title":"- Distributed Training","text":"<p>It is easy to reproduce the reported results with the pre-defined training recipe. For distributed training on multiple Ascend 910 devices, please run <pre><code># distributed training on multiple Ascend devices\nmsrun --worker_num=8 --local_worker_num=8 --bind_core=True --log_dir=./yolox_log python train.py --config ./configs/yolox/yolox-s.yaml --device_target Ascend --is_parallel True\n</code></pre></p> <p>Note: For more information about msrun configuration, please refer to here.</p> <p>For detailed illustration of all hyper-parameters, please refer to config.py.</p> <p>Note:  As the global batch size  (batch_size x num_devices) is an important hyper-parameter, it is recommended to keep the global batch size unchanged for reproduction.</p>"},{"location":"modelzoo/yolox/#-standalone-training","title":"- Standalone Training","text":"<p>If you want to train or finetune the model on a smaller dataset without distributed training, please firstly run:</p> <pre><code># standalone 1st stage training on a CPU/Ascend device\npython train.py --config ./configs/yolox/yolox-s.yaml --device_target Ascend\n</code></pre>"},{"location":"modelzoo/yolox/#validation-and-test","title":"Validation and Test","text":"<p>To validate the accuracy of the trained model, you can use <code>test.py</code> and parse the checkpoint path with <code>--weight</code>.</p> <pre><code>python test.py --config ./configs/yolox/yolox-s.yaml --device_target Ascend --weight /PATH/TO/WEIGHT.ckpt\n</code></pre>"},{"location":"modelzoo/yolox/#deployment","title":"Deployment","text":"<p>See here.</p>"},{"location":"modelzoo/yolox/#references","title":"References","text":"<p>[1] Zheng Ge. YOLOX: Exceeding YOLO Series in 2021. https://arxiv.org/abs/2107.08430, 2021.</p>"},{"location":"notes/changelog/","title":"Change Log","text":"<p>Coming soon.</p>"},{"location":"notes/code_of_conduct/","title":"Code of Conduct","text":"<p>Coming soon.</p>"},{"location":"notes/contributing/","title":"MindYOLO contributing guidelines","text":""},{"location":"notes/contributing/#contributor-license-agreement","title":"Contributor License Agreement","text":"<p>It's required to sign CLA before your first code submission to MindYOLO community.</p> <p>For individual contributor, please refer to ICLA online document for the detailed information.</p>"},{"location":"notes/contributing/#getting-started","title":"Getting Started","text":"<ul> <li>Fork the repository on Github.</li> <li>Read the README.md.</li> </ul>"},{"location":"notes/contributing/#contribution-workflow","title":"Contribution Workflow","text":""},{"location":"notes/contributing/#code-style","title":"Code style","text":"<p>Please follow this style to make MindYOLO easy to review, maintain and develop.</p> <ul> <li> <p>Coding guidelines</p> <p>The Python coding style suggested by Python PEP 8 Coding Style and C++ coding style suggested by Google C++ Coding Guidelines are used in MindYOLO community. The CppLint, CppCheck, CMakeLint, CodeSpell, Lizard, ShellCheck and PyLint are used to check the format of codes, installing these plugins in your IDE is recommended.</p> </li> <li> <p>Unittest guidelines</p> <p>The Python unittest style suggested by pytest and C++ unittest style suggested by Googletest Primer are used in MindYOLO community. The design intent of a testcase should be reflected by its name of comment.</p> </li> <li> <p>Refactoring guidelines</p> <p>We encourage developers to refactor our code to eliminate the code smell. All codes should conform to needs to the coding style and testing style, and refactoring codes are no exception. Lizard threshold for nloc (lines of code without comments) is 100 and for cnc (cyclomatic complexity number) is 20, when you receive a Lizard warning, you have to refactor the code you want to merge.</p> </li> <li> <p>Document guidelines</p> <p>We use MarkdownLint to check the format of markdown documents. MindYOLO CI modifies the following rules based on the default configuration.</p> <ul> <li>MD007 (unordered list indentation): The indent parameter is set to 4, indicating that all content in the unordered list needs to be indented using four spaces.</li> <li>MD009 (spaces at the line end): The br_spaces parameter is set to 2, indicating that there can be 0 or 2 spaces at the end of a line.</li> <li>MD029 (sequence numbers of an ordered list): The style parameter is set to ordered, indicating that the sequence numbers of the ordered list are in ascending order.</li> </ul> <p>For details, please refer to RULES.</p> </li> </ul>"},{"location":"notes/contributing/#fork-pull-development-model","title":"Fork-Pull development model","text":"<ul> <li> <p>Fork MindYOLO repository</p> <p>Before submitting code to MindYOLO project, please make sure that this project have been forked to your own repository. It means that there will be parallel development between MindYOLO repository and your own repository, so be careful to avoid the inconsistency between them.</p> </li> <li> <p>Clone the remote repository</p> <p>If you want to download the code to the local machine, <code>git</code> is the best way:</p> <pre><code># For GitHub\ngit clone https://github.com/{insert_your_forked_repo}/mindyolo.git\ngit remote add upstream https://github.com/mindspore-lab/mindyolo.git\n</code></pre> </li> <li> <p>Develop code locally</p> <p>To avoid inconsistency between multiple branches, checking out to a new branch is <code>SUGGESTED</code>:</p> <pre><code>git checkout -b {new_branch_name} origin/master\n</code></pre> <p>Taking the master branch as an example, MindYOLO may create version branches and downstream development branches as needed, please fix bugs upstream first. Then you can change the code arbitrarily.</p> </li> <li> <p>Push the code to the remote repository</p> <p>After updating the code, you should push the update in the formal way:</p> <pre><code>git add .\ngit status # Check the update status\ngit commit -m \"Your commit title\"\ngit commit -s --amend #Add the concrete description of your commit\ngit push origin {new_branch_name}\n</code></pre> </li> <li> <p>Pull a request to MindYOLO repository</p> <p>In the last step, your need to pull a compare request between your new branch and MindYOLO <code>master</code> branch. After finishing the pull request, the Jenkins CI will be automatically set up for building test. Your pull request should be merged into the upstream master branch as soon as possible to reduce the risk of merging.</p> </li> </ul>"},{"location":"notes/contributing/#report-issues","title":"Report issues","text":"<p>A great way to contribute to the project is to send a detailed report when you encounter an issue. We always appreciate a well-written, thorough bug report, and will thank you for it!</p> <p>When reporting issues, refer to this format:</p> <ul> <li>What version of env (MindSpore, os, python, MindYOLO etc) are you using?</li> <li>Is this a BUG REPORT or FEATURE REQUEST?</li> <li>What kind of issue is, add the labels to highlight it on the issue dashboard.</li> <li>What happened?</li> <li>What you expected to happen?</li> <li>How to reproduce it?(as minimally and precisely as possible)</li> <li>Special notes for your reviewers?</li> </ul> <p>Issues advisory:</p> <ul> <li>If you find an unclosed issue, which is exactly what you are going to solve, please put some comments on that issue to tell others you would be in charge of it.</li> <li>If an issue is opened for a while, it's recommended for contributors to precheck before working on solving that issue.</li> <li>If you resolve an issue which is reported by yourself, it's also required to let others know before closing that issue.</li> <li>If you want the issue to be responded as quickly as possible, please try to label it, you can find kinds of labels on Label List</li> </ul>"},{"location":"notes/contributing/#propose-prs","title":"Propose PRs","text":"<ul> <li>Raise your idea as an issue on GitHub </li> <li>If it is a new feature that needs lots of design details, a design proposal should also be submitted.</li> <li>After reaching consensus in the issue discussions and design proposal reviews, complete the development on the forked repo and submit a PR.</li> <li>None of PRs is not permitted until it receives 2+ LGTM from approvers. Please NOTICE that approver is NOT allowed to add LGTM on his own PR.</li> <li>After PR is sufficiently discussed, it will get merged, abandoned or rejected depending on the outcome of the discussion.</li> </ul> <p>PRs advisory:</p> <ul> <li>Any irrelevant changes should be avoided.</li> <li>Make sure your commit history being ordered.</li> <li>Always keep your branch up with the master branch.</li> <li>For bug-fix PRs, make sure all related issues being linked.</li> </ul>"},{"location":"notes/faq/","title":"FAQ","text":"<p>Coming soon.</p>"},{"location":"reference/data/","title":"Data","text":""},{"location":"reference/data/#data-loader","title":"Data Loader","text":""},{"location":"reference/data/#mindyolo.data.loader.create_loader","title":"<code>mindyolo.data.loader.create_loader(dataset, batch_collate_fn, column_names_getitem, column_names_collate, batch_size, epoch_size=1, rank=0, rank_size=1, num_parallel_workers=8, shuffle=True, drop_remainder=False, python_multiprocessing=False)</code>","text":"<p>Creates dataloader.</p> <p>Applies operations such as transform and batch to the <code>ms.dataset.Dataset</code> object created by the <code>create_dataset</code> function to get the dataloader.</p> PARAMETER DESCRIPTION <code>dataset</code> <p>dataset object created by <code>create_dataset</code>.</p> <p> TYPE: <code>COCODataset</code> </p> <code>batch_size</code> <p>The number of rows each batch is created with. An int or callable object which takes exactly 1 parameter, BatchInfo.</p> <p> TYPE: <code>int or function</code> </p> <code>drop_remainder</code> <p>Determines whether to drop the last block whose data row number is less than batch size (default=False). If True, and if there are less than batch_size rows available to make the last batch, then those rows will be dropped and not propagated to the child node.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>num_parallel_workers</code> <p>Number of workers(threads) to process the dataset in parallel (default=None).</p> <p> TYPE: <code>int</code> DEFAULT: <code>8</code> </p> <code>python_multiprocessing</code> <p>Parallelize Python operations with multiple worker processes. This option could be beneficial if the Python operation is computational heavy (default=False).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <p>BatchDataset, dataset batched.</p> Source code in <code>mindyolo/data/loader.py</code> <pre><code>def create_loader(\n    dataset,\n    batch_collate_fn,\n    column_names_getitem,\n    column_names_collate,\n    batch_size,\n    epoch_size=1,\n    rank=0,\n    rank_size=1,\n    num_parallel_workers=8,\n    shuffle=True,\n    drop_remainder=False,\n    python_multiprocessing=False,\n):\nr\"\"\"Creates dataloader.\n\n    Applies operations such as transform and batch to the `ms.dataset.Dataset` object\n    created by the `create_dataset` function to get the dataloader.\n\n    Args:\n        dataset (COCODataset): dataset object created by `create_dataset`.\n        batch_size (int or function): The number of rows each batch is created with. An\n            int or callable object which takes exactly 1 parameter, BatchInfo.\n        drop_remainder (bool, optional): Determines whether to drop the last block\n            whose data row number is less than batch size (default=False). If True, and if there are less\n            than batch_size rows available to make the last batch, then those rows will\n            be dropped and not propagated to the child node.\n        num_parallel_workers (int, optional): Number of workers(threads) to process the dataset in parallel\n            (default=None).\n        python_multiprocessing (bool, optional): Parallelize Python operations with multiple worker processes. This\n            option could be beneficial if the Python operation is computational heavy (default=False).\n\n    Returns:\n        BatchDataset, dataset batched.\n    \"\"\"\n    cores = multiprocessing.cpu_count()\n    num_parallel_workers = min(int(cores / rank_size), num_parallel_workers)\n    logger.info(f\"Dataloader num parallel workers: [{num_parallel_workers}]\")\n    de.config.set_seed(1236517205 + rank * num_parallel_workers)\n    if rank_size &gt; 1:\n        ds = de.GeneratorDataset(\n            dataset,\n            column_names=column_names_getitem,\n            num_parallel_workers=min(8, num_parallel_workers),\n            shuffle=shuffle,\n            python_multiprocessing=python_multiprocessing,\n            num_shards=rank_size,\n            shard_id=rank,\n        )\n    else:\n        ds = de.GeneratorDataset(\n            dataset,\n            column_names=column_names_getitem,\n            num_parallel_workers=min(32, num_parallel_workers),\n            shuffle=shuffle,\n            python_multiprocessing=python_multiprocessing,\n        )\n    ds = ds.batch(\n        batch_size, per_batch_map=batch_collate_fn,\n        input_columns=column_names_getitem, output_columns=column_names_collate, drop_remainder=drop_remainder\n    )\n    ds = ds.repeat(epoch_size)\n\n    return ds\n</code></pre>"},{"location":"reference/data/#dataset","title":"Dataset","text":""},{"location":"reference/data/#mindyolo.data.dataset.COCODataset","title":"<code>mindyolo.data.dataset.COCODataset</code>","text":"<p>Load the COCO dataset (yolo format coco labels)</p> PARAMETER DESCRIPTION <code>dataset_path</code> <p>dataset label directory for dataset.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>for</code> <p>COCO_ROOT     \u251c\u2500\u2500 train2017.txt     \u251c\u2500\u2500 annotations     \u2502     \u2514\u2500\u2500 instances_train2017.json     \u251c\u2500\u2500 images     \u2502     \u2514\u2500\u2500 train2017     \u2502             \u251c\u2500\u2500 000000000001.jpg     \u2502             \u2514\u2500\u2500 000000000002.jpg     \u2514\u2500\u2500 labels           \u2514\u2500\u2500 train2017                   \u251c\u2500\u2500 000000000001.txt                   \u2514\u2500\u2500 000000000002.txt dataset_path (str): ./coco/train2017.txt</p> <p> TYPE: <code>example</code> </p> <code>transforms</code> <p>A list of images data enhancements that apply data enhancements on data set objects in order.</p> <p> TYPE: <code>list</code> </p> Source code in <code>mindyolo/data/dataset.py</code> <pre><code>class COCODataset:\n\"\"\"\n    Load the COCO dataset (yolo format coco labels)\n\n    Args:\n        dataset_path (str): dataset label directory for dataset.\n        for example:\n            COCO_ROOT\n                \u251c\u2500\u2500 train2017.txt\n                \u251c\u2500\u2500 annotations\n                \u2502     \u2514\u2500\u2500 instances_train2017.json\n                \u251c\u2500\u2500 images\n                \u2502     \u2514\u2500\u2500 train2017\n                \u2502             \u251c\u2500\u2500 000000000001.jpg\n                \u2502             \u2514\u2500\u2500 000000000002.jpg\n                \u2514\u2500\u2500 labels\n                      \u2514\u2500\u2500 train2017\n                              \u251c\u2500\u2500 000000000001.txt\n                              \u2514\u2500\u2500 000000000002.txt\n            dataset_path (str): ./coco/train2017.txt\n        transforms (list): A list of images data enhancements\n            that apply data enhancements on data set objects in order.\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset_path=\"\",\n        img_size=640,\n        transforms_dict=None,\n        is_training=False,\n        augment=False,\n        rect=False,\n        single_cls=False,\n        batch_size=32,\n        stride=32,\n        num_cls=80,\n        pad=0.0,\n        return_segments=False,  # for segment\n        return_keypoints=False, # for keypoint\n        nkpt=0,                 # for keypoint\n        ndim=0                  # for keypoint\n    ):\n        # acceptable image suffixes\n        self.img_formats = ['bmp', 'jpg', 'jpeg', 'png', 'tif', 'tiff', 'dng', 'webp', 'mpo']\n        self.cache_version = 0.2\n\n        self.return_segments = return_segments\n        self.return_keypoints = return_keypoints\n        assert not (return_segments and return_keypoints), 'Can not return both segments and keypoints.'\n\n        self.path = dataset_path\n        self.img_size = img_size\n        self.augment = augment\n        self.rect = rect\n        self.stride = stride\n        self.num_cls = num_cls\n        self.nkpt = nkpt\n        self.ndim = ndim\n        self.transforms_dict = transforms_dict\n        self.is_training = is_training\n\n        # set column names\n        # https://www.mindspore.cn/docs/zh-CN/master/api_python/dataset/mindspore.dataset.config.set_enable_shared_mem.html\n        # MS version limitations, shared memory does not support dict data type \n        self.column_names_getitem = ['im_file', 'cls', 'bboxes', 'segments', 'keypoints', 'bbox_format', 'segment_format', \n                                     'img', 'ori_shape', 'hw_scale', 'hw_pad'] if self.is_training else ['samples']\n        if self.is_training:\n            self.column_names_collate = ['images', 'labels']\n            if self.return_segments:\n                self.column_names_collate = ['images', 'labels', 'masks']\n            elif self.return_keypoints:\n                self.column_names_collate = ['images', 'labels', 'keypoints']\n        else:\n            self.column_names_collate = [\"images\", \"img_files\", \"hw_ori\", \"hw_scale\", \"pad\"]\n\n        try:\n            f = []  # image files\n            for p in self.path if isinstance(self.path, list) else [self.path]:\n                p = Path(p)  # os-agnostic\n                if p.is_dir():  # dir\n                    f += glob.glob(str(p / \"**\" / \"*.*\"), recursive=True)\n                elif p.is_file():  # file\n                    with open(p, \"r\") as t:\n                        t = t.read().strip().splitlines()\n                        parent = str(p.parent) + os.sep\n                        f += [x.replace(\"./\", parent) if x.startswith(\"./\") else x for x in t]  # local to global path\n                else:\n                    raise Exception(f\"{p} does not exist\")\n            self.img_files = sorted([x.replace(\"/\", os.sep) for x in f if x.split(\".\")[-1].lower() in self.img_formats])\n            assert self.img_files, f\"No images found\"\n        except Exception as e:\n            raise Exception(f\"Error loading data from {self.path}: {e}\\n\")\n\n        # Check cache\n        self.label_files = self._img2label_paths(self.img_files)  # labels\n        cache_path = (p if p.is_file() else Path(self.label_files[0]).parent).with_suffix(\".cache.npy\")  # cached labels\n        if cache_path.is_file():\n            cache, exists = np.load(cache_path, allow_pickle=True).item(), True  # load dict\n            if cache[\"version\"] == self.cache_version \\\n                    and cache[\"hash\"] == self._get_hash(self.label_files + self.img_files):\n                logger.info(f\"Dataset Cache file hash/version check success.\")\n                logger.info(f\"Load dataset cache from [{cache_path}] success.\")\n            else:\n                logger.info(f\"Dataset cache file hash/version check fail.\")\n                logger.info(f\"Datset caching now...\")\n                cache, exists = self.cache_labels(cache_path), False  # cache\n                logger.info(f\"Dataset caching success.\")\n        else:\n            logger.info(f\"No dataset cache available, caching now...\")\n            cache, exists = self.cache_labels(cache_path), False  # cache\n            logger.info(f\"Dataset caching success.\")\n\n        # Display cache\n        nf, nm, ne, nc, n = cache.pop(\"results\")  # found, missing, empty, corrupted, total\n        if exists:\n            d = f\"Scanning '{cache_path}' images and labels... {nf} found, {nm} missing, {ne} empty, {nc} corrupted\"\n            tqdm(None, desc=d, total=n, initial=n)  # display cache results\n        assert nf &gt; 0 or not augment, f\"No labels in {cache_path}. Can not train without labels.\"\n\n        # Read cache\n        cache.pop(\"hash\")  # remove hash\n        cache.pop(\"version\")  # remove version\n        self.labels = cache['labels']\n        self.img_files = [lb['im_file'] for lb in self.labels]  # update im_files\n\n        # Check if the dataset is all boxes or all segments\n        lengths = ((len(lb['cls']), len(lb['bboxes']), len(lb['segments'])) for lb in self.labels)\n        len_cls, len_boxes, len_segments = (sum(x) for x in zip(*lengths))\n        if len_segments and len_boxes != len_segments:\n            print(\n                f'WARNING \u26a0\ufe0f Box and segment counts should be equal, but got len(segments) = {len_segments}, '\n                f'len(boxes) = {len_boxes}. To resolve this only boxes will be used and all segments will be removed. '\n                'To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.')\n            for lb in self.labels:\n                lb['segments'] = []\n        if len_cls == 0:\n            raise ValueError(f'All labels empty in {cache_path}, can not start training without labels.')\n\n        if single_cls:\n            for x in self.labels:\n                x['cls'][:, 0] = 0\n\n        n = len(self.labels)  # number of images\n        bi = np.floor(np.arange(n) / batch_size).astype(np.int_)  # batch index\n        nb = bi[-1] + 1  # number of batches\n        self.batch = bi  # batch index of image\n\n        # Cache images into memory for faster training (WARNING: large datasets may exceed system RAM)\n        self.imgs, self.img_hw_ori, self.indices = [None] * n, [None] * n, range(n)\n        # Buffer thread for mosaic images\n        self.buffer = []\n        self.max_buffer_length = min((n, batch_size * 8, 1000)) if self.augment else 0\n\n        # Rectangular Train/Test\n        if self.rect:\n            # Sort by aspect ratio\n            s = self.img_shapes  # wh\n            ar = s[:, 1] / s[:, 0]  # aspect ratio\n            irect = ar.argsort()\n            self.img_files = [self.img_files[i] for i in irect]\n            self.label_files = [self.label_files[i] for i in irect]\n            self.labels = [self.labels[i] for i in irect]\n            self.img_shapes = s[irect]  # wh\n            ar = ar[irect]\n\n            # Set training image shapes\n            shapes = [[1, 1]] * nb\n            for i in range(nb):\n                ari = ar[bi == i]\n                mini, maxi = ari.min(), ari.max()\n                if maxi &lt; 1:\n                    shapes[i] = [maxi, 1]\n                elif mini &gt; 1:\n                    shapes[i] = [1, 1 / mini]\n\n            self.batch_shapes = np.ceil(np.array(shapes) * img_size / stride + pad).astype(np.int_) * stride\n\n        self.imgIds = [int(Path(im_file).stem) for im_file in self.img_files]\n\n    def cache_labels(self, path=Path(\"./labels.cache.npy\")):\n        # Cache dataset labels, check images and read shapes\n        x = {'labels': []}  # dict\n        nm, nf, ne, nc, segments, keypoints = 0, 0, 0, 0, [], None  # number missing, found, empty, duplicate\n        pbar = tqdm(zip(self.img_files, self.label_files), desc=\"Scanning images\", total=len(self.img_files))\n        if self.return_keypoints and (self.nkpt &lt;= 0 or self.ndim not in (2, 3)):\n            raise ValueError(\"'kpt_shape' in data.yaml missing or incorrect. Should be a list with [number of \"\n                             \"keypoints, number of dims (2 for x,y or 3 for x,y,visible)], i.e. 'kpt_shape: [17, 3]'\")\n        for i, (im_file, lb_file) in enumerate(pbar):\n            try:\n                # verify images\n                im = Image.open(im_file)\n                im.verify()  # PIL verify\n                shape = self._exif_size(im)  # image size\n                segments = []  # instance segments\n                assert (shape[0] &gt; 9) &amp; (shape[1] &gt; 9), f\"image size {shape} &lt;10 pixels\"\n                assert im.format.lower() in self.img_formats, f\"invalid image format {im.format}\"\n\n                # verify labels\n                if os.path.isfile(lb_file):\n                    nf += 1  # label found\n                    with open(lb_file, \"r\") as f:\n                        lb = [x.split() for x in f.read().strip().splitlines()]\n                        if any([len(x) &gt; 6 for x in lb]) and (not self.return_keypoints):  # is segment\n                            classes = np.array([x[0] for x in lb], dtype=np.float32)\n                            segments = [np.array(x[1:], dtype=np.float32).reshape(-1, 2) for x in lb]  # (cls, xy1...)\n                            lb = np.concatenate(\n                                (classes.reshape(-1, 1), segments2boxes(segments)), 1\n                            )  # (cls, xywh)\n                        lb = np.array(lb, dtype=np.float32)\n                    nl = len(lb)\n                    if nl:\n                        if self.return_keypoints:\n                            assert lb.shape[1] == (5 + self.nkpt * self.ndim), \\\n                                f'labels require {(5 + self.nkpt * self.ndim)} columns each'\n                            assert (lb[:, 5::self.ndim] &lt;= 1).all(), 'non-normalized or out of bounds coordinate labels'\n                            assert (lb[:, 6::self.ndim] &lt;= 1).all(), 'non-normalized or out of bounds coordinate labels'\n                        else:\n                            assert lb.shape[1] == 5, f'labels require 5 columns, {lb.shape[1]} columns detected'\n                            assert (lb[:, 1:] &lt;= 1).all(), \\\n                                f'non-normalized or out of bounds coordinates {lb[:, 1:][lb[:, 1:] &gt; 1]}'\n                            assert (lb &gt;= 0).all(), f'negative label values {lb[lb &lt; 0]}'\n                        # All labels\n                        max_cls = int(lb[:, 0].max())  # max label count\n                        assert max_cls &lt;= self.num_cls, \\\n                            f'Label class {max_cls} exceeds dataset class count {self.num_cls}. ' \\\n                            f'Possible class labels are 0-{self.num_cls - 1}'\n                        _, j = np.unique(lb, axis=0, return_index=True)\n                        if len(j) &lt; nl:  # duplicate row check\n                            lb = lb[j]  # remove duplicates\n                            if segments:\n                                segments = [segments[x] for x in i]\n                            print(f'WARNING \u26a0\ufe0f {im_file}: {nl - len(j)} duplicate labels removed')\n                    else:\n                        ne += 1  # label empty\n                        lb = np.zeros((0, (5 + self.nkpt * self.ndim)), dtype=np.float32) \\\n                            if self.return_keypoints else np.zeros((0, 5), dtype=np.float32)\n                else:\n                    nm += 1  # label missing\n                    lb = np.zeros((0, (5 + self.nkpt * self.ndim)), dtype=np.float32) \\\n                        if self.return_keypoints else np.zeros((0, 5), dtype=np.float32)\n                if self.return_keypoints:\n                    keypoints = lb[:, 5:].reshape(-1, self.nkpt, self.ndim)\n                    if self.ndim == 2:\n                        kpt_mask = np.ones(keypoints.shape[:2], dtype=np.float32)\n                        kpt_mask = np.where(keypoints[..., 0] &lt; 0, 0.0, kpt_mask)\n                        kpt_mask = np.where(keypoints[..., 1] &lt; 0, 0.0, kpt_mask)\n                        keypoints = np.concatenate([keypoints, kpt_mask[..., None]], axis=-1)  # (nl, nkpt, 3)\n                lb = lb[:, :5]\n                x['labels'].append(\n                    dict(\n                        im_file=im_file,\n                        cls=lb[:, 0:1],     # (n, 1)\n                        bboxes=lb[:, 1:],   # (n, 4)\n                        segments=segments,  # list of (mi, 2)\n                        keypoints=keypoints,\n                        bbox_format='xywhn',\n                        segment_format='polygon'\n                    )\n                )\n            except Exception as e:\n                nc += 1\n                print(f\"WARNING: Ignoring corrupted image and/or label {im_file}: {e}\")\n\n            pbar.desc = f\"Scanning '{path.parent / path.stem}' images and labels... \" \\\n                        f\"{nf} found, {nm} missing, {ne} empty, {nc} corrupted\"\n        pbar.close()\n\n        if nf == 0:\n            print(f\"WARNING: No labels found in {path}.\")\n\n        x[\"hash\"] = self._get_hash(self.label_files + self.img_files)\n        x[\"results\"] = nf, nm, ne, nc, len(self.img_files)\n        x[\"version\"] = self.cache_version  # cache version\n        np.save(path, x)  # save for next time\n        logger.info(f\"New cache created: {path}\")\n        return x\n\n    def __getitem__(self, index):\n        sample = self.get_sample(index)\n\n        for _i, ori_trans in enumerate(self.transforms_dict):\n            _trans = ori_trans.copy()\n            func_name, prob = _trans.pop(\"func_name\"), _trans.pop(\"prob\", 1.0)\n            if func_name == 'copy_paste':\n                sorted = _trans.pop(\"sorted\", False)\n                sample = self.copy_paste(sample, prob, sorted)\n            elif random.random() &lt; prob:\n                if func_name == \"albumentations\" and getattr(self, \"albumentations\", None) is None:\n                    self.albumentations = Albumentations(size=self.img_size, **_trans)\n                if func_name == \"letterbox\":\n                    new_shape = self.img_size if not self.rect else self.batch_shapes[self.batch[index]]\n                    sample = self.letterbox(sample, new_shape, **_trans)\n                else:\n                    sample = getattr(self, func_name)(sample, **_trans)\n\n        sample['img'] = np.ascontiguousarray(sample['img'])\n        if self.is_training:\n            train_sample = []\n            if len(sample['segments']) &gt; 0 and not self.return_segments:\n                sample['segments'] = np.nan\n            for col_name in self.column_names_getitem:\n                if sample.get(col_name) is None:\n                    train_sample.append(np.nan)\n                else:\n                    train_sample.append(sample.get(col_name, np.nan))\n            return tuple(train_sample)\n        return sample\n\n    def __len__(self):\n        return len(self.img_files)\n\n    def get_sample(self, index):\n\"\"\"Get and return label information from the dataset.\"\"\"\n        sample = deepcopy(self.labels[index])\n        img = self.imgs[index]\n        if img is None:\n            path = self.img_files[index]\n            img = cv2.imread(path)  # BGR\n            assert img is not None, \"Image Not Found \" + path\n            h_ori, w_ori = img.shape[:2]  # orig hw\n            r = self.img_size / max(h_ori, w_ori)  # resize image to img_size\n            if r != 1:  # always resize down, only resize up if training with augmentation\n                interp = cv2.INTER_AREA if r &lt; 1 and not self.augment else cv2.INTER_LINEAR\n                img = cv2.resize(img, (int(w_ori * r), int(h_ori * r)), interpolation=interp)\n\n            if self.augment:\n                self.imgs[index], self.img_hw_ori[index] = img, np.array([h_ori, w_ori]) # img, hw_original\n                self.buffer.append(index)\n                if 1 &lt; len(self.buffer) &gt;= self.max_buffer_length:\n                    j = self.buffer.pop(0)\n                    self.imgs[j], self.img_hw_ori[j] = None, np.array([None, None])\n            sample['img'], sample['ori_shape'] = img, np.array([h_ori, w_ori])  # img, hw_original\n        else:\n            sample['img'], sample['ori_shape'] = self.imgs[index], self.img_hw_ori[index]  # img, hw_original\n\n        return sample\n\n    def mosaic(\n        self,\n        sample,\n        mosaic9_prob=0.0,\n        post_transform=None,\n    ):\n        segment_format = sample['segment_format']\n        bbox_format = sample['bbox_format']\n        assert segment_format == 'polygon', f'The segment format should be polygon, but got {segment_format}'\n        assert bbox_format == 'xywhn', f'The bbox format should be xywhn, but got {bbox_format}'\n\n        mosaic9_prob = min(1.0, max(mosaic9_prob, 0.0))\n        if random.random() &lt; (1 - mosaic9_prob):\n            sample = self._mosaic4(sample)\n        else:\n            sample = self._mosaic9(sample)\n\n        if post_transform:\n            for _i, ori_trans in enumerate(post_transform):\n                _trans = ori_trans.copy()\n                func_name, prob = _trans.pop(\"func_name\"), _trans.pop(\"prob\", 1.0)\n                sample = getattr(self, func_name)(sample, **_trans)\n\n        return sample\n\n    def _mosaic4(self, sample):\n        # loads images in a 4-mosaic\n        classes4, bboxes4, segments4 = [], [], []\n        mosaic_samples = [sample, ]\n        indices = random.choices(self.buffer, k=3)  # 3 additional image indices\n\n        segments_is_list = isinstance(sample['segments'], list)\n        if segments_is_list:\n            mosaic_samples += [self.get_sample(i) for i in indices]\n        else:\n            mosaic_samples += [self.resample_segments(self.get_sample(i)) for i in indices]\n\n        s = self.img_size\n        mosaic_border = [-s // 2, -s // 2]\n        yc, xc = [int(random.uniform(-x, 2 * s + x)) for x in mosaic_border]  # mosaic center x, y\n\n        for i, mosaic_sample in enumerate(mosaic_samples):\n            # Load image\n            img = mosaic_sample['img']\n            (h, w) = img.shape[:2]\n\n            # place img in img4\n            if i == 0:  # top left\n                img4 = np.full((s * 2, s * 2, img.shape[2]), 114, dtype=np.uint8)  # base image with 4 tiles\n                x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)\n                x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)\n            elif i == 1:  # top right\n                x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\n                x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n            elif i == 2:  # bottom left\n                x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)\n                x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, w, min(y2a - y1a, h)\n            elif i == 3:  # bottom right\n                x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)\n                x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n\n            img4[y1a:y2a, x1a:x2a] = img[y1b:y2b, x1b:x2b]  # img4[ymin:ymax, xmin:xmax]\n            padw = x1a - x1b\n            padh = y1a - y1b\n\n            # box and cls\n            cls, bboxes = mosaic_sample['cls'], mosaic_sample['bboxes']\n            assert mosaic_sample['bbox_format'] == 'xywhn'\n            bboxes = xywhn2xyxy(bboxes, w, h, padw, padh)  # normalized xywh to pixel xyxy format\n            classes4.append(cls)\n            bboxes4.append(bboxes)\n\n            # seg\n            assert mosaic_sample['segment_format'] == 'polygon'\n            segments = mosaic_sample['segments']\n            if segments_is_list:\n                segments = [xyn2xy(x, w, h, padw, padh) for x in segments]\n                segments4.extend(segments)\n            else:\n                segments = xyn2xy(segments, w, h, padw, padh)\n                segments4.append(segments)\n\n        classes4 = np.concatenate(classes4, 0)\n        bboxes4 = np.concatenate(bboxes4, 0)\n        bboxes4 = bboxes4.clip(0, 2 * s)\n\n        if segments_is_list:\n            for x in segments4:\n                np.clip(x, 0, 2 * s, out=x)\n        else:\n            segments4 = np.concatenate(segments4, 0)\n            segments4 = segments4.clip(0, 2 * s)\n\n        sample['img'] = img4\n        sample['cls'] = classes4\n        sample['bboxes'] = bboxes4\n        sample['bbox_format'] = 'ltrb'\n        sample['segments'] = segments4\n        sample['mosaic_border'] = mosaic_border\n\n        return sample\n\n    def _mosaic9(self, sample):\n        # loads images in a 9-mosaic\n        classes9, bboxes9, segments9 = [], [], []\n        mosaic_samples = [sample, ]\n        indices = random.choices(self.buffer, k=8)  # 8 additional image indices\n\n        segments_is_list = isinstance(sample['segments'], list)\n        if segments_is_list:\n            mosaic_samples += [self.get_sample(i) for i in indices]\n        else:\n            mosaic_samples += [self.resample_segments(self.get_sample(i)) for i in indices]\n        s = self.img_size\n        mosaic_border = [-s // 2, -s // 2]\n\n        for i, mosaic_sample in enumerate(mosaic_samples):\n            # Load image\n            img = mosaic_sample['img']\n            (h, w) = img.shape[:2]\n\n            # place img in img9\n            if i == 0:  # center\n                img9 = np.full((s * 3, s * 3, img.shape[2]), 114, dtype=np.uint8)  # base image with 4 tiles\n                h0, w0 = h, w\n                c = s, s, s + w, s + h  # xmin, ymin, xmax, ymax (base) coordinates\n            elif i == 1:  # top\n                c = s, s - h, s + w, s\n            elif i == 2:  # top right\n                c = s + wp, s - h, s + wp + w, s\n            elif i == 3:  # right\n                c = s + w0, s, s + w0 + w, s + h\n            elif i == 4:  # bottom right\n                c = s + w0, s + hp, s + w0 + w, s + hp + h\n            elif i == 5:  # bottom\n                c = s + w0 - w, s + h0, s + w0, s + h0 + h\n            elif i == 6:  # bottom left\n                c = s + w0 - wp - w, s + h0, s + w0 - wp, s + h0 + h\n            elif i == 7:  # left\n                c = s - w, s + h0 - h, s, s + h0\n            elif i == 8:  # top left\n                c = s - w, s + h0 - hp - h, s, s + h0 - hp\n\n            padx, pady = c[:2]\n            x1, y1, x2, y2 = [max(x, 0) for x in c]  # allocate coords\n\n            # box and cls\n            assert mosaic_sample['bbox_format'] == 'xywhn'\n            cls, bboxes = mosaic_sample['cls'], mosaic_sample['bboxes']\n            bboxes = xywhn2xyxy(bboxes, w, h, padx, pady)  # normalized xywh to pixel xyxy format\n            classes9.append(cls)\n            bboxes9.append(bboxes)\n\n            # seg\n            assert mosaic_sample['segment_format'] == 'polygon'\n            segments = mosaic_sample['segments']\n            if segments_is_list:\n                segments = [xyn2xy(x, w, h, padx, pady) for x in segments]\n                segments9.extend(segments)\n            else:\n                segments = xyn2xy(segments, w, h, padx, pady)\n                segments9.append(segments)\n\n            # Image\n            img9[y1:y2, x1:x2] = img[y1 - pady:, x1 - padx:]  # img9[ymin:ymax, xmin:xmax]\n            hp, wp = h, w  # height, width previous\n\n        # Offset\n        yc, xc = [int(random.uniform(0, s)) for _ in mosaic_border]  # mosaic center x, y\n        img9 = img9[yc: yc + 2 * s, xc: xc + 2 * s]\n\n        # Concat/clip labels\n        classes9 = np.concatenate(classes9, 0)\n        bboxes9 = np.concatenate(bboxes9, 0)\n        bboxes9[:, [0, 2]] -= xc\n        bboxes9[:, [1, 3]] -= yc\n        bboxes9 = bboxes9.clip(0, 2 * s)\n\n        if segments_is_list:\n            c = np.array([xc, yc])  # centers\n            segments9 = [x - c for x in segments9]\n            for x in segments9:\n                np.clip(x, 0, 2 * s, out=x)\n        else:\n            segments9 = np.concatenate(segments9, 0)\n            segments9[..., 0] -= xc\n            segments9[..., 1] -= yc\n            segments9 = segments9.clip(0, 2 * s)\n\n        sample['img'] = img9\n        sample['cls'] = classes9\n        sample['bboxes'] = bboxes9\n        sample['bbox_format'] = 'ltrb'\n        sample['segments'] = segments9\n        sample['mosaic_border'] = mosaic_border\n\n        return sample\n\n    def resample_segments(self, sample, n=1000):\n        segment_format = sample['segment_format']\n        assert segment_format == 'polygon', f'The segment format is should be polygon, but got {segment_format}'\n\n        segments = sample['segments']\n        if len(segments) &gt; 0:\n            # Up-sample an (n,2) segment\n            for i, s in enumerate(segments):\n                s = np.concatenate((s, s[0:1, :]), axis=0)\n                x = np.linspace(0, len(s) - 1, n)\n                xp = np.arange(len(s))\n                segments[i] = np.concatenate([np.interp(x, xp, s[:, i]) for i in range(2)]).reshape(2, -1).T  # segment xy\n            segments = np.stack(segments, axis=0)\n        else:\n            segments = np.zeros((0, 1000, 2), dtype=np.float32)\n        sample['segments'] = segments\n        return sample\n\n    def copy_paste(self, sample, probability=0.5, sorted=False):\n        # Implement Copy-Paste augmentation https://arxiv.org/abs/2012.07177, labels as nx5 np.array(cls, xyxy)\n        bbox_format, segment_format = sample['bbox_format'], sample['segment_format']\n        assert bbox_format == 'ltrb', f'The bbox format should be ltrb, but got {bbox_format}'\n        assert segment_format == 'polygon', f'The segment format should be polygon, but got {segment_format}'\n\n        img = sample['img']\n        cls = sample['cls']\n        bboxes = sample['bboxes']\n        segments = sample['segments']\n\n        n = len(segments)\n        if len(segments) == 0 or probability == 0:\n            return sample\n\n        h, w, _ = img.shape  # height, width, channels\n        im_new = np.zeros(img.shape, np.uint8)\n\n        if not sorted:\n            for j in random.sample(range(n), k=round(probability * n)):\n                c, l, s = cls[j], bboxes[j], segments[j]\n                box = np.array([[w - l[2], l[1], w - l[0], l[3]]], dtype=np.float32)\n                ioa = bbox_ioa(box, bboxes)  # intersection over area\n                if (ioa &lt; 0.30).all():  # allow 30% obscuration of existing labels\n                    cls = np.concatenate((cls, [c]), 0)\n                    bboxes = np.concatenate((bboxes, [box]), 0)\n                    if isinstance(segments, list):\n                        segments.append(np.concatenate((w - s[:, 0:1], s[:, 1:2]), 1))\n                    else:\n                        segments = np.concatenate((segments, [np.concatenate((w - s[:, 0:1], s[:, 1:2]), 1)]), 0)\n                    cv2.drawContours(im_new, [segments[j].astype(np.int32)], -1, (255, 255, 255), cv2.FILLED)\n        else:\n            bboxes2 = bboxes.copy()\n            bboxes2[:, 0] = w - bboxes[:, 2]\n            bboxes2[:, 2] = w - bboxes[:, 0]\n\n            ioa = bbox_ioa(bboxes2, bboxes)  # intersection over area, (N, M)\n            indexes = np.nonzero((ioa &lt; 0.30).all(1))[0]  # (N, ) allow 30% obscuration of existing labels\n\n            n = len(indexes)\n            sorted_idx = np.argsort(ioa.max(1)[indexes])\n            indexes = indexes[sorted_idx]\n            for j in indexes[: round(probability * n)]:\n                c, s = cls[j], segments[j]\n                cls = np.concatenate((cls, [c]), 0)\n                bboxes = np.concatenate((bboxes, [bboxes2[j]]), 0)\n                if isinstance(segments, list):\n                    segments.append(np.concatenate((w - s[:, 0:1], s[:, 1:2]), 1))\n                else:\n                    segments = np.concatenate((segments, [np.concatenate((w - s[:, 0:1], s[:, 1:2]), 1)]), 0)\n                cv2.drawContours(im_new, [segments[j].astype(np.int32)], -1, (255, 255, 255), cv2.FILLED)\n\n        result = cv2.bitwise_and(src1=img, src2=im_new)\n        result = cv2.flip(result, 1)  # augment segments (flip left-right)\n        i = result &gt; 0  # pixels to replace\n        img[i] = result[i]\n\n        sample['img'] = img\n        sample['cls'] = cls\n        sample['bboxes'] = bboxes\n        sample['segments'] = segments\n\n        return sample\n\n    def random_perspective(\n            self, sample, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, border=(0, 0)\n    ):\n        bbox_format, segment_format = sample['bbox_format'], sample['segment_format']\n        assert bbox_format == 'ltrb', f'The bbox format should be ltrb, but got {bbox_format}'\n        assert segment_format == 'polygon', f'The segment format should be polygon, but got {segment_format}'\n\n        img = sample['img']\n        cls = sample['cls']\n        targets = sample['bboxes']\n        segments = sample['segments']\n        assert isinstance(segments, np.ndarray), f\"segments type expect numpy.ndarray, but got {type(segments)}; \" \\\n                                                 f\"maybe you should resample_segments before that.\"\n\n        border = sample.pop('mosaic_border', border)\n        height = img.shape[0] + border[0] * 2  # shape(h,w,c)\n        width = img.shape[1] + border[1] * 2\n\n        # Center\n        C = np.eye(3)\n        C[0, 2] = -img.shape[1] / 2  # x translation (pixels)\n        C[1, 2] = -img.shape[0] / 2  # y translation (pixels)\n\n        # Perspective\n        P = np.eye(3)\n        P[2, 0] = random.uniform(-perspective, perspective)  # x perspective (about y)\n        P[2, 1] = random.uniform(-perspective, perspective)  # y perspective (about x)\n\n        # Rotation and Scale\n        R = np.eye(3)\n        a = random.uniform(-degrees, degrees)\n        s = random.uniform(1 - scale, 1 + scale)\n        R[:2] = cv2.getRotationMatrix2D(angle=a, center=(0, 0), scale=s)\n\n        # Shear\n        S = np.eye(3)\n        S[0, 1] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  # x shear (deg)\n        S[1, 0] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  # y shear (deg)\n\n        # Translation\n        T = np.eye(3)\n        T[0, 2] = random.uniform(0.5 - translate, 0.5 + translate) * width  # x translation (pixels)\n        T[1, 2] = random.uniform(0.5 - translate, 0.5 + translate) * height  # y translation (pixels)\n\n        # Combined rotation matrix\n        M = T @ S @ R @ P @ C  # order of operations (right to left) is IMPORTANT\n        if (border[0] != 0) or (border[1] != 0) or (M != np.eye(3)).any():  # image changed\n            if perspective:\n                img = cv2.warpPerspective(img, M, dsize=(width, height), borderValue=(114, 114, 114))\n            else:  # affine\n                img = cv2.warpAffine(img, M[:2], dsize=(width, height), borderValue=(114, 114, 114))\n\n        # Transform label coordinates\n        n = len(targets)\n        if n:\n            use_segments = len(segments)\n            new_bboxes = np.zeros((n, 4))\n            if use_segments:  # warp segments\n                point_num = segments[0].shape[0]\n                new_segments = np.zeros((n, point_num, 2))\n                for i, segment in enumerate(segments):\n                    xy = np.ones((len(segment), 3))\n                    xy[:, :2] = segment\n                    xy = xy @ M.T  # transform\n                    xy = xy[:, :2] / xy[:, 2:3] if perspective else xy[:, :2]  # perspective rescale or affine\n\n                    # clip\n                    new_segments[i] = xy\n                    new_bboxes[i] = segment2box(xy, width, height)\n\n            else:  # warp boxes\n                xy = np.ones((n * 4, 3))\n                xy[:, :2] = targets[:, [0, 1, 2, 3, 0, 3, 2, 1]].reshape(n * 4, 2)  # x1y1, x2y2, x1y2, x2y1\n                xy = xy @ M.T  # transform\n                xy = (xy[:, :2] / xy[:, 2:3] if perspective else xy[:, :2]).reshape(n, 8)  # perspective rescale or affine\n\n                # create new boxes\n                x = xy[:, [0, 2, 4, 6]]\n                y = xy[:, [1, 3, 5, 7]]\n                new_bboxes = np.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, n).T\n\n                # clip\n                new_bboxes[:, [0, 2]] = new_bboxes[:, [0, 2]].clip(0, width)\n                new_bboxes[:, [1, 3]] = new_bboxes[:, [1, 3]].clip(0, height)\n\n            # filter candidates\n            i = box_candidates(box1=targets.T * s, box2=new_bboxes.T, area_thr=0.01 if use_segments else 0.10)\n\n            cls = cls[i]\n            targets = new_bboxes[i]\n            sample['cls'] = cls\n            sample['bboxes'] = targets\n            if use_segments:\n                sample['segments'] = new_segments[i]\n\n        sample['img'] = img\n\n        return sample\n\n    def mixup(self, sample, alpha: 32.0, beta: 32.0, pre_transform=None):\n        bbox_format, segment_format = sample['bbox_format'], sample['segment_format']\n        assert bbox_format == 'ltrb', f'The bbox format should be ltrb, but got {bbox_format}'\n        assert segment_format == 'polygon', f'The segment format should be polygon, but got {segment_format}'\n\n        index = random.choices(self.indices, k=1)[0]\n        sample2 = self.get_sample(index)\n        if pre_transform:\n            for _i, ori_trans in enumerate(pre_transform):\n                _trans = ori_trans.copy()\n                func_name, prob = _trans.pop(\"func_name\"), _trans.pop(\"prob\", 1.0)\n                if func_name == 'copy_paste':\n                    sorted = _trans.pop(\"sorted\", False)\n                    sample2 = self.copy_paste(sample2, prob, sorted)\n                elif random.random() &lt; prob:\n                    if func_name == \"albumentations\" and getattr(self, \"albumentations\", None) is None:\n                        self.albumentations = Albumentations(size=self.img_size, **_trans)\n                    sample2 = getattr(self, func_name)(sample2, **_trans)\n\n        assert isinstance(sample['segments'], np.ndarray), \\\n            f\"MixUp: sample segments type expect numpy.ndarray, but got {type(sample['segments'])}; \" \\\n            f\"maybe you should resample_segments before that.\"\n        assert isinstance(sample2['segments'], np.ndarray), \\\n            f\"MixUp: sample2 segments type expect numpy.ndarray, but got {type(sample2['segments'])}; \" \\\n            f\"maybe you should add resample_segments in pre_transform.\"\n\n        image, image2 = sample['img'], sample2['img']\n        r = np.random.beta(alpha, beta)  # mixup ratio, alpha=beta=8.0\n        image = (image * r + image2 * (1 - r)).astype(np.uint8)\n\n        sample['img'] = image\n        sample['cls'] = np.concatenate((sample['cls'], sample2['cls']), 0)\n        sample['bboxes'] = np.concatenate((sample['bboxes'], sample2['bboxes']), 0)\n        sample['segments'] = np.concatenate((sample['segments'], sample2['segments']), 0)\n        return sample\n\n    def pastein(self, sample, num_sample=30):\n        bbox_format = sample['bbox_format']\n        assert bbox_format == 'ltrb', f'The bbox format should be ltrb, but got {bbox_format}'\n        assert not self.return_segments, \"pastein currently does not support seg data.\"\n        assert not self.return_keypoints, \"pastein currently does not support keypoint data.\"\n\n        image = sample['img']\n        cls = sample['cls']\n        bboxes = sample['bboxes']\n        # load sample\n        sample_labels, sample_images, sample_masks = [], [], []\n        while len(sample_labels) &lt; num_sample:\n            sample_labels_, sample_images_, sample_masks_ = self._pastin_load_samples()\n            sample_labels += sample_labels_\n            sample_images += sample_images_\n            sample_masks += sample_masks_\n            if len(sample_labels) == 0:\n                break\n\n        # Applies image cutout augmentation https://arxiv.org/abs/1708.04552\n        h, w = image.shape[:2]\n\n        # create random masks\n        scales = [0.75] * 2 + [0.5] * 4 + [0.25] * 4 + [0.125] * 4 + [0.0625] * 6  # image size fraction\n        for s in scales:\n            if random.random() &lt; 0.2:\n                continue\n            mask_h = random.randint(1, int(h * s))\n            mask_w = random.randint(1, int(w * s))\n\n            # box\n            xmin = max(0, random.randint(0, w) - mask_w // 2)\n            ymin = max(0, random.randint(0, h) - mask_h // 2)\n            xmax = min(w, xmin + mask_w)\n            ymax = min(h, ymin + mask_h)\n\n            box = np.array([[xmin, ymin, xmax, ymax]], dtype=np.float32)\n            if len(bboxes):\n                ioa = bbox_ioa(box, bboxes)  # intersection over area\n            else:\n                ioa = np.zeros(1)\n\n            if (\n                    (ioa &lt; 0.30).all() and len(sample_labels) and (xmax &gt; xmin + 20) and (ymax &gt; ymin + 20)\n            ):  # allow 30% obscuration of existing labels\n                sel_ind = random.randint(0, len(sample_labels) - 1)\n                hs, ws, cs = sample_images[sel_ind].shape\n                r_scale = min((ymax - ymin) / hs, (xmax - xmin) / ws)\n                r_w = int(ws * r_scale)\n                r_h = int(hs * r_scale)\n\n                if (r_w &gt; 10) and (r_h &gt; 10):\n                    r_mask = cv2.resize(sample_masks[sel_ind], (r_w, r_h))\n                    r_image = cv2.resize(sample_images[sel_ind], (r_w, r_h))\n                    temp_crop = image[ymin: ymin + r_h, xmin: xmin + r_w]\n                    m_ind = r_mask &gt; 0\n                    if m_ind.astype(np.int_).sum() &gt; 60:\n                        temp_crop[m_ind] = r_image[m_ind]\n                        box = np.array([xmin, ymin, xmin + r_w, ymin + r_h], dtype=np.float32)\n                        if len(bboxes):\n                            cls = np.concatenate((cls, [[sample_labels[sel_ind]]]), 0)\n                            bboxes = np.concatenate((bboxes, [box]), 0)\n                        else:\n                            cls = np.array([[sample_labels[sel_ind]]])\n                            bboxes = np.array([box])\n\n                        image[ymin: ymin + r_h, xmin: xmin + r_w] = temp_crop  # Modify on the original image\n\n        sample['img'] = image\n        sample['bboxes'] = bboxes\n        sample['cls'] = cls\n        return sample\n\n    def _pastin_load_samples(self):\n        # loads images in a 4-mosaic\n        classes4, bboxes4, segments4 = [], [], []\n        mosaic_samples = []\n        indices = random.choices(self.indices, k=4)  # 3 additional image indices\n        mosaic_samples += [self.get_sample(i) for i in indices]\n        s = self.img_size\n        mosaic_border = [-s // 2, -s // 2]\n        yc, xc = [int(random.uniform(-x, 2 * s + x)) for x in mosaic_border]  # mosaic center x, y\n\n        for i, sample in enumerate(mosaic_samples):\n            # Load image\n            img = sample['img']\n            (h, w) = img.shape[:2]\n\n            # place img in img4\n            if i == 0:  # top left\n                img4 = np.full((s * 2, s * 2, img.shape[2]), 114, dtype=np.uint8)  # base image with 4 tiles\n                x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)\n                x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)\n            elif i == 1:  # top right\n                x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\n                x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n            elif i == 2:  # bottom left\n                x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)\n                x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, w, min(y2a - y1a, h)\n            elif i == 3:  # bottom right\n                x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)\n                x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n\n            img4[y1a:y2a, x1a:x2a] = img[y1b:y2b, x1b:x2b]  # img4[ymin:ymax, xmin:xmax]\n            padw = x1a - x1b\n            padh = y1a - y1b\n\n            # Labels\n            cls, bboxes = sample['cls'], sample['bboxes']\n            bboxes = xywhn2xyxy(bboxes, w, h, padw, padh)  # normalized xywh to pixel xyxy format\n\n            classes4.append(cls)\n            bboxes4.append(bboxes)\n\n            segments = sample['segments']\n            segments_is_list = isinstance(segments, list)\n            if segments_is_list:\n                segments = [xyn2xy(x, w, h, padw, padh) for x in segments]\n                segments4.extend(segments)\n            else:\n                segments = xyn2xy(segments, w, h, padw, padh)\n                segments4.append(segments)\n\n        # Concat/clip labels\n        classes4 = np.concatenate(classes4, 0)\n        bboxes4 = np.concatenate(bboxes4, 0)\n        bboxes4 = bboxes4.clip(0, 2 * s)\n\n        if segments_is_list:\n            for x in segments4:\n                np.clip(x, 0, 2 * s, out=x)\n        else:\n            segments4 = np.concatenate(segments4, 0)\n            segments4 = segments4.clip(0, 2 * s)\n\n        # Augment\n        sample_labels, sample_images, sample_masks = \\\n            self._pastin_sample_segments(img4, classes4, bboxes4, segments4, probability=0.5)\n\n        return sample_labels, sample_images, sample_masks\n\n    def _pastin_sample_segments(self, img, classes, bboxes, segments, probability=0.5):\n        # Implement Copy-Paste augmentation https://arxiv.org/abs/2012.07177, labels as nx5 np.array(cls, xyxy)\n        n = len(segments)\n        sample_labels = []\n        sample_images = []\n        sample_masks = []\n        if probability and n:\n            h, w, c = img.shape  # height, width, channels\n            for j in random.sample(range(n), k=round(probability * n)):\n                cls, l, s = classes[j], bboxes[j], segments[j]\n                box = (\n                    l[0].astype(int).clip(0, w - 1),\n                    l[1].astype(int).clip(0, h - 1),\n                    l[2].astype(int).clip(0, w - 1),\n                    l[3].astype(int).clip(0, h - 1),\n                )\n\n                if (box[2] &lt;= box[0]) or (box[3] &lt;= box[1]):\n                    continue\n\n                sample_labels.append(cls[0])\n\n                mask = np.zeros(img.shape, np.uint8)\n\n                cv2.drawContours(mask, [segments[j].astype(np.int32)], -1, (255, 255, 255), cv2.FILLED)\n                sample_masks.append(mask[box[1]: box[3], box[0]: box[2], :])\n\n                result = cv2.bitwise_and(src1=img, src2=mask)\n                i = result &gt; 0  # pixels to replace\n                mask[i] = result[i]  # cv2.imwrite('debug.jpg', img)  # debug\n                sample_images.append(mask[box[1]: box[3], box[0]: box[2], :])\n\n        return sample_labels, sample_images, sample_masks\n\n    def hsv_augment(self, sample, hgain=0.5, sgain=0.5, vgain=0.5):\n        image = sample['img']\n        r = np.random.uniform(-1, 1, 3) * [hgain, sgain, vgain] + 1  # random gains\n        hue, sat, val = cv2.split(cv2.cvtColor(image, cv2.COLOR_BGR2HSV))\n        dtype = image.dtype  # uint8\n\n        x = np.arange(0, 256, dtype=np.int16)\n        lut_hue = ((x * r[0]) % 180).astype(dtype)\n        lut_sat = np.clip(x * r[1], 0, 255).astype(dtype)\n        lut_val = np.clip(x * r[2], 0, 255).astype(dtype)\n\n        img_hsv = cv2.merge((cv2.LUT(hue, lut_hue), cv2.LUT(sat, lut_sat), cv2.LUT(val, lut_val))).astype(dtype)\n        cv2.cvtColor(img_hsv, cv2.COLOR_HSV2BGR, dst=image)  # Modify on the original image\n\n        sample['img'] = image\n        return sample\n\n    def fliplr(self, sample):\n        # flip image left-right\n        image = sample['img']\n        image = np.fliplr(image)\n        sample['img'] = image\n\n        # flip box\n        _, w = image.shape[:2]\n        bboxes, bbox_format = sample['bboxes'], sample['bbox_format']\n        if bbox_format == \"ltrb\":\n            if len(bboxes):\n                x1 = bboxes[:, 0].copy()\n                x2 = bboxes[:, 2].copy()\n                bboxes[:, 0] = w - x2\n                bboxes[:, 2] = w - x1\n        elif bbox_format == \"xywhn\":\n            if len(bboxes):\n                bboxes[:, 0] = 1 - bboxes[:, 0]\n        else:\n            raise NotImplementedError\n        sample['bboxes'] = bboxes\n\n        # flip seg\n        if self.return_segments:\n            segment_format, segments = sample['segment_format'], sample['segments']\n            assert segment_format == 'polygon', \\\n                f'FlipLR: The segment format should be polygon, but got {segment_format}'\n            assert isinstance(segments, np.ndarray), \\\n                f\"FlipLR: segments type expect numpy.ndarray, but got {type(segments)}; \" \\\n                f\"maybe you should resample_segments before that.\"\n\n            if len(segments):\n                segments[..., 0] = w - segments[..., 0]\n\n            sample['segments'] = segments\n\n        return sample\n\n    def letterbox(self, sample, new_shape=None, xywhn2xyxy_=True, scaleup=False, only_image=False, color=(114, 114, 114)):\n        # Resize and pad image while meeting stride-multiple constraints\n        if sample['bbox_format'] == 'ltrb':\n            xywhn2xyxy_ = False\n\n        if not new_shape:\n            new_shape = self.img_size\n\n        if isinstance(new_shape, int):\n            new_shape = (new_shape, new_shape)\n\n        image = sample['img']\n        shape = image.shape[:2]  # current shape [height, width]\n\n        h, w = shape[:]\n        ori_shape = sample['ori_shape']\n        h0, w0 = ori_shape\n        hw_scale = np.array([h / h0, w / w0])\n        sample['hw_scale'] = hw_scale\n\n        # Scale ratio (new / old)\n        r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n        if not scaleup:  # only scale down, do not scale up (for better test mAP)\n            r = min(r, 1.0)\n\n        # Compute padding\n        new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n        dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n\n        dw /= 2  # divide padding into 2 sides\n        dh /= 2\n        hw_pad = np.array([dh, dw])\n\n        if shape != new_shape:\n            if shape[::-1] != new_unpad:  # resize\n                image = cv2.resize(image, new_unpad, interpolation=cv2.INTER_LINEAR)\n            top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n            left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n            image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n            sample['hw_pad'] = hw_pad\n        else:\n            sample['hw_pad'] = np.array([0., 0.])\n        bboxes = sample['bboxes']\n        if not only_image:\n            # convert bboxes\n            if len(bboxes):\n                if xywhn2xyxy_:\n                    bboxes = xywhn2xyxy(bboxes, r * w, r * h, padw=dw, padh=dh)\n                else:\n                    bboxes *= r\n                    bboxes[:, [0, 2]] += dw\n                    bboxes[:, [1, 3]] += dh\n                sample['bboxes'] = bboxes\n            sample['bbox_format'] = 'ltrb'\n\n            # convert segments\n            if 'segments' in sample:\n                segments, segment_format = sample['segments'], sample['segment_format']\n                assert segment_format == 'polygon', f'The segment format should be polygon, but got {segment_format}'\n\n                if len(segments):\n                    if isinstance(segments, np.ndarray):\n                        if xywhn2xyxy_:\n                            segments[..., 0] *= w\n                            segments[..., 1] *= h\n                        else:\n                            segments *= r\n                        segments[..., 0] += dw\n                        segments[..., 1] += dh\n                    elif isinstance(segments, list):\n                        for segment in segments:\n                            if xywhn2xyxy_:\n                                segment[..., 0] *= w\n                                segment[..., 1] *= h\n                            else:\n                                segment *= r\n                            segment[..., 0] += dw\n                            segment[..., 1] += dh\n                    sample['segments'] = segments\n\n        sample['img'] = image\n        return sample\n\n    def label_norm(self, sample, xyxy2xywh_=True):\n        bbox_format = sample['bbox_format']\n        if bbox_format == \"xywhn\":\n            return sample\n\n        bboxes = sample['bboxes']\n        if len(bboxes) == 0:\n            sample['bbox_format'] = 'xywhn'\n            return sample\n\n        if xyxy2xywh_:\n            bboxes = xyxy2xywh(bboxes)  # convert xyxy to xywh\n        height, width = sample['img'].shape[:2]\n        bboxes[:, [1, 3]] /= height  # normalized height 0-1\n        bboxes[:, [0, 2]] /= width  # normalized width 0-1\n        sample['bboxes'] = bboxes\n        sample['bbox_format'] = 'xywhn'\n\n        return sample\n\n    def label_pad(self, sample, padding_size=160, padding_value=-1):\n        # create fixed label, avoid dynamic shape problem.\n        bbox_format = sample['bbox_format']\n        assert bbox_format == 'xywhn', f'The bbox format should be xywhn, but got {bbox_format}'\n\n        cls, bboxes = sample['cls'], sample['bboxes']\n        cls_pad = np.full((padding_size, 1), padding_value, dtype=np.float32)\n        bboxes_pad = np.full((padding_size, 4), padding_value, dtype=np.float32)\n        nL = len(bboxes)\n        if nL:\n            cls_pad[:min(nL, padding_size)] = cls[:min(nL, padding_size)]\n            bboxes_pad[:min(nL, padding_size)] = bboxes[:min(nL, padding_size)]\n        sample['cls'] = cls_pad\n        sample['bboxes'] = bboxes_pad\n\n        if \"segments\" in sample:\n            if sample['segment_format'] == \"mask\":\n                segments = sample['segments']\n                assert isinstance(segments, np.ndarray), \\\n                    f\"Label Pad: segments type expect numpy.ndarray, but got {type(segments)}; \" \\\n                    f\"maybe you should resample_segments before that.\"\n                assert nL == segments.shape[0], f\"Label Pad: segments len not equal bboxes\"\n                h, w = segments.shape[1:]\n                segments_pad = np.full((padding_size, h, w), padding_value, dtype=np.float32)\n                segments_pad[:min(nL, padding_size)] = segments[:min(nL, padding_size)]\n                sample['segments'] = segments_pad\n\n        return sample\n\n    def image_norm(self, sample, scale=255.0):\n        image = sample['img']\n        image = image.astype(np.float32, copy=False)\n        image /= scale\n        sample['img'] = image\n        return sample\n\n    def image_transpose(self, sample, bgr2rgb=True, hwc2chw=True):\n        image = sample['img']\n        if bgr2rgb:\n            image = image[:, :, ::-1]\n        if hwc2chw:\n            image = image.transpose(2, 0, 1)\n        sample['img'] = image\n        return sample\n\n    def segment_poly2mask(self, sample, mask_overlap, mask_ratio):\n\"\"\"convert polygon points to bitmap.\"\"\"\n        segments, segment_format = sample['segments'], sample['segment_format']\n        assert segment_format == 'polygon', f'The segment format should be polygon, but got {segment_format}'\n        assert isinstance(segments, np.ndarray), \\\n            f\"Segment Poly2Mask: segments type expect numpy.ndarray, but got {type(segments)}; \" \\\n            f\"maybe you should resample_segments before that.\"\n\n        h, w = sample['img'].shape[:2]\n        if mask_overlap:\n            masks, sorted_idx = polygons2masks_overlap((h, w), segments, downsample_ratio=mask_ratio)\n            sample['cls'] = sample['cls'][sorted_idx]\n            sample['bboxes'] = sample['bboxes'][sorted_idx]\n            sample['segments'] = masks  # (h/mask_ratio, w/mask_ratio)\n            sample['segment_format'] = 'overlap'\n        else:\n            masks = polygons2masks((h, w), segments, color=1, downsample_ratio=mask_ratio)\n            sample['segments'] = masks\n            sample['segment_format'] = 'mask'\n\n        return sample\n\n    def _img2label_paths(self, img_paths):\n        # Define label paths as a function of image paths\n        sa, sb = os.sep + \"images\" + os.sep, os.sep + \"labels\" + os.sep  # /images/, /labels/ substrings\n        return [\"txt\".join(x.replace(sa, sb, 1).rsplit(x.split(\".\")[-1], 1)) for x in img_paths]\n\n    def _get_hash(self, paths):\n        # Returns a single hash value of a list of paths (files or dirs)\n        size = sum(os.path.getsize(p) for p in paths if os.path.exists(p))  # sizes\n        h = hashlib.md5(str(size).encode())  # hash sizes\n        h.update(\"\".join(paths).encode())  # hash paths\n        return h.hexdigest()  # return hash\n\n    def _exif_size(self, img):\n        # Returns exif-corrected PIL size\n        s = img.size  # (width, height)\n        try:\n            rotation = dict(img._getexif().items())[orientation]\n            if rotation == 6:  # rotation 270\n                s = (s[1], s[0])\n            elif rotation == 8:  # rotation 90\n                s = (s[1], s[0])\n        except:\n            pass\n\n        return s\n\n    def train_collate_fn(self, im_file, cls, bboxes, segments, keypoints, bbox_format, \n                         segment_format, img, ori_shape, hw_scale, hw_pad, batch_info):\n        labels = []\n        for i, (c, b) in enumerate(zip(cls, bboxes)):\n            labels.append(np.concatenate((np.full_like(c, i), c, b), axis=-1))\n        return_items = [np.stack(img, 0), np.stack(labels, 0)]\n        if self.return_segments:\n            return_items.append(np.stack(segments, 0))\n        if self.return_keypoints:\n            return_items.append(np.stack(keypoints, 0))\n\n        return tuple(return_items)\n\n    def test_collate_fn(self, batch_samples, batch_info):\n        imgs = [sample.pop('img') for sample in batch_samples]\n        path = [sample.pop('im_file') for sample in batch_samples]\n        hw_ori = [sample.pop('ori_shape') for sample in batch_samples]\n        hw_scale = [sample.pop('hw_scale') for sample in batch_samples]\n        pad = [sample.pop('hw_pad') for sample in batch_samples]\n        return (\n            np.stack(imgs, 0),\n            path,\n            np.stack(hw_ori, 0),\n            np.stack(hw_scale, 0),\n            np.stack(pad, 0),\n        )\n</code></pre>"},{"location":"reference/data/#mindyolo.data.dataset.COCODataset.get_sample","title":"<code>mindyolo.data.dataset.COCODataset.get_sample(index)</code>","text":"<p>Get and return label information from the dataset.</p> Source code in <code>mindyolo/data/dataset.py</code> <pre><code>def get_sample(self, index):\n\"\"\"Get and return label information from the dataset.\"\"\"\n    sample = deepcopy(self.labels[index])\n    img = self.imgs[index]\n    if img is None:\n        path = self.img_files[index]\n        img = cv2.imread(path)  # BGR\n        assert img is not None, \"Image Not Found \" + path\n        h_ori, w_ori = img.shape[:2]  # orig hw\n        r = self.img_size / max(h_ori, w_ori)  # resize image to img_size\n        if r != 1:  # always resize down, only resize up if training with augmentation\n            interp = cv2.INTER_AREA if r &lt; 1 and not self.augment else cv2.INTER_LINEAR\n            img = cv2.resize(img, (int(w_ori * r), int(h_ori * r)), interpolation=interp)\n\n        if self.augment:\n            self.imgs[index], self.img_hw_ori[index] = img, np.array([h_ori, w_ori]) # img, hw_original\n            self.buffer.append(index)\n            if 1 &lt; len(self.buffer) &gt;= self.max_buffer_length:\n                j = self.buffer.pop(0)\n                self.imgs[j], self.img_hw_ori[j] = None, np.array([None, None])\n        sample['img'], sample['ori_shape'] = img, np.array([h_ori, w_ori])  # img, hw_original\n    else:\n        sample['img'], sample['ori_shape'] = self.imgs[index], self.img_hw_ori[index]  # img, hw_original\n\n    return sample\n</code></pre>"},{"location":"reference/data/#mindyolo.data.dataset.COCODataset.segment_poly2mask","title":"<code>mindyolo.data.dataset.COCODataset.segment_poly2mask(sample, mask_overlap, mask_ratio)</code>","text":"<p>convert polygon points to bitmap.</p> Source code in <code>mindyolo/data/dataset.py</code> <pre><code>def segment_poly2mask(self, sample, mask_overlap, mask_ratio):\n\"\"\"convert polygon points to bitmap.\"\"\"\n    segments, segment_format = sample['segments'], sample['segment_format']\n    assert segment_format == 'polygon', f'The segment format should be polygon, but got {segment_format}'\n    assert isinstance(segments, np.ndarray), \\\n        f\"Segment Poly2Mask: segments type expect numpy.ndarray, but got {type(segments)}; \" \\\n        f\"maybe you should resample_segments before that.\"\n\n    h, w = sample['img'].shape[:2]\n    if mask_overlap:\n        masks, sorted_idx = polygons2masks_overlap((h, w), segments, downsample_ratio=mask_ratio)\n        sample['cls'] = sample['cls'][sorted_idx]\n        sample['bboxes'] = sample['bboxes'][sorted_idx]\n        sample['segments'] = masks  # (h/mask_ratio, w/mask_ratio)\n        sample['segment_format'] = 'overlap'\n    else:\n        masks = polygons2masks((h, w), segments, color=1, downsample_ratio=mask_ratio)\n        sample['segments'] = masks\n        sample['segment_format'] = 'mask'\n\n    return sample\n</code></pre>"},{"location":"reference/data/#albumentations","title":"Albumentations","text":""},{"location":"reference/data/#mindyolo.data.albumentations.Albumentations","title":"<code>mindyolo.data.albumentations.Albumentations</code>","text":"Source code in <code>mindyolo/data/albumentations.py</code> <pre><code>class Albumentations:\n    # Implement Albumentations augmentation https://github.com/ultralytics/yolov5\n    # YOLOv5 Albumentations class (optional, only used if package is installed)\n    def __init__(self, size=640, random_resized_crop=True, **kwargs):\n        self.transform = None\n        prefix = _colorstr(\"albumentations: \")\n        try:\n            import albumentations as A\n\n            _check_version(A.__version__, \"1.0.3\", hard=True)  # version requirement\n            T = []\n            if random_resized_crop:\n                T.extend([\n                    A.RandomResizedCrop(height=size, width=size, scale=(0.8, 1.0), ratio=(0.9, 1.11), p=0.0),\n                ])\n            T.extend([\n                A.Blur(p=0.01),\n                A.MedianBlur(p=0.01),\n                A.ToGray(p=0.01),\n                A.CLAHE(p=0.01),\n                A.RandomBrightnessContrast(p=0.0),\n                A.RandomGamma(p=0.0),\n                A.ImageCompression(quality_lower=75, p=0.0),\n            ])\n            self.transform = A.Compose(T, bbox_params=A.BboxParams(format=\"yolo\", label_fields=[\"class_labels\"]))\n\n            print(prefix + \", \".join(f\"{x}\".replace(\"always_apply=False, \", \"\") for x in T if x.p), flush=True)\n            print(\"[INFO] albumentations load success\", flush=True)\n        except ImportError:  # package not installed, skip\n            pass\n            print(\"[WARNING] package not installed, albumentations load failed\", flush=True)\n        except Exception as e:\n            print(f\"{prefix}{e}\", flush=True)\n            print(\"[WARNING] albumentations load failed\", flush=True)\n\n    def __call__(self, sample, p=1.0, **kwargs):\n        if self.transform and random.random() &lt; p:\n            im, bboxes, cls, bbox_format = sample['img'], sample['bboxes'], sample['cls'], sample['bbox_format']\n            assert bbox_format in (\"ltrb\", \"xywhn\")\n            if bbox_format == \"ltrb\" and bboxes.shape[0] &gt; 0:\n                h, w = im.shape[:2]\n                bboxes = xyxy2xywh(bboxes)\n                bboxes[:, [0, 2]] /= w\n                bboxes[:, [1, 3]] /= h\n\n            new = self.transform(image=im, bboxes=bboxes, class_labels=cls)  # transformed\n\n            sample['img'] = new['image']\n            sample['bboxes'] = np.array(new['bboxes'])\n            sample['cls'] = np.array(new['class_labels']).reshape(-1, 1)\n            sample['bbox_format'] = \"xywhn\"\n\n        return sample\n</code></pre>"},{"location":"reference/models/","title":"Models","text":""},{"location":"reference/models/#create-model","title":"Create Model","text":""},{"location":"reference/models/#mindyolo.models.model_factory.create_model","title":"<code>mindyolo.models.model_factory.create_model(model_name, model_cfg=None, in_channels=3, num_classes=80, checkpoint_path='', **kwargs)</code>","text":"Source code in <code>mindyolo/models/model_factory.py</code> <pre><code>def create_model(\n    model_name: str,\n    model_cfg: dict = None,\n    in_channels: int = 3,\n    num_classes: int = 80,\n    checkpoint_path: str = \"\",\n    **kwargs,\n):\n    model_args = dict(cfg=model_cfg, num_classes=num_classes, in_channels=in_channels)\n    kwargs = {k: v for k, v in kwargs.items() if v is not None}\n\n    if not is_model(model_name):\n        raise RuntimeError(f\"Unknown model {model_name}\")\n\n    create_fn = model_entrypoint(model_name)\n    model = create_fn(**model_args, **kwargs)\n\n    if checkpoint_path:\n        assert os.path.isfile(checkpoint_path) and checkpoint_path.endswith(\n            \".ckpt\"\n        ), f\"[{checkpoint_path}] not a ckpt file.\"\n        checkpoint_param = load_checkpoint(checkpoint_path)\n        load_param_into_net(model, checkpoint_param)\n        logger.info(f\"Load checkpoint from [{checkpoint_path}] success.\")\n\n    return model\n</code></pre>"},{"location":"reference/models/#yolov3","title":"YOLOV3","text":""},{"location":"reference/models/#mindyolo.models.yolov3","title":"<code>mindyolo.models.yolov3(cfg, in_channels=3, num_classes=None, **kwargs)</code>","text":"<p>Get yolov3 model.</p> Source code in <code>mindyolo/models/yolov3.py</code> <pre><code>@register_model\ndef yolov3(cfg, in_channels=3, num_classes=None, **kwargs) -&gt; YOLOv3:\n\"\"\"Get yolov3 model.\"\"\"\n    model = YOLOv3(cfg=cfg, in_channels=in_channels, num_classes=num_classes, **kwargs)\n    return model\n</code></pre>"},{"location":"reference/models/#yolov4","title":"YOLOV4","text":""},{"location":"reference/models/#mindyolo.models.yolov4","title":"<code>mindyolo.models.yolov4(cfg, in_channels=3, num_classes=None, **kwargs)</code>","text":"<p>Get yolov4 model.</p> Source code in <code>mindyolo/models/yolov4.py</code> <pre><code>@register_model\ndef yolov4(cfg, in_channels=3, num_classes=None, **kwargs) -&gt; YOLOv4:\n\"\"\"Get yolov4 model.\"\"\"\n    model = YOLOv4(cfg=cfg, in_channels=in_channels, num_classes=num_classes, **kwargs)\n    return model\n</code></pre>"},{"location":"reference/models/#yolov5","title":"YOLOV5","text":""},{"location":"reference/models/#mindyolo.models.yolov5","title":"<code>mindyolo.models.yolov5(cfg, in_channels=3, num_classes=None, **kwargs)</code>","text":"<p>Get yolov5 model.</p> Source code in <code>mindyolo/models/yolov5.py</code> <pre><code>@register_model\ndef yolov5(cfg, in_channels=3, num_classes=None, **kwargs) -&gt; YOLOv5:\n\"\"\"Get yolov5 model.\"\"\"\n    model = YOLOv5(cfg=cfg, in_channels=in_channels, num_classes=num_classes, **kwargs)\n    return model\n</code></pre>"},{"location":"reference/models/#yolov7","title":"YOLOV7","text":""},{"location":"reference/models/#mindyolo.models.yolov7","title":"<code>mindyolo.models.yolov7(cfg, in_channels=3, num_classes=None, **kwargs)</code>","text":"<p>Get yolov7 model.</p> Source code in <code>mindyolo/models/yolov7.py</code> <pre><code>@register_model\ndef yolov7(cfg, in_channels=3, num_classes=None, **kwargs) -&gt; YOLOv7:\n\"\"\"Get yolov7 model.\"\"\"\n    model = YOLOv7(cfg=cfg, in_channels=in_channels, num_classes=num_classes, **kwargs)\n    return model\n</code></pre>"},{"location":"reference/models/#yolov8","title":"YOLOV8","text":""},{"location":"reference/models/#mindyolo.models.yolov8","title":"<code>mindyolo.models.yolov8(cfg, in_channels=3, num_classes=None, **kwargs)</code>","text":"<p>Get yolov8 model.</p> Source code in <code>mindyolo/models/yolov8.py</code> <pre><code>@register_model\ndef yolov8(cfg, in_channels=3, num_classes=None, **kwargs) -&gt; YOLOv8:\n\"\"\"Get yolov8 model.\"\"\"\n    model = YOLOv8(cfg=cfg, in_channels=in_channels, num_classes=num_classes, **kwargs)\n    return model\n</code></pre>"},{"location":"reference/models/#yolox","title":"YOLOX","text":""},{"location":"reference/models/#mindyolo.models.yolox","title":"<code>mindyolo.models.yolox(cfg, in_channels=3, num_classes=None, **kwargs)</code>","text":"<p>Get yolox model.</p> Source code in <code>mindyolo/models/yolox.py</code> <pre><code>@register_model\ndef yolox(cfg, in_channels=3, num_classes=None, **kwargs) -&gt; YOLOX:\n\"\"\"Get yolox model.\"\"\"\n    model = YOLOX(cfg, in_channels=in_channels, num_classes=num_classes, **kwargs)\n    return model\n</code></pre>"},{"location":"tutorials/configuration/","title":"Configuration","text":"<p>MindYOLO supports parameter parsing from both yaml files and command lines. The parameters which are fixed, complex, highly related to model or with nested structure are placed in yaml files. While the simpler ones or parameters variants with actual cases could be passed in from the command line.</p> <p>The following takes yolov3 as an example to explain how to configure the corresponding parameters.</p>"},{"location":"tutorials/configuration/#parameter-inheritance-relationship","title":"Parameter Inheritance Relationship","text":"<p>As follows, the parameter priority is from high to low. When a parameter with the same name appears, the low-priority parameter will be overwritten by the high-priority parameter.</p> <ul> <li>Parameters inputted with user command lines</li> <li>Default parameters set in parser from .py files</li> <li>Parameters in yaml files specified by user command lines</li> <li>Parameters in yaml files set by <code>__BASE__</code> contained in yaml files specified by user command lines. Take yolov3 as an example, it contains: <pre><code>__BASE__: [\n'../coco.yaml',\n'./hyp.scratch.yaml',\n]\n</code></pre></li> </ul>"},{"location":"tutorials/configuration/#basic-parameters","title":"Basic Parameters","text":""},{"location":"tutorials/configuration/#parameter-description","title":"Parameter Description","text":"<ul> <li>device_target: device used, Ascend/CPU</li> <li>save_dir: the path to save the running results, the default is ./runs</li> <li>log_interval: step interval to print logs, the default is 100</li> <li>is_parallel: whether to perform distributed training, the default is False</li> <li>ms_mode: whether to use static graph mode (0) or dynamic graph mode (1), the default is 0.</li> <li>config: yaml configuration file path</li> <li>per_batch_size: batch size of each card, default is 32</li> <li>epochs: number of training epochs, default is 300</li> <li>...</li> </ul>"},{"location":"tutorials/configuration/#parse-parameter-settings","title":"Parse parameter settings","text":"<p>This part of the parameters is usually passed in from the command line. Examples are as follows:</p> <pre><code>msrun --worker_num=8 --local_worker_num=8 --bind_core=True --log_dir=./yolov7_log python train.py --config ./configs/yolov7/yolov7.yaml  --is_parallel True --log_interval 50\n</code></pre>"},{"location":"tutorials/configuration/#dataset","title":"Dataset","text":""},{"location":"tutorials/configuration/#parameter-description_1","title":"Parameter Description","text":"<ul> <li>dataset_name: dataset name</li> <li>train_set: the path where the training set is located</li> <li>val_set: the path where the verification set is located</li> <li>test_set: the path where the test set is located</li> <li>nc: number of categories in the data set</li> <li>names: category name    -...</li> </ul>"},{"location":"tutorials/configuration/#yaml-file-sample","title":"Yaml file sample","text":"<p>This part of the parameters is defined in configs/coco.yaml, and the data set path usually needs to be modified.</p> <pre><code>data:\ndataset_name: coco\n\ntrain_set: ./coco/train2017.txt  # 118287 images\nval_set: ./coco/val2017.txt  # 5000 images\ntest_set: ./coco/test-dev2017.txt  # 20288 of 40670 images, submit to https://competitions.codalab.org/competitions/20794\n\nnc: 80\n\n# class names\nnames: [ 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n'hair drier', 'toothbrush' ]\n</code></pre>"},{"location":"tutorials/configuration/#data-augmentation","title":"Data Augmentation","text":""},{"location":"tutorials/configuration/#parameter-description_2","title":"Parameter Description","text":"<ul> <li>num_parallel_workers: number of worker processes reading data</li> <li>train_transformers: data enhancement during training process</li> <li>test_transformers: verification process data enhancement    -...</li> </ul>"},{"location":"tutorials/configuration/#yaml-file-sample_1","title":"Yaml file sample","text":"<p>This part of the parameters is defined in configs/yolov3/hyp.scratch.yaml, where train_transformers and test_transformers are lists composed of dictionaries, each dictionary contains data enhancement operations name, probability of occurrence and parameters related to the enhancement method</p> <pre><code>data:\nnum_parallel_workers: 8\n\ntrain_transforms:\n- { func_name: mosaic, prob: 1.0, mosaic9_prob: 0.0, translate: 0.1, scale: 0.9 }\n- { func_name: mixup, prob: 0.1, alpha: 8.0, beta: 8.0, needed_mosaic: True }\n- { func_name: hsv_augment, prob: 1.0, hgain: 0.015, sgain: 0.7, vgain: 0.4 }\n- { func_name: label_norm, xyxy2xywh_: True }\n- { func_name: albumentations }\n- { func_name: fliplr, prob: 0.5 }\n- { func_name: label_pad, padding_size: 160, padding_value: -1 }\n- { func_name: image_norm, scale: 255. }\n- { func_name: image_transpose, bgr2rgb: True, hwc2chw: True }\n\ntest_transforms:\n- { func_name: letterbox, scaleup: False }\n- { func_name: label_norm, xyxy2xywh_: True }\n- { func_name: label_pad, padding_size: 160, padding_value: -1 }\n- { func_name: image_norm, scale: 255. }\n- { func_name: image_transpose, bgr2rgb: True, hwc2chw: True }\n</code></pre>"},{"location":"tutorials/configuration/#model","title":"Model","text":""},{"location":"tutorials/configuration/#parameter-description_3","title":"Parameter Description","text":"<ul> <li>model_name: model name</li> <li>depth_multiple: model depth factor</li> <li>width_multiple: model width factor</li> <li>stride: feature map downsampling multiple</li> <li>anchors: default anchor box</li> <li>backbone: model backbone network</li> <li>head: model detection head</li> </ul>"},{"location":"tutorials/configuration/#yaml-file-sample_2","title":"Yaml file sample","text":"<p>This part of the parameters is defined in configs/yolov3/yolov3.yaml. The network is constructed based on the backbone and head parameters. The parameters are presented in the form of a nested list, with each line representing a The layer module contains 4 parameters, namely the input layer number (-1 represents the previous layer), the number of module repetitions, the module name and the corresponding parameters of the module. Users can also define and register networks directly in py files without resorting to yaml files. <pre><code>network:\nmodel_name: yolov3\n\ndepth_multiple: 1.0  # model depth multiple\nwidth_multiple: 1.0  # layer channel multiple\nstride: [8, 16, 32]\nanchors:\n- [10,13, 16,30, 33,23]  # P3/8\n- [30,61, 62,45, 59,119]  # P4/16\n- [116,90, 156,198, 373,326]  # P5/32\n\n# darknet53 backbone\nbackbone:\n# [from, number, module, args]\n[[-1, 1, ConvNormAct, [32, 3, 1]],  # 0\n[-1, 1, ConvNormAct, [64, 3, 2]],  # 1-P1/2\n[-1, 1, Bottleneck, [64]],\n[-1, 1, ConvNormAct, [128, 3, 2]],  # 3-P2/4\n[-1, 2, Bottleneck, [128]],\n[-1, 1, ConvNormAct, [256, 3, 2]],  # 5-P3/8\n[-1, 8, Bottleneck, [256]],\n[-1, 1, ConvNormAct, [512, 3, 2]],  # 7-P4/16\n[-1, 8, Bottleneck, [512]],\n[-1, 1, ConvNormAct, [1024, 3, 2]],  # 9-P5/32\n[-1, 4, Bottleneck, [1024]],  # 10\n]\n\n# YOLOv3 head\nhead:\n[[-1, 1, Bottleneck, [1024, False]],\n[-1, 1, ConvNormAct, [512, 1, 1]],\n[-1, 1, ConvNormAct, [1024, 3, 1]],\n[-1, 1, ConvNormAct, [512, 1, 1]],\n[-1, 1, ConvNormAct, [1024, 3, 1]],  # 15 (P5/32-large)\n\n[-2, 1, ConvNormAct, [256, 1, 1]],\n[-1, 1, Upsample, [None, 2, 'nearest']],\n[[-1, 8], 1, Concat, [1]],  # cat backbone P4\n[-1, 1, Bottleneck, [512, False]],\n[-1, 1, Bottleneck, [512, False]],\n[-1, 1, ConvNormAct, [256, 1, 1]],\n[-1, 1, ConvNormAct, [512, 3, 1]],  # 22 (P4/16-medium)\n\n[-2, 1, ConvNormAct, [128, 1, 1]],\n[-1, 1, Upsample, [None, 2, 'nearest']],\n[[-1, 6], 1, Concat, [1]],  # cat backbone P3\n[-1, 1, Bottleneck, [256, False]],\n[-1, 2, Bottleneck, [256, False]],  # 27 (P3/8-small)\n\n[[27, 22, 15], 1, YOLOv3Head, [nc, anchors, stride]],   # Detect(P3, P4, P5)\n]\n</code></pre></p>"},{"location":"tutorials/configuration/#loss-function","title":"Loss function","text":""},{"location":"tutorials/configuration/#parameter-description_4","title":"Parameter Description","text":"<ul> <li>name: loss function name</li> <li>box: box loss weight</li> <li>cls: class loss weight</li> <li>cls_pw: class loss positive sample weight</li> <li>obj: object loss weight</li> <li>obj_pw: object loss positive sample weight</li> <li>fl_gamma: focal loss gamma</li> <li>anchor_t: anchor shape proportion threshold</li> <li>label_smoothing: label smoothing value</li> </ul>"},{"location":"tutorials/configuration/#yaml-file-sample_3","title":"Yaml file sample","text":"<p>This part of the parameters is defined in configs/yolov3/hyp.scratch.yaml</p> <pre><code>loss:\nname: YOLOv7Loss\nbox: 0.05  # box loss gain\ncls: 0.5  # cls loss gain\ncls_pw: 1.0  # cls BCELoss positive_weight\nobj: 1.0  # obj loss gain (scale with pixels)\nobj_pw: 1.0  # obj BCELoss positive_weight\nfl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\nanchor_t: 4.0  # anchor-multiple threshold\nlabel_smoothing: 0.0 # label smoothing epsilon\n</code></pre>"},{"location":"tutorials/configuration/#optimizer","title":"Optimizer","text":""},{"location":"tutorials/configuration/#parameter-description_5","title":"Parameter Description","text":"<ul> <li>optimizer: optimizer name.</li> <li>lr_init: initial value of learning rate</li> <li>warmup_epochs: number of warmup epochs</li> <li>warmup_momentum: initial value of warmup momentum</li> <li>warmup_bias_lr: initial value of warmup bias learning rate</li> <li>min_warmup_step: minimum number of warmup steps</li> <li>group_param: parameter grouping strategy</li> <li>gp_weight_decay: Group parameter weight decay coefficient</li> <li>start_factor: initial learning rate factor</li> <li>end_factor: end learning rate factor</li> <li>momentum: momentum of the moving average</li> <li>loss_scale: loss scaling coefficient</li> <li>nesterov: Whether to use the Nesterov Accelerated Gradient (NAG) algorithm to update the gradient.</li> </ul>"},{"location":"tutorials/configuration/#yaml-file-sample_4","title":"Yaml file sample","text":"<p>This part of the parameters is defined in configs/yolov3/hyp.scratch.yaml. In the following example, the initial learning rate after the warmup stage is lr_init * start_factor = 0.01 * 1.0 = 0.01, the final learning rate is lr_init * end_factor = 0.01 * 0.01 = 0.0001</p> <pre><code>optimizer:\noptimizer: momentum\nlr_init: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\nmomentum: 0.937  # SGD momentum/Adam beta1\nnesterov: True # update gradients with NAG(Nesterov Accelerated Gradient) algorithm\nloss_scale: 1.0 # loss scale for optimizer\nwarmup_epochs: 3  # warmup epochs (fractions ok)\nwarmup_momentum: 0.8  # warmup initial momentum\nwarmup_bias_lr: 0.1  # warmup initial bias lr\nmin_warmup_step: 1000 # minimum warmup step\ngroup_param: yolov7 # group param strategy\ngp_weight_decay: 0.0005  # group param weight decay 5e-4\nstart_factor: 1.0\nend_factor: 0.01\n</code></pre>"},{"location":"tutorials/data_augmentation/","title":"Data Augmentation","text":""},{"location":"tutorials/data_augmentation/#list-of-data-enhancement-methods-that-come-with-the-package","title":"List of data enhancement methods that come with the package","text":"Data Enhancement Method Name Summary Explanation mosaic randomly select mosaic4 and mosaic9 mosaic4 4-part splicing mosaic9 9-point splicing mixup linearly mix two images pastein clipping enhancement random_perspective random perspective transformation hsv_augment random color transformation fliplr flip horizontally flipud vertical flip letterbox scale and fill label_norm label normalization and coordinates normalized to 0-1 to range label_pad fill label information into a fixed-size array image_norm image data normalization image_transpose channel transpose and dimension transpose albumentations albumentations data enhancement <p>These data augmentation functions are defined in mindyolo/data/dataset.py.</p>"},{"location":"tutorials/data_augmentation/#instructions","title":"Instructions","text":"<p>The MindYOLO data enhancement method is configured in the yaml file. For example, to add a data enhancement during the training process, you need to add a dictionary list under the data.train_transforms field of the yaml file. The data enhancement methods are listed from top to bottom.</p> <p>A typical data enhancement method configuration dictionary must have func_name, which represents the name of the applied data enhancement method, and then lists the parameters that need to be set for the method. If no parameter item is configured in the data enhancement configuration dictionary, the data enhancement method will be selected by default. value.</p> <p>Data enhancement common configuration dictionary: <pre><code>- {func_name: data enhancement method name 1, args11=x11, args12=x12, ..., args1n=x1n}\n- {func_name: data enhancement method name 2, args21=x21, args22=x22, ..., args2n=x2n}\n...\n- {func_name: data enhancement method name n, argsn1=xn1, argsn2=xn2, ..., argsnn=xnn}\n</code></pre></p> <p>Example enhanced with YOLOv7 training data: <pre><code>#File directory: configs/yolov7/hyp.scratch.tiny.yaml (https://github.com/mindspore-lab/mindyolo/blob/master/configs/yolov7/hyp.scratch.tiny.yaml)\ntrain_transforms:\n- {func_name: mosaic, prob: 1.0, mosaic9_prob: 0.2, translate: 0.1, scale: 0.5}\n- {func_name: mixup, prob: 0.05, alpha: 8.0, beta: 8.0, needed_mosaic: True}\n- {func_name: hsv_augment, prob: 1.0, hgain: 0.015, sgain: 0.7, vgain: 0.4}\n- {func_name: pastein, prob: 0.05, num_sample: 30}\n- {func_name: label_norm, xyxy2xywh_: True}\n- {func_name: fliplr, prob: 0.5}\n- {func_name: label_pad, padding_size: 160, padding_value: -1}\n- {func_name: image_norm, scale: 255.}\n- {func_name: image_transpose, bgr2rgb: True, hwc2chw: True}\n</code></pre> Note: func_name represents the name of the data enhancement method, prob, mosaic9_prob, translate, and scale are the method parameters. Among them, prob is a parameter common to all methods, indicating the execution probability of the data enhancement method. The default value is 1</p> <p>The specific operations performed by the above yaml file are as follows:</p> <ul> <li> <p><code>mosaic</code>: Perform mosaic operation on the input image with a probability of 1.0, that is, splicing 4 different images into one image. mosaic9_prob represents the probability of splicing using the 9-square grid method, and translate and scale represent the degree of random translation and scaling respectively. as the picture shows: </p> </li> <li> <p><code>mixup</code>: Perform a mixup operation on the input image with a probability of 0.05, that is, mix two different images. Among them, alpha and beta represent the mixing coefficient, and needed_mosaic represents whether mosaic needs to be used for mixing.</p> </li> <li> <p><code>hsv_augment</code>: HSV enhancement, adjust the HSV color space of the input image with a probability of 1.0 to increase data diversity. Among them, hgain, sgain and vgain represent the degree of adjustment of H, S and V channels respectively.</p> </li> <li> <p><code>pastein</code>: randomly paste some samples into the input image with a probability of 0.05. Among them, num_sample represents the number of randomly posted samples.</p> </li> <li> <p><code>label_norm</code>: Convert the input label from the format of (x1, y1, x2, y2) to the format of (x, y, w, h).</p> </li> <li> <p><code>fliplr</code>: Flip the input image horizontally with a probability of 0.5 to increase data diversity.</p> </li> <li> <p><code>label_pad</code>: Pad the input labels so that each image has the same number of labels. padding_size represents the number of labels after padding, and padding_value represents the value of padding.</p> </li> <li> <p><code>image_norm</code>: Scale the input image pixel value from the range [0, 255] to the range [0, 1].</p> </li> <li> <p><code>image_transpose</code>: Convert the input image from BGR format to RGB format, and convert the number of channels of the image from HWC format to CHW format.</p> </li> </ul> <p>Test data enhancement needs to be marked with the test_transforms field, and the configuration method is the same as training.</p>"},{"location":"tutorials/data_augmentation/#custom-data-enhancement","title":"Custom data enhancement","text":"<p>Writing Guide:</p> <ul> <li>Add custom data enhancement methods to the COCODataset class in the mindyolo/data/dataset.py file</li> <li>Inputs to data augmentation methods usually include images, labels, and custom parameters. -Write function body content and customize output</li> </ul> <p>A typical data enhancement method: <pre><code>#Add submethods in mindyolo/data/dataset.py COCODataset\ndef data_trans_func(self, image, labels, args1=x1, args2=x2, ..., argsn=xn):\n    # Data enhancement logic\n    ...\n    return image, labels\n</code></pre> Customize a data enhancement function whose function is rotation <pre><code>#mindyolo/data/dataset.py\ndef rotate(self, image, labels, angle):\n    # rotate image\n    image = np.rot90(image, angle // 90)\n    if len(labels):\n        if angle == 90:\n            labels[:, 0], labels[:, 1] = 1 - labels[:, 1], labels[:, 0]\n        elif angle == 180:\n            labels[:, 0], labels[:, 1] = 1 - labels[:, 0], 1 - labels[:, 1]\n        elif angle == 270:\n            labels[:, 0], labels[:, 1] = labels[:, 1], 1 - labels[:, 0]\n    return image, labels\n</code></pre></p> <p>user's guidance: - Define this data augmentation method in the form of a dictionary in the model's yaml file. Same usage as described above <pre><code>    - {func_name: rotate, angle: 90}\n</code></pre></p> <p>Show results:</p> <p> </p>"},{"location":"tutorials/deployment/","title":"Deployment","text":""},{"location":"tutorials/deployment/#dependencies","title":"Dependencies","text":"<pre><code>pip install -r requirement.txt\n</code></pre>"},{"location":"tutorials/deployment/#mindspore-lite-environment-preparation","title":"MindSpore Lite environment preparation","text":"<p>Reference: Lite environment configuration     Note: The python environment that MindSpore Lite is adapted to is 3.7. Please prepare the python3.7 environment before installing Lite </p> <ol> <li> <p>Depending on the environment, download the matching tar.gz package and whl package.</p> </li> <li> <p>Unzip the tar.gz package and install the corresponding version of the whl package    <pre><code>tar -zxvf mindspore_lite-2.0.0a0-cp37-cp37m-{os}_{platform}_64.tar.gz\npip install mindspore_lite-2.0.0a0-cp37-cp37m-{os}_{platform}_64.whl\n</code></pre></p> </li> <li>Configure Lite environment variables    LITE_HOME is the folder path extracted from tar.gz. It is recommended to use the absolute path.    <pre><code>export LITE_HOME=/path/to/mindspore-lite-{version}-{os}-{platform}\nexport LD_LIBRARY_PATH=$LITE_HOME/runtime/lib:$LITE_HOME/tools/converter/lib:$LD_LIBRARY_PATH\nexport PATH=$LITE_HOME/tools/converter/converter:$LITE_HOME/tools/benchmark:$PATH\n</code></pre></li> </ol>"},{"location":"tutorials/deployment/#quick-start","title":"Quick Start","text":""},{"location":"tutorials/deployment/#model-conversion","title":"Model conversion","text":"<p>Convert ckpt model to mindir model, this step can be run on CPU/Ascend910    <pre><code>python ./deploy/export.py --config ./path_to_config/model.yaml --weight ./path_to_ckpt/weight.ckpt --per_batch_size 1 --file_format MINDIR --device_target [CPU/Ascend]\ne.g.\n#Run on CPU\npython ./deploy/export.py --config ./configs/yolov5/yolov5n.yaml --weight yolov5n_300e_mAP273-9b16bd7b.ckpt --per_batch_size 1 --file_format MINDIR --device_target CPU\n# Run on Ascend\npython ./deploy/export.py --config ./configs/yolov5/yolov5n.yaml --weight yolov5n_300e_mAP273-9b16bd7b.ckpt --per_batch_size 1 --file_format MINDIR --device_target Ascend\n</code></pre></p>"},{"location":"tutorials/deployment/#lite-test","title":"Lite Test","text":"<pre><code>python deploy/test.py --model_type Lite --model_path ./path_to_mindir/weight.mindir --config ./path_to_config/yolo.yaml\ne.g.\npython deploy/test.py --model_type Lite --model_path ./yolov5n.mindir --config ./configs/yolov5/yolov5n.yaml\n</code></pre>"},{"location":"tutorials/deployment/#lite-predict","title":"Lite Predict","text":"<pre><code>python ./deploy/predict.py --model_type Lite --model_path ./path_to_mindir/weight.mindir --config ./path_to_conifg/yolo.yaml --image_path ./path_to_image/image.jpg\ne.g.\npython deploy/predict.py --model_type Lite --model_path ./yolov5n.mindir --config ./configs/yolov5/yolov5n.yaml --image_path ./coco/image/val2017/image.jpg\n</code></pre>"},{"location":"tutorials/deployment/#script-description","title":"Script description","text":"<ul> <li>predict.py supports single image inference</li> <li>test.py supports COCO data set inference</li> <li>Note: currently only supports inference on Ascend 310</li> </ul>"},{"location":"tutorials/deployment/#mindx-deployment","title":"MindX Deployment","text":""},{"location":"tutorials/deployment/#environment-configuration","title":"Environment configuration","text":"<p>Reference: MindX environment preparation  Note: MindX currently supports python version 3.9. Please prepare the python3.9 environment before installing MindX </p> <ol> <li> <p>Obtain the [Environment Installation Package] (https://www.hiascend.com/software/mindx-sdk/commercial) from the MindX official website. Currently, version 3.0.0 of MindX infer is supported.</p> </li> <li> <p>Jump to the Download page Download Ascend-mindxsdk-mxmanufacture_{version}_linux-{arch}.run</p> </li> <li> <p>Place the installation package in the Ascend310 machine directory and unzip it</p> </li> <li> <p>If you are not a root user, you need to add executable permissions to the package: <pre><code>chmod +x Ascend-mindxsdk-mxmanufacture_{version}_linux-{arch}.run\n</code></pre></p> </li> <li>Enter the upload path of the development kit package and install the mxManufacture development kit package. <pre><code>./Ascend-mindxsdk-mxmanufacture_{version}_linux-{arch}.run --install\n</code></pre> After the installation is completed, if the following echo appears, it means that the software was successfully installed. <pre><code>The installation is successful\n</code></pre> After the installation is complete, the mxManufacture software directory structure is as follows: <pre><code>.\n\u251c\u2500\u2500 bin\n\u251c\u2500\u2500 config\n\u251c\u2500\u2500 filelist.txt\n\u251c\u2500\u2500 include\n\u251c\u2500\u2500 lib\n\u251c\u2500\u2500 opensource\n\u251c\u2500\u2500 operators\n\u251c\u2500\u2500 python\n\u251c\u2500\u2500 samples\n\u251c\u2500\u2500 set_env.sh\n\u251c\u2500\u2500 toolkit\n\u2514\u2500\u2500 version.info\n</code></pre></li> <li>Enter the installation directory of mxmanufacture and run the following command to make the MindX SDK environment variables take effect. <pre><code>source set_env.sh\n</code></pre></li> <li>Enter ./mxVision-3.0.0/python/ and install mindx-3.0.0-py3-none-any.whl <pre><code>pip install mindx-3.0.0-py3-none-any.whl\n</code></pre></li> </ol>"},{"location":"tutorials/deployment/#model-conversion_1","title":"Model conversion","text":"<ol> <li> <p>Convert ckpt model to air model. This step needs to be performed on Ascend910.    <pre><code>python ./deploy/export.py --config ./path_to_config/model.yaml --weight ./path_to_ckpt/weight.ckpt --per_batch_size 1 --file_format AIR\ne.g.\npython ./deploy/export.py --config ./configs/yolov5/yolov5n.yaml --weight yolov5n_300e_mAP273-9b16bd7b.ckpt --per_batch_size 1 --file_format AIR\n</code></pre> yolov7 needs to run export on an Ascend910 machine with version 2.0 or above</p> </li> <li> <p>To convert the air model to the om model, use atc conversion tool. This step requires the installation of MindX Environment, running on Ascend310    <pre><code>atc --model=./path_to_air/weight.air --framework=1 --output=yolo --soc_version=Ascend310\n</code></pre></p> </li> </ol>"},{"location":"tutorials/deployment/#mindx-test","title":"MindX Test","text":"<p>Infer COCO data:    <pre><code>python ./deploy/test.py --model_type MindX --model_path ./path_to_om/weight.om --config ./path_to_config/yolo.yaml\ne.g.\npython ./deploy/test.py --model_type MindX --model_path ./yolov5n.om --config ./configs/yolov5/yolov5n.yaml\n</code></pre></p>"},{"location":"tutorials/deployment/#mindx-predict","title":"MindX Predict","text":"<p>Infer a single image:    <pre><code>python ./deploy/predict.py --model_type MindX --model_path ./path_to_om/weight.om --config ./path_to_config/yolo.yaml --image_path ./path_to_image/image.jpg\ne.g.\npython ./deploy/predict.py --model_type MindX --model_path ./yolov5n.om --config ./configs/yolov5/yolov5n.yaml --image_path ./coco/image/val2017/image.jpg\n</code></pre></p>"},{"location":"tutorials/deployment/#mindir-deployment","title":"MindIR Deployment","text":""},{"location":"tutorials/deployment/#environmental-requirements","title":"Environmental requirements","text":"<p>mindspore&gt;=2.1</p>"},{"location":"tutorials/deployment/#precautions","title":"Precautions","text":"<ol> <li> <p>Currently only supports Predict</p> </li> <li> <p>Theoretically, it can also run on Ascend910, but it has not been tested.</p> </li> </ol>"},{"location":"tutorials/deployment/#model-conversion_2","title":"Model conversion","text":"<p>Convert the ckpt model to the mindir model, this step can be run on the CPU    <pre><code>python ./deploy/export.py --config ./path_to_config/model.yaml --weight ./path_to_ckpt/weight.ckpt --per_batch_size 1 --file_format MINDIR --device_target CPU\ne.g.\n#Run on CPU\npython ./deploy/export.py --config ./configs/yolov5/yolov5n.yaml --weight yolov5n_300e_mAP273-9b16bd7b.ckpt --per_batch_size 1 --file_format MINDIR --device_target CPU\n</code></pre></p>"},{"location":"tutorials/deployment/#mindir-test","title":"MindIR Test","text":"<p>Coming soon</p>"},{"location":"tutorials/deployment/#mindir-predict","title":"MindIR Predict","text":"<p>Infer a single image:    <pre><code>python ./deploy/predict.py --model_type MindIR --model_path ./path_to_mindir/weight.mindir --config ./path_to_conifg/yolo.yaml --image_path ./path_to_image/image.jpg\ne.g.\npython deploy/predict.py --model_type MindIR --model_path ./yolov5n.mindir --config ./configs/yolov5/yolov5n.yaml --image_path ./coco/image/val2017/image.jpg\n</code></pre></p>"},{"location":"tutorials/deployment/#onnx-deployment","title":"ONNX deployment","text":""},{"location":"tutorials/deployment/#environment-configuration_1","title":"Environment configuration","text":"<pre><code>pip install onnx&gt;=1.9.0\npip install onnxruntime&gt;=1.8.0\n</code></pre>"},{"location":"tutorials/deployment/#precautions_1","title":"Precautions","text":"<ol> <li> <p>Currently not all mindyolo supports ONNX export and inference (only YoloV3 is used as an example)</p> </li> <li> <p>Currently only supports the Predict function</p> </li> <li> <p>Exporting ONNX requires adjusting the nn.SiLU operator and using the underlying implementation of the sigmoid operator.</p> </li> </ol> <p>For example: add the following custom layer and replace all nn.SiLU in mindyolo <pre><code>class EdgeSiLU(nn.Cell):\n\"\"\"\n    SiLU activation function: x * sigmoid(x). To support for onnx export with nn.SiLU.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    def construct(self, x):\n        return x * ops.sigmoid(x)\n</code></pre></p>"},{"location":"tutorials/deployment/#model-conversion_3","title":"Model conversion","text":"<p>Convert the ckpt model to an ONNX model. This step and the Test step can only be run on the CPU.    <pre><code>python ./deploy/export.py --config ./path_to_config/model.yaml --weight ./path_to_ckpt/weight.ckpt --per_batch_size 1 --file_format ONNX --device_target [CPU]\ne.g.\n#Run on CPU\npython ./deploy/export.py --config ./configs/yolov3/yolov3.yaml --weight yolov3-darknet53_300e_mAP455-adfb27af.ckpt --per_batch_size 1 --file_format ONNX --device_target CPU\n</code></pre></p>"},{"location":"tutorials/deployment/#onnx-test","title":"ONNX Test","text":"<p>Coming soon</p>"},{"location":"tutorials/deployment/#onnxruntime-predict","title":"ONNXRuntime Predict","text":"<p>Infer a single image:    <pre><code>python ./deploy/predict.py --model_type ONNX --model_path ./path_to_onnx_model/model.onnx --config ./path_to_config/yolo.yaml --image_path ./path_to_image/image.jpg\ne.g.\npython ./deploy/predict.py --model_type ONNX --model_path ./yolov3.onnx --config ./configs/yolov3/yolov3.yaml --image_path ./coco/image/val2017/image.jpg\n</code></pre></p>"},{"location":"tutorials/deployment/#standard-and-supported-model-libraries","title":"Standard and supported model libraries","text":"<ul> <li> YOLOv8</li> <li> YOLOv7</li> <li> YOLOX</li> <li> YOLOv5</li> <li> YOLOv4</li> <li> YOLOv3</li> </ul> Name Scale Context ImageSize Dataset Box mAP (%) Params FLOPs Recipe Download YOLOv8 N D310x1-G 640 MS COCO 2017 37.2 3.2M 8.7G yaml ckpt mindir YOLOv8 S D310x1-G 640 MS COCO 2017 44.6 11.2M 28.6G yaml ckpt mindir YOLOv8 M D310x1-G 640 MS COCO 2017 50.5 25.9M 78.9G yaml ckpt mindir YOLOv8 L D310x1-G 640 MS COCO 2017 52.8 43.7M 165.2G yaml ckpt mindir YOLOv8 X D310x1-G 640 MS COCO 2017 53.7 68.2M 257.8G yaml ckpt mindir YOLOv7 Tiny D310x1-G 640 MS COCO 2017 37.5 6.2M 13.8G yaml ckpt mindir YOLOv7 L D310x1-G 640 MS COCO 2017 50.8 36.9M 104.7G yaml ckpt mindir YOLOv7 X D310x1-G 640 MS COCO 2017 52.4 71.3M 189.9G yaml ckpt mindir YOLOv5 N D310x1-G 640 MS COCO 2017 27.3 1.9M 4.5G yaml ckpt mindir YOLOv5 S D310x1-G 640 MS COCO 2017 37.6 7.2M 16.5G yaml ckpt mindir YOLOv5 M D310x1-G 640 MS COCO 2017 44.9 21.2M 49.0G yaml ckpt mindir YOLOv5 L D310x1-G 640 MS COCO 2017 48.5 46.5M 109.1G yaml ckpt mindir YOLOv5 X D310x1-G 640 MS COCO 2017 50.5 86.7M 205.7G yaml ckpt mindir YOLOv4 CSPDarknet53 D310x1-G 608 MS COCO 2017 45.4 27.6M 52G yaml ckpt mindir YOLOv4 CSPDarknet53(silu) D310x1-G 640 MS COCO 2017 45.8 27.6M 52G yaml ckpt mindir YOLOv3 Darknet53 D310x1-G 640 MS COCO 2017 45.5 61.9M 156.4G yaml ckpt mindir YOLOX N D310x1-G 416 MS COCO 2017 24.1 0.9M 1.1G yaml ckpt mindir YOLOX Tiny D310x1-G 416 MS COCO 2017 33.3 5.1M 6.5G yaml ckpt mindir YOLOX S D310x1-G 640 MS COCO 2017 40.7 9.0M 26.8G yaml ckpt mindir YOLOX M D310x1-G 640 MS COCO 2017 46.7 25.3M 73.8G yaml ckpt mindir YOLOX L D310x1-G 640 MS COCO 2017 49.2 54.2M 155.6G yaml ckpt mindir YOLOX X D310x1-G 640 MS COCO 2017 51.6 99.1M 281.9G yaml ckpt mindir YOLOX Darknet53 D310x1-G 640 MS COCO 2017 47.7 63.7M 185.3G yaml ckpt mindir"},{"location":"tutorials/finetune/","title":"Fine-tuning","text":""},{"location":"tutorials/finetune/#custom-dataset-finetune-process","title":"Custom Dataset Finetune Process","text":"<p>This article takes the Safety Hat Wearing Detection Dataset (SHWD) as an example to introduce the main process of finetune on MindYOLO with a custom data set.</p>"},{"location":"tutorials/finetune/#dataset-conversion","title":"Dataset Conversion","text":"<p>SHWD Dataset uses data labels in voc format, and its file directory is as follows: <pre><code>             Root directory\n                \u251c\u2500\u2500 Comments\n                \u2502 \u251c\u2500\u2500 000000.xml\n                \u2502 \u2514\u2500\u2500 000002.xml\n                \u251c\u2500\u2500 Image Collection\n                \u2502 \u2514\u2500\u2500 Main\n                \u2502 \u251c\u2500\u2500 test.txt\n                \u2502 \u251c\u2500\u2500 train.txt\n                \u2502 \u251c\u2500\u2500 trainval.txt\n                \u2502 \u2514\u2500\u2500 val.txt\n                \u2514\u2500\u2500 JPEG image\n                        \u251c\u2500\u2500 000000.jpg\n                        \u2514\u2500\u2500 000002.jpg\n</code></pre> The xml file under the Annotations folder contains annotation information for each picture. The main contents are as follows: <pre><code>&lt;annotation&gt;\n  &lt;folder&gt;JPEGImages&lt;/folder&gt;\n  &lt;filename&gt;000377.jpg&lt;/filename&gt;\n  &lt;path&gt;F:\\baidu\\VOC2028\\JPEGImages\\000377.jpg&lt;/path&gt;\n  &lt;source&gt;\n    &lt;database&gt;Unknown&lt;/database&gt;\n  &lt;/source&gt;\n  &lt;size&gt;\n    &lt;width&gt;750&lt;/width&gt;\n    &lt;height&gt;558&lt;/height&gt;\n    &lt;depth&gt;3&lt;/depth&gt;\n  &lt;/size&gt;\n  &lt;segmented&gt;0&lt;/segmented&gt;\n  &lt;object&gt;\n    &lt;name&gt;hat&lt;/name&gt;\n    &lt;pose&gt;Unspecified&lt;/pose&gt;\n    &lt;truncated&gt;0&lt;/truncated&gt;\n    &lt;difficult&gt;0&lt;/difficult&gt;\n    &lt;bndbox&gt;\n      &lt;xmin&gt;142&lt;/xmin&gt;\n      &lt;ymin&gt;388&lt;/ymin&gt;\n      &lt;xmax&gt;177&lt;/xmax&gt;\n      &lt;ymax&gt;426&lt;/ymax&gt;\n    &lt;/bndbox&gt;\n  &lt;/object&gt;\n</code></pre> It contains multiple objects. The name in object is the category name, and xmin, ymin, xmax, and ymax are the coordinates of the upper left corner and lower right corner of the detection frame.</p> <p>The data set format supported by MindYOLO is YOLO format. For details, please refer to Data Preparation</p> <p>Since MindYOLO selects the image name as image_id during the verification phase, the image name can only be of numeric type, not of string type, and the image needs to be renamed. Conversion to SHWD data set format includes the following steps: * Copy the image to the corresponding path and rename it * Write the relative path of the image in the corresponding txt file in the root directory * Parse the xml file and generate the corresponding txt annotation file under the corresponding path * The verification set also needs to generate the final json file</p> <p>For detailed implementation, please refer to convert_shwd2yolo.py. The operation method is as follows:</p> <p><pre><code>python examples/finetune_SHWD/convert_shwd2yolo.py --root_dir /path_to_shwd/SHWD\n</code></pre> Running the above command will generate a SHWD data set in yolo format in the same directory without changing the original data set.</p>"},{"location":"tutorials/finetune/#write-yaml-configuration-file","title":"Write yaml configuration file","text":"<p>The configuration file mainly contains the corresponding parameters related to the data set, data enhancement, loss, optimizer, and model structure. Since MindYOLO provides a yaml file inheritance mechanism, you can only write the parameters that need to be adjusted as yolov7-tiny_shwd.yaml and inherit the native ones provided by MindYOLO. yaml file, its content is as follows: <pre><code>__BASE__: [\n  '../../configs/yolov7/yolov7-tiny.yaml',\n]\n\nper_batch_size: 16 # Single card batchsize, total batchsize=per_batch_size * device_num\nimg_size: 640 # image sizes\nweight: ./yolov7-tiny_pretrain.ckpt\nstrict_load: False # Whether to strictly load the internal parameters of ckpt. The default is True. If set to False, when the number of classifications is inconsistent, the weight of the last layer of classifiers will be discarded.\nlog_interval: 10 #Print the loss result every log_interval iterations\n\ndata:\n  dataset_name: shwd\n  train_set: ./SHWD/train.txt # Actual training data path\n  val_set: ./SHWD/val.txt\n  test_set: ./SHWD/val.txt\n  nc: 2 # Number of categories\n  # class names\n  names: [ 'person', 'hat' ] # The name of each category\n\noptimizer:\n  lr_init: 0.001 # initial learning rate\n</code></pre> * <code>__BASE__</code> is a list, indicating the path of the inherited yaml file. Multiple yaml files can be inherited. * per_batch_size and img_size respectively represent the batch_size on a single card and the image size used for data processing images. * weight is the file path of the pre-trained model mentioned above, and strict_load means discarding parameters with inconsistent shapes. * log_interval represents the log printing interval * All parameters under the data field are data set related parameters, where dataset_name is the name of the custom data set, train_set, val_set, and test_set are the txt file paths that save the training set, validation set, and test set image paths respectively, nc is the number of categories, and names is classification name * lr_init under the optimizer field is the initial learning rate after warm_up, which is 10 times smaller than the default parameters.</p> <p>For parameter inheritance relationship and parameter description, please refer to Configuration.</p>"},{"location":"tutorials/finetune/#download-pre-trained-model","title":"Download pre-trained model","text":"<p>You can choose the Model Warehouse provided by MindYOLO as the pre-training model for the custom data set. The pre-training model already has better accuracy performance on the COCO data set. Compared with training from scratch, loading a pre-trained model will generally have faster convergence speed and higher final accuracy, and will most likely avoid problems such as gradient disappearance and gradient explosion caused by improper initialization.</p> <p>The number of categories in the custom data set is usually inconsistent with the COCO data set. The detection head structure of each model in MindYOLO is related to the number of categories in the data set. Directly importing the pre-trained model may fail due to inconsistent shape. You can configure it in the yaml configuration file. Set the strict_load parameter to False, MindYOLO will automatically discard parameters with inconsistent shapes and throw a warning that the module parameter is not imported.</p>"},{"location":"tutorials/finetune/#model-fine-tuning-finetune","title":"Model fine-tuning (Finetune)","text":"<p>During the process of model fine-tuning, you can first train according to the default configuration. If the effect is not good, you can consider adjusting the following parameters: * The learning rate can be adjusted smaller to prevent loss from being difficult to converge. * per_batch_size can be adjusted according to the actual video memory usage. Generally, the larger per_batch_size is, the more accurate the gradient calculation will be. * Epochs can be adjusted according to whether the loss converges * Anchor can be adjusted according to the actual object size</p> <p>Since the SHWD training set only has about 6,000 images, the yolov7-tiny model was selected for training. * Distributed model training on multi-card NPU, taking 8 cards as an example:</p> <pre><code>msrun --worker_num=8 --local_worker_num=8 --bind_core=True --log_dir=./yolov7-tiny_log python train.py --config ./examples/finetune_SHWD/yolov7-tiny_shwd.yaml --is_parallel True\n</code></pre> <ul> <li>Train the model on a single card NPU/CPU:</li> </ul> <p><pre><code>python train.py --config ./examples/finetune_SHWD/yolov7-tiny_shwd.yaml\n</code></pre> *Note: Directly using the default parameters of yolov7-tiny to train on the SHWD data set can achieve an accuracy of AP50 87.0. Changing the lr_init parameter from 0.01 to 0.001 can achieve an accuracy result of ap50 of 89.2. *</p>"},{"location":"tutorials/finetune/#visual-reasoning","title":"Visual reasoning","text":"<p>Use /demo/predict.py to use the trained model for visual reasoning. The operation method is as follows:</p> <p><pre><code>python demo/predict.py --config ./examples/finetune_SHWD/yolov7-tiny_shwd.yaml --weight=/path_to_ckpt/WEIGHT.ckpt --image_path /path_to_image/IMAGE.jpg\n</code></pre> The reasoning effect is as follows:</p>"},{"location":"tutorials/modelarts/","title":"MindYOLO ModelArts Training Quick Start","text":"<p>This article mainly introduces the training method of MindYOLO using the ModelArts platform. ModelArts related tutorial reference Help Center</p>"},{"location":"tutorials/modelarts/#prepare-data-and-code","title":"Prepare data and code","text":"<p>Use the OBS service to upload data sets. For related operation tutorials, see OBS User Guide to obtain the [AK] of this account (https:// docs.xckpjs.com/zh-cn/browsertg/obs/obs_03_1007.html), please consult the corresponding platform administrator or account person in charge for the server address. If the AK is not in the location specified in the user guide, please also consult the platform administrator or account person in charge. .  operate:</p> <ol> <li>Log in to obs browser+  </li> <li>Create a bucket -&gt; create a new folder (eg: coco)  </li> <li>To upload data files, please place the data files in a separate folder (that is, coco in the use case). The code will copy the data in the obs bucket, and the copied content will be all files in this folder (such as coco). document. Without creating a new folder, you cannot select the complete data set.</li> </ol> <p></p>"},{"location":"tutorials/modelarts/#prepare-code","title":"Prepare code","text":"<p>Also use the OBS service to upload the training code.  Operation: Create a bucket -&gt; Create a new folder (such as: mindyolo) -&gt; Upload the code file, create an output folder at the same level of mindyolo to store training records, and create a log folder to store logs.   </p>"},{"location":"tutorials/modelarts/#create-new-algorithm","title":"Create new algorithm","text":"<ol> <li>Select Algorithm Management-&gt;Create in the tab.  </li> <li>Customize the algorithm name, select Ascend-Powered-Engine for the prefabricated framework, select the MindSpore-2.0 version image for the master branch, and select the MindSpore-1.8.1 version image for the r0.1 branch. Set the code directory, startup file, input, and output. and superparameters.  </li> </ol> <ul> <li>If you need to load pre-trained weights, you can select the uploaded model file in the model selection and add the ckpt_dir parameter in the running parameters.  </li> <li>The startup file is train.py</li> <li>To run super parameters, enable_modelarts needs to be added, and the value is True.</li> <li>The running super parameter config path refers to the directory of the running environment preview in the training job, such as /home/ma-user/modelarts/user-job-dir/mindyolo/configs/yolov5/yolov5n.yaml</li> <li>If distributed training scenarios are involved, the hyperparameter is_parallel needs to be added and set to True when running in distributed mode and False when running on a single card.</li> </ul>"},{"location":"tutorials/modelarts/#create-new-job","title":"Create new job","text":"<ol> <li>Select in the ModelArts service: Training Management -&gt; Training Jobs -&gt; Create a training job, set the job name, and choose not to include it in the experiment; Create Method -&gt; My Algorithm, select the newly created algorithm;   </li> <li>Training input -&gt; Data storage location, select the obs data bucket just created (coco in the example), select the output folder when preparing the code for training output, and set the config hyperparameter value according to the running environment preview;  </li> <li>Select the resource pool, specifications, computing nodes, and select the log folder when creating the code for the job log path.   </li> <li>Submit training and it will be running after queuing.</li> </ol>"},{"location":"tutorials/modelarts/#modify-job","title":"Modify job","text":"<p>Select Rebuild on the training job page to modify the selected job configuration.</p>"},{"location":"tutorials/quick_start/","title":"Quick Start","text":""},{"location":"tutorials/quick_start/#getting-started-with-mindyolo","title":"Getting Started with MindYOLO","text":"<p>This document provides a brief introduction to the usage of built-in command-line tools in MindYOLO.</p>"},{"location":"tutorials/quick_start/#inference-demo-with-pre-trained-models","title":"Inference Demo with Pre-trained Models","text":"<ol> <li>Pick a model and its config file from the Model Zoo, such as, <code>./configs/yolov7/yolov7.yaml</code>.</li> <li>Download the corresponding pre-trained checkpoint from the Model Zoo of each model.</li> <li>To run YOLO object detection with the built-in configs, please run:</li> </ol> <pre><code># Run with Ascend (By default)\npython demo/predict.py --config ./configs/yolov7/yolov7.yaml --weight=/path_to_ckpt/WEIGHT.ckpt --image_path /path_to_image/IMAGE.jpg\n</code></pre> <p>For details of the command line arguments, see <code>demo/predict.py -h</code> or look at its source code. to understand their behavior. Some common arguments are: * To run on cpu, modify device_target to CPU. * The results will be saved in <code>./detect_results</code></p>"},{"location":"tutorials/quick_start/#training-evaluation-in-command-line","title":"Training &amp; Evaluation in Command Line","text":"<ul> <li>Prepare your dataset in YOLO format. If training with COCO (YOLO format), please prepare it from yolov5 or the darknet.</li> </ul> <pre><code>  coco/\n    {train,val}2017.txt\n    annotations/\n      instances_{train,val}2017.json\n    images/\n      {train,val}2017/\n          00000001.jpg\n          ...\n          # image files that are mentioned in the corresponding train/val2017.txt\n    labels/\n      {train,val}2017/\n          00000001.txt\n          ...\n          # label files that are mentioned in the corresponding train/val2017.txt\n</code></pre> <ul> <li> <p>To train a model on 8 NPUs:   <pre><code>msrun --worker_num=8 --local_worker_num=8 --bind_core=True --log_dir=./yolov7_log python train.py --config ./configs/yolov7/yolov7.yaml  --is_parallel True\n</code></pre></p> </li> <li> <p>To train a model on 1 NPU/CPU:   <pre><code>python train.py --config ./configs/yolov7/yolov7.yaml </code></pre></p> </li> <li> <p>To evaluate a model's performance on 1 NPU/CPU:   <pre><code>python test.py --config ./configs/yolov7/yolov7.yaml --weight /path_to_ckpt/WEIGHT.ckpt\n</code></pre></p> </li> <li>To evaluate a model's performance 8 NPUs:   <pre><code>msrun --worker_num=8 --local_worker_num=8 --bind_core=True --log_dir=./yolov7_log python test.py --config ./configs/yolov7/yolov7.yaml --weight /path_to_ckpt/WEIGHT.ckpt --is_parallel True\n</code></pre> Notes:</li> </ul> <p>(1) The default hyper-parameter is used for 8-card training, and some parameters need to be adjusted in the case of a single card.</p> <p>(2) The default device is Ascend, and you can modify it by specifying 'device_target' as Ascend/CPU, as these are currently supported.</p> <p>(3) For more options, see <code>train/test.py -h</code>.</p> <p>(4) To train on CloudBrain, see here</p>"},{"location":"tutorials/quick_start/#deployment","title":"Deployment","text":"<p>See here.</p>"},{"location":"tutorials/quick_start/#to-use-mindyolo-apis-in-your-code","title":"To use MindYOLO APIs in Your Code","text":"<p>Coming soon.</p>"},{"location":"zh/","title":"MindYOLO","text":"<p>MindYOLO\u57fa\u4e8emindspore\u5b9e\u73b0\u4e86\u6700\u65b0\u7684YOLO\u7cfb\u5217\u7b97\u6cd5\u3002\u4ee5\u4e0b\u662fmindyolo\u7684\u5206\u652f\u4e0emindspore\u7248\u672c\u7684\u5bf9\u5e94\u5173\u7cfb\uff1a</p> mindyolo mindspore master master 0.5 2.5.0 0.4 2.3.0/2.3.1 0.3 2.2.10 0.2 2.0 0.1 1.8 <p></p>"},{"location":"zh/#_1","title":"\u6a21\u578b\u4ed3\u5e93\u548c\u57fa\u51c6","text":"<p>\u8be6\u89c1 \u6a21\u578b\u4ed3\u5e93\u8868\u683c</p>"},{"location":"zh/#_2","title":"\u652f\u6301\u6a21\u578b\u5217\u8868","text":"<ul> <li> YOLOv10 (\u6b22\u8fce\u5f00\u6e90\u8d21\u732e\u8005\u53c2\u4e0e\u5f00\u53d1)</li> <li> YOLOv9 (\u6b22\u8fce\u5f00\u6e90\u8d21\u732e\u8005\u53c2\u4e0e\u5f00\u53d1)</li> <li> YOLOv8</li> <li> YOLOv7</li> <li> YOLOX</li> <li> YOLOv5</li> <li> YOLOv4</li> <li> YOLOv3</li> </ul>"},{"location":"zh/#_3","title":"\u5b89\u88c5","text":"<p>\u8be6\u89c1 \u5b89\u88c5</p>"},{"location":"zh/#_4","title":"\u5feb\u901f\u5f00\u59cb","text":"<p>\u8be6\u89c1 \u5feb\u901f\u5f00\u59cb</p>"},{"location":"zh/#_5","title":"\u8bf4\u660e","text":"<p>\u26a0\ufe0f \u5f53\u524d\u7248\u672c\u57fa\u4e8e \u56fe\u6a21\u5f0f\u9759\u6001shape\u5f00\u53d1\u3002 \u52a8\u6001shape\u5c06\u5728\u540e\u7eed\u652f\u6301\uff0c\u656c\u8bf7\u671f\u5f85\u3002</p>"},{"location":"zh/#_6","title":"\u53c2\u4e0e\u9879\u76ee","text":"<p>\u4e3a\u4e86\u8ba9mindyolo\u66f4\u52a0\u5b8c\u5584\u548c\u4e30\u5bcc\uff0c\u6211\u4eec\u6b22\u8fce\u5305\u62ecissue\u548cpr\u5728\u5185\u7684\u4efb\u4f55\u5f00\u6e90\u8d21\u732e\u3002</p> <p>\u8bf7\u53c2\u8003 \u53c2\u4e0e\u9879\u76ee \u83b7\u53d6\u63d0\u4f9b\u5f00\u6e90\u8d21\u732e\u7684\u76f8\u5173\u6307\u5bfc\u3002</p>"},{"location":"zh/#_7","title":"\u8bb8\u53ef","text":"<p>MindYOLO\u57fa\u4e8e Apache License 2.0 \u53d1\u5e03\u3002</p>"},{"location":"zh/#_8","title":"\u987b\u77e5","text":"<p>MindYOLO \u662f\u4e00\u4e2a\u5f00\u6e90\u9879\u76ee\uff0c\u6211\u4eec\u6b22\u8fce\u4efb\u4f55\u8d21\u732e\u548c\u53cd\u9988\u3002\u6211\u4eec\u5e0c\u671b\u8be5mindyolo\u80fd\u591f\u901a\u8fc7\u63d0\u4f9b\u7075\u6d3b\u4e14\u6807\u51c6\u5316\u7684\u5de5\u5177\u5305\u6765\u652f\u6301\u4e0d\u65ad\u58ee\u5927\u7684\u7814\u7a76\u793e\u533a\uff0c\u91cd\u73b0\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u81ea\u5df1\u7684\u65b0\u5b9e\u65f6\u5bf9\u8c61\u68c0\u6d4b\u65b9\u6cd5\u3002</p>"},{"location":"zh/#_9","title":"\u5f15\u7528","text":"<p>\u5982\u679c\u60a8\u53d1\u73b0\u8be5\u9879\u76ee\u5bf9\u60a8\u7684\u7814\u7a76\u6709\u7528\uff0c\u8bf7\u8003\u8651\u5f15\u7528\uff1a</p> <pre><code>@misc{MindSpore Object Detection YOLO 2023,\n    title={{MindSpore Object Detection YOLO}:MindSpore Object Detection YOLO Toolbox and Benchmark},\n    author={MindSpore YOLO Contributors},\n    howpublished = {\\url{https://github.com/mindspore-lab/mindyolo}},\n    year={2023}\n}\n</code></pre>"},{"location":"zh/installation/","title":"\u5b89\u88c5","text":""},{"location":"zh/installation/#_2","title":"\u4f9d\u8d56","text":"<ul> <li>mindspore &gt;= 2.3</li> <li>numpy &gt;= 1.17.0</li> <li>pyyaml &gt;= 5.3</li> <li>openmpi 4.0.3 (\u5206\u5e03\u5f0f\u8bad\u7ec3\u6240\u9700)</li> </ul> <p>\u5982\u9700\u5b89\u88c5<code>python</code>\u76f8\u5173\u5e93\u4f9d\u8d56\uff0c\u53ea\u9700\u8fd0\u884c\uff1a</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>\u5982\u9700\u5b89\u88c5MindSpore\uff0c\u4f60\u53ef\u4ee5\u901a\u8fc7\u9075\u5faa\u5b98\u65b9\u6307\u5f15\uff0c\u5728\u4e0d\u540c\u7684\u786c\u4ef6\u5e73\u53f0\u4e0a\u83b7\u5f97\u6700\u4f18\u7684\u5b89\u88c5\u4f53\u9a8c\u3002 \u4e3a\u4e86\u5728\u5206\u5e03\u5f0f\u6a21\u5f0f\u4e0b\u8fd0\u884c\uff0c\u60a8\u8fd8\u9700\u8981\u5b89\u88c5OpenMPI\u3002</p> <p>\u26a0\ufe0f \u5f53\u524d\u7248\u672c\u4ec5\u652f\u6301Ascend\u5e73\u53f0\uff0cGPU\u4f1a\u5728\u540e\u7eed\u652f\u6301\uff0c\u656c\u8bf7\u671f\u5f85\u3002</p>"},{"location":"zh/installation/#pypi","title":"PyPI\u6e90\u5b89\u88c5","text":"<p>MindYOLO \u73b0\u5df2\u53d1\u5e03\u4e3a\u4e00\u4e2a<code>Python\u5305</code>\u5e76\u80fd\u591f\u901a\u8fc7<code>pip</code>\u8fdb\u884c\u5b89\u88c5\u3002\u6211\u4eec\u63a8\u8350\u60a8\u5728<code>\u865a\u62df\u73af\u5883</code>\u5b89\u88c5\u4f7f\u7528\u3002 \u6253\u5f00\u7ec8\u7aef\uff0c\u8f93\u5165\u4ee5\u4e0b\u6307\u4ee4\u6765\u5b89\u88c5 MindYOLO:</p> <pre><code>pip install mindyolo\n</code></pre>"},{"location":"zh/installation/#_3","title":"\u6e90\u7801\u5b89\u88c5 (\u672a\u7ecf\u6d4b\u8bd5\u7248\u672c)","text":""},{"location":"zh/installation/#vsc","title":"\u901a\u8fc7VSC\u5b89\u88c5","text":"<pre><code>pip install git+https://github.com/mindspore-lab/mindyolo.git\n</code></pre>"},{"location":"zh/installation/#src","title":"\u901a\u8fc7\u672c\u5730src\u5b89\u88c5","text":"<p>\u7531\u4e8e\u672c\u9879\u76ee\u5904\u4e8e\u6d3b\u8dc3\u5f00\u53d1\u9636\u6bb5\uff0c\u5982\u679c\u60a8\u662f\u5f00\u53d1\u8005\u6216\u8005\u8d21\u732e\u8005\uff0c\u8bf7\u4f18\u5148\u9009\u62e9\u6b64\u5b89\u88c5\u65b9\u5f0f\u3002</p> <p>MindYOLO \u53ef\u4ee5\u5728\u7531 <code>GitHub</code> \u514b\u9686\u4ed3\u5e93\u5230\u672c\u5730\u6587\u4ef6\u5939\u540e\u76f4\u63a5\u4f7f\u7528\u3002 \u8fd9\u5bf9\u4e8e\u60f3\u4f7f\u7528\u6700\u65b0\u7248\u672c\u7684\u5f00\u53d1\u8005\u5341\u5206\u65b9\u4fbf:</p> <pre><code>git clone https://github.com/mindspore-lab/mindyolo.git\n</code></pre> <p>\u5728\u514b\u9686\u5230\u672c\u5730\u4e4b\u540e\uff0c\u63a8\u8350\u60a8\u4f7f\u7528\"\u53ef\u7f16\u8f91\"\u6a21\u5f0f\u8fdb\u884c\u5b89\u88c5\uff0c\u8fd9\u6709\u52a9\u4e8e\u89e3\u51b3\u6f5c\u5728\u7684\u6a21\u5757\u5bfc\u5165\u95ee\u9898\u3002</p> <pre><code>cd mindyolo\npip install -e .\n</code></pre> <p>\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u9009\u7684 fast coco api \u63a5\u53e3\u7528\u4e8e\u63d0\u5347\u9a8c\u8bc1\u8fc7\u7a0b\u7684\u901f\u5ea6\u3002\u4ee3\u7801\u662f\u4ee5C++\u5f62\u5f0f\u63d0\u4f9b\u7684\uff0c\u53ef\u4ee5\u5c1d\u8bd5\u7528\u4ee5\u4e0b\u7684\u547d\u4ee4\u8fdb\u884c\u5b89\u88c5 (\u6b64\u64cd\u4f5c\u662f\u53ef\u9009\u7684) :</p> <pre><code>cd mindyolo/csrc\nsh build.sh\n</code></pre> <p>\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u57fa\u4e8eMindSpore Custom\u81ea\u5b9a\u4e49\u7b97\u5b50 \u7684GPU\u878d\u5408\u7b97\u5b50\uff0c\u7528\u4e8e\u63d0\u5347\u8bad\u7ec3\u8fc7\u7a0b\u7684\u901f\u5ea6\u3002\u4ee3\u7801\u91c7\u7528C++\u548cCUDA\u5f00\u53d1\uff0c\u4f4d\u4e8e<code>examples/custom_gpu_op/</code>\u8def\u5f84\u4e0b\u3002\u60a8\u53ef\u53c2\u8003\u793a\u4f8b\u811a\u672c<code>examples/custom_gpu_op/iou_loss_fused.py</code>\uff0c\u4fee\u6539<code>mindyolo/models/losses/iou_loss.py</code>\u7684<code>bbox_iou</code>\u65b9\u6cd5\uff0c\u5728GPU\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u8be5\u7279\u6027\u3002\u8fd0\u884c<code>iou_loss_fused.py</code>\u524d\uff0c\u9700\u8981\u4f7f\u7528\u4ee5\u4e0b\u7684\u547d\u4ee4\uff0c\u7f16\u8bd1\u751f\u6210GPU\u878d\u5408\u7b97\u5b50\u8fd0\u884c\u6240\u4f9d\u8d56\u7684\u52a8\u6001\u5e93 (\u6b64\u64cd\u4f5c\u5e76\u975e\u5fc5\u9700) :</p> <pre><code>bash examples/custom_gpu_op/fused_op/build.sh\n</code></pre>"},{"location":"zh/how_to_guides/callback/","title":"MindYOLO\u56de\u8c03\u51fd\u6570\u7528\u6cd5","text":"<p>\u56de\u8c03\u51fd\u6570\uff1a\u5f53\u7a0b\u5e8f\u8fd0\u884c\u5230\u67d0\u4e2a\u6302\u8f7d\u70b9\u65f6\uff0c\u4f1a\u81ea\u52a8\u8c03\u7528\u5728\u8fd0\u884c\u65f6\u6ce8\u518c\u5230\u8be5\u6302\u8f7d\u70b9\u7684\u6240\u6709\u65b9\u6cd5\u3002 \u901a\u8fc7\u56de\u8c03\u51fd\u6570\u7684\u5f62\u5f0f\u53ef\u4ee5\u589e\u52a0\u7a0b\u5e8f\u7684\u7075\u6d3b\u6027\u548c\u6269\u5c55\u6027\uff0c\u56e0\u4e3a\u7528\u6237\u53ef\u4ee5\u5c06\u81ea\u5b9a\u4e49\u65b9\u6cd5\u6ce8\u518c\u5230\u8981\u8c03\u7528\u7684\u6302\u8f7d\u70b9\uff0c\u800c\u65e0\u9700\u4fee\u6539\u7a0b\u5e8f\u4e2d\u7684\u4ee3\u7801\u3002</p> <p>\u5728MindYOLO\u4e2d\uff0c\u56de\u8c03\u51fd\u6570\u5177\u4f53\u5b9e\u73b0\u5728mindyolo/utils/callback.py\u6587\u4ef6\u4e2d\u3002 <pre><code>#mindyolo/utils/callback.py\n@CALLBACK_REGISTRY.registry_module()\nclass callback_class_name(BaseCallback):\n\n    def __init__(self, **kwargs):\n        super().__init__()\n        ...\n    def callback_fn_name(self, run_context: RunContext):\n        pass\n</code></pre></p> <p>\u901a\u8fc7\u6a21\u578b\u7684yaml\u6587\u4ef6callback\u5b57\u6bb5\u4e0b\u6dfb\u52a0\u4e00\u4e2a\u5b57\u5178\u5217\u8868\u6765\u5b9e\u73b0\u8c03\u7528 <pre><code>#\u56de\u8c03\u51fd\u6570\u914d\u7f6e\u5b57\u5178\uff1a\ncallback:\n- { name: callback_class_name, args: xx }\n- { name: callback_class_name2, args: xx }\n</code></pre> \u4f8b\u5982\u4ee5YOLOX\u4e3a\u793a\u4f8b\uff1a</p> <p>\u5728mindyolo/utils/callback.py\u6587\u4ef6YoloxSwitchTrain\u7c7b\u4e2don_train_step_begin\u65b9\u6cd5\u91cc\u9762\u6dfb\u52a0\u903b\u8f91\uff0c\u6253\u5370\u201ctrain step begin\u201d\u7684\u65e5\u5fd7 <pre><code>@CALLBACK_REGISTRY.registry_module()\nclass YoloxSwitchTrain(BaseCallback):\n\n    def on_train_step_begin(self, run_context: RunContext):\n         # \u81ea\u5b9a\u4e49\u903b\u8f91\n        logger.info(\"train step begin\")\n        pass\n</code></pre> YOLOX\u5bf9\u5e94\u7684yaml\u6587\u4ef6configs/yolox/hyp.scratch.yaml\u7684callback\u5b57\u6bb5\u4e0b\u6dfb\u52a0\u8be5\u56de\u8c03\u51fd\u6570 <pre><code>callback:\n- { name: YoloxSwitchTrain, switch_epoch_num: 285 }\n</code></pre> \u5219\u6bcf\u4e2a\u8bad\u7ec3step\u6267\u884c\u524d\u90fd\u4f1a\u6267\u884clogger.info(\"train step begin\")\u8bed\u53e5\u3002</p> <p>\u501f\u52a9\u56de\u8c03\u51fd\u6570\uff0c\u7528\u6237\u53ef\u4ee5\u81ea\u5b9a\u4e49\u67d0\u4e2a\u6302\u8f7d\u70b9\u9700\u8981\u6267\u884c\u7684\u903b\u8f91\uff0c\u800c\u65e0\u9700\u7406\u89e3\u5b8c\u6574\u7684\u8bad\u7ec3\u6d41\u7a0b\u7684\u4ee3\u7801\u3002</p>"},{"location":"zh/how_to_guides/data_preparation/","title":"\u6570\u636e\u51c6\u5907","text":""},{"location":"zh/how_to_guides/data_preparation/#_2","title":"\u6570\u636e\u96c6\u683c\u5f0f\u4ecb\u7ecd","text":"<p>\u4e0b\u8f7dcoco2017 YOLO\u683c\u5f0f coco2017labels-segments \u4ee5\u53cacoco2017 \u539f\u59cb\u56fe\u7247 train2017 , val2017 \uff0c\u7136\u540e\u5c06coco2017 \u539f\u59cb\u56fe\u7247\u653e\u5230coco2017 YOLO\u683c\u5f0f images\u76ee\u5f55\u4e0b\uff1a <pre><code>\u2514\u2500 coco2017_yolo\n    \u251c\u2500 annotations\n    \u2502    \u2514\u2500 instances_val2017.json\n    \u251c\u2500 images\n    \u2502    \u251c\u2500 train2017   # coco2017 \u539f\u59cb\u56fe\u7247\n    \u2502    \u2514\u2500 val2017     # coco2017 \u539f\u59cb\u56fe\u7247\n    \u251c\u2500 labels\n    \u2502    \u251c\u2500 train2017\n    \u2502    \u2514\u2500 val2017\n    \u251c\u2500 train2017.txt\n    \u251c\u2500 val2017.txt\n    \u2514\u2500 test-dev2017.txt\n</code></pre> \u5176\u4e2dtrain.txt\u6587\u4ef6\u6bcf\u884c\u5bf9\u5e94\u5355\u5f20\u56fe\u7247\u7684\u76f8\u5bf9\u8def\u5f84\uff0c\u4f8b\u5982\uff1a <pre><code>./images/train2017/00000000.jpg\n./images/train2017/00000001.jpg\n./images/train2017/00000002.jpg\n./images/train2017/00000003.jpg\n./images/train2017/00000004.jpg\n./images/train2017/00000005.jpg\n</code></pre> labels\u4e0b\u7684train2017\u6587\u4ef6\u5939\u4e0b\u7684txt\u6587\u4ef6\u4e3a\u76f8\u5e94\u56fe\u7247\u7684\u6807\u6ce8\u4fe1\u606f\uff0c\u652f\u6301detect\u548csegment\u4e24\u79cd\u683c\u5f0f\u3002</p> <p>detect\u683c\u5f0f\uff1a\u901a\u5e38\u6bcf\u884c\u67095\u5217\uff0c\u5206\u522b\u5bf9\u5e94\u7c7b\u522bid\u4ee5\u53ca\u6807\u6ce8\u6846\u5f52\u4e00\u5316\u4e4b\u540e\u7684\u4e2d\u5fc3\u70b9\u5750\u6807xy\u548c\u5bbd\u9ad8wh <pre><code>62 0.417040 0.206280 0.403600 0.412560\n62 0.818810 0.197933 0.174740 0.189680\n39 0.684540 0.277773 0.086240 0.358960\n0 0.620220 0.725853 0.751680 0.525840\n63 0.197190 0.364053 0.394380 0.669653\n39 0.932330 0.226240 0.034820 0.076640\n</code></pre> segment\u683c\u5f0f\uff1a\u6bcf\u884c\u7b2c\u4e00\u4e2a\u6570\u636e\u4e3a\u7c7b\u522bid\uff0c\u540e\u7eed\u4e3a\u4e24\u4e24\u6210\u5bf9\u7684\u5f52\u4e00\u5316\u5750\u6807\u70b9x,y</p> <p><pre><code>45 0.782016 0.986521 0.937078 0.874167 0.957297 0.782021 0.950562 0.739333 0.825844 0.561792 0.714609 0.420229 0.657297 0.391021 0.608422 0.4 0.0303438 0.750562 0.0016875 0.811229 0.003375 0.889896 0.0320156 0.986521\n45 0.557859 0.143813 0.487078 0.0314583 0.859547 0.00897917 0.985953 0.130333 0.984266 0.184271 0.930344 0.386521 0.80225 0.480896 0.763484 0.485396 0.684266 0.39775 0.670781 0.3955 0.679219 0.310104 0.642141 0.253937 0.561234 0.155063 0.559547 0.137083\n50 0.39 0.727063 0.418234 0.649417 0.455297 0.614125 0.476469 0.614125 0.51 0.590583 0.54 0.569417 0.575297 0.562354 0.601766 0.56 0.607062 0.536479 0.614125 0.522354 0.637063 0.501167 0.665297 0.48 0.69 0.477646 0.698828 0.494125 0.698828 0.534125 0.712938 0.529417 0.742938 0.548229 0.760594 0.564708 0.774703 0.550583 0.778234 0.536479 0.781766 0.531771 0.792359 0.541167 0.802937 0.555292 0.802937 0.569417 0.802937 0.576479 0.822359 0.576479 0.822359 0.597646 0.811766 0.607062 0.811766 0.618833 0.818828 0.637646 0.820594 0.656479 0.827641 0.687063 0.827641 0.703521 0.829406 0.727063 0.838234 0.708229 0.852359 0.729417 0.868234 0.750583 0.871766 0.792938 0.877063 0.821167 0.884125 0.861167 0.817062 0.92 0.734125 0.976479 0.711172 0.988229 0.48 0.988229 0.494125 0.967063 0.517062 0.912937 0.508234 0.832937 0.485297 0.788229 0.471172 0.774125 0.395297 0.729417\n45 0.375219 0.0678333 0.375219 0.0590833 0.386828 0.0503542 0.424156 0.0315208 0.440797 0.0281458 0.464 0.0389167 0.525531 0.115583 0.611797 0.222521 0.676359 0.306583 0.678875 0.317354 0.677359 0.385271 0.66475 0.394687 0.588594 0.407458 0.417094 0.517771 0.280906 0.604521 0.0806562 0.722208 0.0256719 0.763917 0.00296875 0.809646 0 0.786104 0 0.745083 0 0.612583 0.03525 0.613271 0.0877187 0.626708 0.130594 0.626708 0.170437 0.6025 0.273844 0.548708 0.338906 0.507 0.509906 0.4115 0.604734 0.359042 0.596156 0.338188 0.595141 0.306583 0.595141 0.291792 0.579516 0.213104 0.516969 0.129042 0.498297 0.100792 0.466516 0.0987708 0.448875 0.0786042 0.405484 0.0705208 0.375219 0.0678333 0.28675 0.108375 0.282719 0.123167 0.267078 0.162854 0.266062 0.189083 0.245391 0.199833 0.203516 0.251625 0.187375 0.269771 0.159641 0.240188 0.101125 0.249604 0 0.287271 0 0.250271 0 0.245563 0.0975938 0.202521 0.203516 0.145354 0.251953 0.123167 0.28675 0.108375\n49 0.587812 0.128229 0.612281 0.0965625 0.663391 0.0840833 0.690031 0.0908125 0.700109 0.10425 0.705859 0.133042 0.700109 0.143604 0.686422 0.146479 0.664828 0.153188 0.644672 0.157042 0.629563 0.175271 0.605797 0.181021 0.595 0.147437\n49 0.7405 0.178417 0.733719 0.173896 0.727781 0.162583 0.729484 0.150167 0.738812 0.124146 0.747281 0.0981458 0.776109 0.0811875 0.804094 0.0845833 0.814266 0.102667 0.818516 0.115104 0.812578 0.133208 0.782906 0.151292 0.754063 0.172771\n49 0.602656 0.178854 0.636125 0.167875 0.655172 0.165125 0.6665 0.162375 0.680391 0.155521 0.691719 0.153458 0.703047 0.154146 0.713859 0.162375 0.724156 0.174729 0.730844 0.193271 0.733422 0.217979 0.733938 0.244063 0.733422 0.281813 0.732391 0.295542 0.728266 0.300354 0.702016 0.294854 0.682969 0.28525 0.672156 0.270146\n49 0.716891 0.0519583 0.683766 0.0103958 0.611688 0.0051875 0.568828 0.116875 0.590266 0.15325 0.590266 0.116875 0.613641 0.0857083 0.631172 0.0857083 0.6565 0.083125 0.679875 0.0883125 0.691563 0.0961042 0.711031 0.0649375\n</code></pre> instances_val2017.json\u4e3acoco\u683c\u5f0f\u7684\u9a8c\u8bc1\u96c6\u6807\u6ce8\uff0c\u53ef\u76f4\u63a5\u8c03\u7528coco api\u7528\u4e8emap\u7684\u8ba1\u7b97\u3002</p> <p>\u8bad\u7ec3&amp;\u63a8\u7406\u65f6\uff0c\u9700\u4fee\u6539<code>configs/coco.yaml</code>\u4e2d\u7684<code>train_set</code>,<code>val_set</code>,<code>test_set</code>\u4e3a\u771f\u5b9e\u6570\u636e\u8def\u5f84</p> <p>\u4f7f\u7528MindYOLO\u5957\u4ef6\u5b8c\u6210\u81ea\u5b9a\u4e49\u6570\u636e\u96c6finetune\u7684\u5b9e\u9645\u6848\u4f8b\u53ef\u53c2\u8003 \u5fae\u8c03</p>"},{"location":"zh/how_to_guides/write_a_new_model/","title":"\u6a21\u578b\u7f16\u5199\u6307\u5357","text":"<p>\u672c\u6587\u6863\u63d0\u4f9bMindYOLO\u7f16\u5199\u81ea\u5b9a\u4e49\u6a21\u578b\u7684\u6559\u7a0b\u3002 \u5206\u4e3a\u4e09\u4e2a\u90e8\u5206\uff1a</p> <ul> <li>\u6a21\u578b\u5b9a\u4e49\uff1a\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u5b9a\u4e49\u4e00\u4e2a\u7f51\u7edc\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528yaml\u6587\u4ef6\u65b9\u5f0f\u5b9a\u4e49\u4e00\u4e2a\u7f51\u7edc\u3002</li> <li>\u6ce8\u518c\u6a21\u578b\uff1a\u53ef\u9009\uff0c\u6ce8\u518c\u4e4b\u540e\u53ef\u4ee5\u5728create_model\u63a5\u53e3\u4e2d\u4f7f\u7528\u6587\u4ef6\u540d\u521b\u5efa\u81ea\u5b9a\u4e49\u7684\u6a21\u578b</li> <li>\u9a8c\u8bc1: \u9a8c\u8bc1\u6a21\u578b\u662f\u5426\u53ef\u8fd0\u884c</li> </ul>"},{"location":"zh/how_to_guides/write_a_new_model/#_2","title":"\u6a21\u578b\u5b9a\u4e49","text":""},{"location":"zh/how_to_guides/write_a_new_model/#1python","title":"1.\u76f4\u63a5\u4f7f\u7528python\u4ee3\u7801\u6765\u7f16\u5199\u7f51\u7edc","text":""},{"location":"zh/how_to_guides/write_a_new_model/#_3","title":"\u6a21\u5757\u5bfc\u5165","text":"<p>\u5bfc\u5165MindSpore\u6846\u67b6\u4e2d\u7684nn\u6a21\u5757\u548cops\u6a21\u5757\uff0c\u7528\u4e8e\u5b9a\u4e49\u795e\u7ecf\u7f51\u7edc\u7684\u7ec4\u4ef6\u548c\u64cd\u4f5c\u3002 <pre><code>import mindspore.nn as nn\nimport mindspore.ops.operations as ops\n</code></pre></p>"},{"location":"zh/how_to_guides/write_a_new_model/#_4","title":"\u521b\u5efa\u6a21\u578b","text":"<p>\u5b9a\u4e49\u4e86\u4e00\u4e2a\u7ee7\u627f\u81eann.Cell\u7684\u6a21\u578b\u7c7bMyModel\u3002\u5728\u6784\u9020\u51fd\u6570__init__\u4e2d\uff0c\u5b9a\u4e49\u6a21\u578b\u7684\u5404\u4e2a\u7ec4\u4ef6\uff1a</p> <pre><code>class MyModel(nn.Cell):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        #conv1\u662f\u4e00\u4e2a2D\u5377\u79ef\u5c42\uff0c\u8f93\u5165\u901a\u9053\u6570\u4e3a3\uff0c\u8f93\u51fa\u901a\u9053\u6570\u4e3a16\uff0c\u5377\u79ef\u6838\u5927\u5c0f\u4e3a3x3\uff0c\u6b65\u957f\u4e3a1\uff0c\u586b\u5145\u4e3a1\u3002\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n        #relu\u662f\u4e00\u4e2aReLU\u6fc0\u6d3b\u51fd\u6570\u64cd\u4f5c\u3002\n        self.relu = ops.ReLU()\n        #maxpool\u662f\u4e00\u4e2a2D\u6700\u5927\u6c60\u5316\u5c42\uff0c\u6c60\u5316\u7a97\u53e3\u5927\u5c0f\u4e3a2x2\uff0c\u6b65\u957f\u4e3a2\u3002\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n        #conv2\u662f\u53e6\u4e00\u4e2a2D\u5377\u79ef\u5c42\uff0c\u8f93\u5165\u901a\u9053\u6570\u4e3a16\uff0c\u8f93\u51fa\u901a\u9053\u6570\u4e3a32\uff0c\u5377\u79ef\u6838\u5927\u5c0f\u4e3a3x3\uff0c\u6b65\u957f\u4e3a1\uff0c\u586b\u5145\u4e3a1\u3002\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n        #fc\u662f\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\uff0c\u8f93\u5165\u7279\u5f81\u7ef4\u5ea6\u4e3a32x8x8\uff0c\u8f93\u51fa\u7279\u5f81\u7ef4\u5ea6\u4e3a10\u3002\n        self.fc = nn.Dense(32 * 8 * 8, 10)\n\n    #\u5728construct\u65b9\u6cd5\u4e2d\uff0c\u5b9a\u4e49\u4e86\u6a21\u578b\u7684\u524d\u5411\u4f20\u64ad\u8fc7\u7a0b\u3002\u8f93\u5165x\u7ecf\u8fc7\u5377\u79ef\u3001\u6fc0\u6d3b\u51fd\u6570\u3001\u6c60\u5316\u7b49\u64cd\u4f5c\u540e\uff0c\u901a\u8fc7\u5c55\u5e73\u64cd\u4f5c\u5c06\u7279\u5f81\u5f20\u91cf\u53d8\u4e3a\u4e00\u7ef4\u5411\u91cf\uff0c\u7136\u540e\u901a\u8fc7\u5168\u8fde\u63a5\u5c42\u5f97\u5230\u6700\u7ec8\u7684\u8f93\u51fa\u7ed3\u679c\u3002    \n    def construct(self, x): \n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.conv2(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = x.view(x.shape[0], -1)\n        x = self.fc(x)\n        return x\n</code></pre>"},{"location":"zh/how_to_guides/write_a_new_model/#_5","title":"\u521b\u5efa\u6a21\u578b\u5b9e\u4f8b","text":"<p>\u901a\u8fc7\u5b9e\u4f8b\u5316MyModel\u7c7b\uff0c\u521b\u5efa\u4e00\u4e2a\u6a21\u578b\u5b9e\u4f8bmodel\uff0c\u540e\u7eed\u53ef\u4ee5\u4f7f\u7528\u8be5\u5b9e\u4f8b\u8fdb\u884c\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u3002 <pre><code>model = MyModel()\n</code></pre></p>"},{"location":"zh/how_to_guides/write_a_new_model/#2yaml","title":"2.\u4f7f\u7528yaml\u6587\u4ef6\u7f16\u5199\u7f51\u7edc","text":"<p>\u901a\u5e38\u9700\u8981\u4ee5\u4e0b\u4e09\u4e2a\u6b65\u9aa4\uff1a</p> <ul> <li>\u65b0\u5efa\u4e00\u4e2amymodel.yaml\u6587\u4ef6</li> <li>\u65b0\u5efa\u5bf9\u5e94\u7684mymodel.py\u6587\u4ef6 </li> <li>\u5728mindyolo/models/init.py\u6587\u4ef6\u4e2d\u5f15\u5165\u8be5\u6a21\u578b</li> </ul> <p>\u4ee5\u4e0b\u662f\u7f16\u5199mymodel.yaml\u6587\u4ef6\u7684\u8be6\u7ec6\u6307\u5bfc: \u4ee5\u7f16\u5199\u4e00\u4e2a\u7b80\u5355\u7f51\u7edc\u4e3a\u4f8b\uff1a \u4ee5yaml\u683c\u5f0f\u7f16\u5199\u5fc5\u8981\u53c2\u6570\uff0c\u540e\u7eed\u5728mymodel.py\u6587\u4ef6\u91cc\u9762\u53ef\u4ee5\u7528\u5230\u8fd9\u4e9b\u53c2\u6570\u3002 \u5176\u4e2dnetwork\u90e8\u5206\u4e3a\u6a21\u578b\u7f51\u7edc  [[from, number, module, args], ...]\uff1a\u6bcf\u4e2a\u5143\u7d20\u4ee3\u8868\u4e00\u4e2a\u7f51\u7edc\u5c42\u7684\u914d\u7f6e\u3002 <pre><code># __BASE__\u4e2d\u7684yaml\u8868\u793a\u7528\u4e8e\u7ee7\u627f\u7684\u57fa\u7840\u914d\u7f6e\u6587\u4ef6\uff0c\u91cd\u590d\u7684\u53c2\u6570\u4f1a\u88ab\u5f53\u524d\u6587\u4ef6\u8986\u76d6\uff1b\n__BASE__:\n- '../coco.yaml'\n- './hyp.scratch-high.yaml'\n\nper_batch_size: 32\nimg_size: 640\nsync_bn: False\n\nnetwork:\nmodel_name: mymodel\ndepth_multiple: 1.0  # model depth multiple\nwidth_multiple: 1.0  # layer channel multiple\nstride: [ 8, 16, 32 ]\n\n# \u9aa8\u5e72\u7f51\u7edc\u90e8\u5206\u7684\u914d\u7f6e\uff0c\u6bcf\u5c42\u7684\u5143\u7d20\u542b\u4e49\u4e3a\n# [from, number, module, args]\n# \u4ee5\u7b2c\u4e00\u5c42\u4e3a\u4f8b\uff0c[-1, 1, ConvNormAct, [32, 3, 1]], \u8868\u793a\u8f93\u5165\u6765\u81ea `-1`(\u4e0a\u4e00\u5c42) \uff0c\u91cd\u590d\u6b21\u6570\u4e3a 1\uff0c\u6a21\u5757\u540d\u4e3a ConvNormAct\uff0c\u6a21\u5757\u8f93\u5165\u53c2\u6570\u4e3a [32, 3, 1]\uff1b\nbackbone: [[-1, 1, ConvNormAct, [32, 3, 1]],  # 0\n[-1, 1, ConvNormAct, [64, 3, 2]],  # 1-P1/2\n[-1, 1, Bottleneck, [64]],\n[-1, 1, ConvNormAct, [128, 3, 2]],  # 3-P2/4\n[-1, 2, Bottleneck, [128]],\n[-1, 1, ConvNormAct, [256, 3, 2]],  # 5-P3/8\n[-1, 8, Bottleneck, [256]],\n]\n\n#head\u90e8\u5206\u7684\u914d\u7f6e \nhead: [\n[ -1, 1, ConvNormAct, [ 512, 3, 2 ] ],  # 7-P4/16\n[ -1, 8, Bottleneck, [ 512 ] ],\n[ -1, 1, ConvNormAct, [ 1024, 3, 2 ] ],  # 9-P5/32\n[ -1, 4, Bottleneck, [ 1024 ] ],  # 10\n]\n</code></pre></p> <p>\u7f16\u5199mymodel.py\u6587\u4ef6:</p>"},{"location":"zh/how_to_guides/write_a_new_model/#_6","title":"\u6a21\u5757\u5bfc\u5165","text":"<p>\u9700\u8981\u5bfc\u5165\u5957\u4ef6\u5185\u7684\u6a21\u5757\u3002 \u5982<code>from .registry import register_model</code>\u7b49\u7b49</p> <pre><code>import numpy as np\n\nimport mindspore as ms\nfrom mindspore import Tensor, nn\n\n\nfrom .initializer import initialize_defult #\u7528\u4e8e\u521d\u59cb\u5316\u6a21\u578b\u7684\u9ed8\u8ba4\u53c2\u6570\uff0c\u5305\u62ec\u6743\u91cd\u521d\u59cb\u5316\u65b9\u5f0f\u3001BN \u5c42\u53c2\u6570\u7b49\u3002\nfrom .model_factory import build_model_from_cfg #\u7528\u4e8e\u6839\u636e YAML \u914d\u7f6e\u6587\u4ef6\u4e2d\u7684\u53c2\u6570\u6784\u5efa\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff0c\u5e76\u8fd4\u56de\u8be5\u6a21\u578b\u7684\u5b9e\u4f8b\u3002\nfrom .registry import register_model #\u7528\u4e8e\u5c06\u81ea\u5b9a\u4e49\u7684\u6a21\u578b\u6ce8\u518c\u5230 Mindyolo \u4e2d\uff0c\u4ee5\u4fbf\u5728 YAML \u914d\u7f6e\u6587\u4ef6\u4e2d\u4f7f\u7528\u3002\n\n#\u53ef\u89c1\u6027\u58f0\u660e\n__all__ = [\"MYmodel\", \"mymodel\"]\n</code></pre>"},{"location":"zh/how_to_guides/write_a_new_model/#_7","title":"\u521b\u5efa\u914d\u7f6e\u5b57\u5178","text":"<p>_cfg\u51fd\u6570\u662f\u4e00\u4e2a\u8f85\u52a9\u51fd\u6570\uff0c\u7528\u4e8e\u521b\u5efa\u914d\u7f6e\u5b57\u5178\u3002\u5b83\u63a5\u53d7\u4e00\u4e2aurl\u53c2\u6570\u548c\u5176\u4ed6\u5173\u952e\u5b57\u53c2\u6570\uff0c\u5e76\u8fd4\u56de\u4e00\u4e2a\u5305\u542burl\u548c\u5176\u4ed6\u53c2\u6570\u7684\u5b57\u5178\u3002 default_cfgs\u662f\u4e00\u4e2a\u5b57\u5178\uff0c\u7528\u4e8e\u5b58\u50a8\u9ed8\u8ba4\u914d\u7f6e\u3002\u5728\u8fd9\u91cc\uff0cmymodel\u4f5c\u4e3a\u952e\uff0c\u4f7f\u7528_cfg\u51fd\u6570\u521b\u5efa\u4e86\u4e00\u4e2a\u914d\u7f6e\u5b57\u5178\u3002 <pre><code>def _cfg(url=\"\", **kwargs):\n    return {\"url\": url, **kwargs}\n\ndefault_cfgs = {\"mymodel\": _cfg(url=\"\")}\n</code></pre></p>"},{"location":"zh/how_to_guides/write_a_new_model/#_8","title":"\u521b\u5efa\u6a21\u578b","text":"<p>\u5728<code>MindSpore</code>\u4e2d\uff0c\u6a21\u578b\u7684\u7c7b\u7ee7\u627f\u4e8e<code>nn.Cell</code>\uff0c\u4e00\u822c\u6765\u8bf4\u9700\u8981\u91cd\u8f7d\u4ee5\u4e0b\u4e24\u4e2a\u51fd\u6570\uff1a</p> <ul> <li>\u5728<code>__init__</code>\u51fd\u6570\u4e2d\uff0c\u5e94\u5f53\u5b9a\u4e49\u6a21\u578b\u4e2d\u9700\u8981\u7528\u5230\u7684module\u5c42\u3002</li> <li>\u5728<code>construct</code>\u51fd\u6570\u4e2d\u5b9a\u4e49\u6a21\u578b\u524d\u5411\u903b\u8f91\u3002 </li> </ul> <pre><code>class MYmodel(nn.Cell):\n\n    def __init__(self, cfg, in_channels=3, num_classes=None, sync_bn=False):\n        super(MYmodel, self).__init__()\n        self.cfg = cfg\n        self.stride = Tensor(np.array(cfg.stride), ms.int32)\n        self.stride_max = int(max(self.cfg.stride))\n        ch, nc = in_channels, num_classes\n\n        self.nc = nc  # override yaml value\n        self.model = build_model_from_cfg(model_cfg=cfg, in_channels=ch, num_classes=nc, sync_bn=sync_bn)\n        self.names = [str(i) for i in range(nc)]  # default names\n\n        initialize_defult()  # \u53ef\u9009\uff0c\u4f60\u53ef\u80fd\u9700\u8981initialize_defult\u65b9\u6cd5\u4ee5\u83b7\u5f97\u548cpytorch\u4e00\u6837\u7684conv2d\u3001dense\u5c42\u7684\u521d\u59cb\u5316\u65b9\u5f0f\uff1b\n\n    def construct(self, x):\n        return self.model(x)\n</code></pre>"},{"location":"zh/how_to_guides/write_a_new_model/#_9","title":"\u6ce8\u518c\u6a21\u578b\uff08\u53ef\u9009\uff09","text":"<p>\u5982\u679c\u9700\u8981\u4f7f\u7528mindyolo\u63a5\u53e3\u521d\u59cb\u5316\u81ea\u5b9a\u4e49\u7684\u6a21\u578b\uff0c\u90a3\u4e48\u9700\u8981\u5148\u5bf9\u6a21\u578b\u8fdb\u884c**\u6ce8\u518c**\u548c**\u5bfc\u5165**</p> <p>\u6a21\u578b\u6ce8\u518c <pre><code>@register_model #\u6ce8\u518c\u540e\u7684\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7 create_model \u63a5\u53e3\u4ee5\u6a21\u578b\u540d\u7684\u65b9\u5f0f\u8fdb\u884c\u8bbf\u95ee\uff1b\ndef mymodel(cfg, in_channels=3, num_classes=None, **kwargs) -&gt; MYmodel:\n\"\"\"Get GoogLeNet model.\n    Refer to the base class `models.GoogLeNet` for more details.\"\"\"\n    model = MYmodel(cfg=cfg, in_channels=in_channels, num_classes=num_classes, **kwargs)\n    return model\n</code></pre> \u6a21\u578b\u5bfc\u5165 </p> <pre><code>#\u5728mindyolo/models/_init_.py\u6587\u4ef6\u4e2d\u6dfb\u52a0\u4ee5\u4e0b\u4ee3\u7801\n\nfrom . import mymodel #mymodel.py\u6587\u4ef6\u901a\u5e38\u653e\u5728mindyolo/models/\u76ee\u5f55\u4e0b\n__all__.extend(mymodel.__all__)\nfrom .mymodel import *\n</code></pre>"},{"location":"zh/how_to_guides/write_a_new_model/#main","title":"\u9a8c\u8bc1main","text":"<p>\u521d\u59cb\u7f16\u5199\u9636\u6bb5\u5e94\u5f53\u4fdd\u8bc1\u6a21\u578b\u662f\u53ef\u8fd0\u884c\u7684\u3002\u53ef\u901a\u8fc7\u4e0b\u8ff0\u4ee3\u7801\u5757\u8fdb\u884c\u57fa\u7840\u9a8c\u8bc1\uff1a \u9996\u5148\u5bfc\u5165\u6240\u9700\u7684\u6a21\u5757\u548c\u51fd\u6570\u3002\u7136\u540e\uff0c\u901a\u8fc7\u89e3\u6790\u914d\u7f6e\u5bf9\u8c61\u3002</p> <p><pre><code>if __name__ == \"__main__\":\n    from mindyolo.models.model_factory import create_model\n    from mindyolo.utils.config import parse_config\n\n    opt = parse_config()\n</code></pre> \u521b\u5efa\u6a21\u578b\u5e76\u6307\u5b9a\u76f8\u5173\u53c2\u6570\uff0c\u6ce8\u610f\uff1a\u5982\u679c\u8981\u5728create_model\u4e2d\u4f7f\u7528\u6587\u4ef6\u540d\u521b\u5efa\u81ea\u5b9a\u4e49\u7684\u6a21\u578b\uff0c\u90a3\u4e48\u9700\u8981\u5148\u4f7f\u7528\u6ce8\u518c\u5668@register_model\u8fdb\u884c\u6ce8\u518c\uff0c\u8bf7\u53c2\u89c1\u4e0a\u6587 \u6ce8\u518c\u6a21\u578b\uff08\u53ef\u9009)\u90e8\u5206\u5185\u5bb9 <pre><code>    model = create_model(\n        model_name=\"mymodel\",\n        model_cfg=opt.net,\n        num_classes=opt.data.nc,\n        sync_bn=opt.sync_bn if hasattr(opt, \"sync_bn\") else False,\n    ) \n</code></pre></p> <p>\u5426\u5219\uff0c\u8bf7\u4f7f\u7528import\u7684\u65b9\u5f0f\u5f15\u5165\u6a21\u578b</p> <p><pre><code>    from mindyolo.models.mymodel import MYmodel\n    model = MYmodel(\n        model_name=\"mymodel\",\n        model_cfg=opt.net,\n        num_classes=opt.data.nc,\n        sync_bn=opt.sync_bn if hasattr(opt, \"sync_bn\") else False,\n    ) \n</code></pre> \u6700\u540e\uff0c\u521b\u5efa\u4e00\u4e2a\u8f93\u5165\u5f20\u91cfx\u5e76\u5c06\u5176\u4f20\u9012\u7ed9\u6a21\u578b\u8fdb\u884c\u524d\u5411\u8ba1\u7b97\u3002 <pre><code>    x = Tensor(np.random.randn(1, 3, 640, 640), ms.float32)\n    out = model(x)\n    out = out[0] if isinstance(out, (list, tuple)) else out\n    print(f\"Output shape is {[o.shape for o in out]}\")\n</code></pre></p>"},{"location":"zh/modelzoo/benchmark/","title":"\u6a21\u578b\u4ed3\u5e93","text":""},{"location":"zh/modelzoo/benchmark/#_2","title":"\u68c0\u6d4b\u4efb\u52a1","text":"performance tested on Ascend 910(8p) with graph mode Name Scale BatchSize ImageSize Dataset Box mAP (%) Params Recipe Download YOLOv8 N 16 * 8 640 MS COCO 2017 37.2 3.2M yaml weights YOLOv8 S 16 * 8 640 MS COCO 2017 44.6 11.2M yaml weights YOLOv8 M 16 * 8 640 MS COCO 2017 50.5 25.9M yaml weights YOLOv8 L 16 * 8 640 MS COCO 2017 52.8 43.7M yaml weights YOLOv8 X 16 * 8 640 MS COCO 2017 53.7 68.2M yaml weights YOLOv7 Tiny 16 * 8 640 MS COCO 2017 37.5 6.2M yaml weights YOLOv7 L 16 * 8 640 MS COCO 2017 50.8 36.9M yaml weights YOLOv7 X 12 * 8 640 MS COCO 2017 52.4 71.3M yaml weights YOLOv5 N 32 * 8 640 MS COCO 2017 27.3 1.9M yaml weights YOLOv5 S 32 * 8 640 MS COCO 2017 37.6 7.2M yaml weights YOLOv5 M 32 * 8 640 MS COCO 2017 44.9 21.2M yaml weights YOLOv5 L 32 * 8 640 MS COCO 2017 48.5 46.5M yaml weights YOLOv5 X 16 * 8 640 MS COCO 2017 50.5 86.7M yaml weights YOLOv4 CSPDarknet53 16 * 8 608 MS COCO 2017 45.4 27.6M yaml weights YOLOv4 CSPDarknet53(silu) 16 * 8 608 MS COCO 2017 45.8 27.6M yaml weights YOLOv3 Darknet53 16 * 8 640 MS COCO 2017 45.5 61.9M yaml weights YOLOX N 8 * 8 416 MS COCO 2017 24.1 0.9M yaml weights YOLOX Tiny 8 * 8 416 MS COCO 2017 33.3 5.1M yaml weights YOLOX S 8 * 8 640 MS COCO 2017 40.7 9.0M yaml weights YOLOX M 8 * 8 640 MS COCO 2017 46.7 25.3M yaml weights YOLOX L 8 * 8 640 MS COCO 2017 49.2 54.2M yaml weights YOLOX X 8 * 8 640 MS COCO 2017 51.6 99.1M yaml weights YOLOX Darknet53 8 * 8 640 MS COCO 2017 47.7 63.7M yaml weights \u8bbe\u5907 Ascend 910*(8p) \u6d4b\u8bd5\u7ed3\u679c Name Scale BatchSize ImageSize Dataset Box mAP (%) ms/step Params Recipe Download YOLOv8 N 16 * 8 640 MS COCO 2017 37.3 373.55 3.2M yaml weights YOLOv8 S 16 * 8 640 MS COCO 2017 44.7 365.53 11.2M yaml weights YOLOv7 Tiny 16 * 8 640 MS COCO 2017 37.5 496.21 6.2M yaml weights YOLOv5 N 32 * 8 640 MS COCO 2017 27.4 736.08 1.9M yaml weights YOLOv5 S 32 * 8 640 MS COCO 2017 37.6 787.34 7.2M yaml weights YOLOv4 CSPDarknet53 16 * 8 608 MS COCO 2017 46.1 337.25 27.6M yaml weights YOLOv3 Darknet53 16 * 8 640 MS COCO 2017 46.6 396.60 61.9M yaml weights YOLOX S 8 * 8 640 MS COCO 2017 41.0 242.15 9.0M yaml weights"},{"location":"zh/modelzoo/benchmark/#_3","title":"\u56fe\u50cf\u5206\u5272","text":"\u8bbe\u5907 Ascend 910(8p) \u6d4b\u8bd5\u7ed3\u679c Name Scale BatchSize ImageSize Dataset Box mAP (%) Mask mAP (%) Params Recipe Download YOLOv8-seg X 16 * 8 640 MS COCO 2017 52.5 42.9 71.8M yaml weights"},{"location":"zh/modelzoo/benchmark/#_4","title":"\u90e8\u7f72","text":"<ul> <li>\u8be6\u89c1 \u90e8\u7f72</li> </ul>"},{"location":"zh/modelzoo/benchmark/#_5","title":"\u8bf4\u660e","text":"<ul> <li>Box mAP\uff1a\u5728\u9a8c\u8bc1\u96c6\u4e0a\u8ba1\u7b97\u7684\u51c6\u786e\u5ea6\u3002</li> </ul>"},{"location":"zh/modelzoo/yolov3/#_1","title":"\u6458\u8981","text":"<p>\u6211\u4eec\u5bf9YOLO\u8fdb\u884c\u4e86\u4e00\u7cfb\u5217\u66f4\u65b0\uff01\u5b83\u5305\u542b\u4e00\u5806\u5c0f\u8bbe\u8ba1\uff0c\u53ef\u4ee5\u4f7f\u7cfb\u7edf\u7684\u6027\u80fd\u5f97\u5230\u66f4\u65b0\u3002\u6211\u4eec\u4e5f\u8bad\u7ec3\u4e86\u4e00\u4e2a\u65b0\u7684\u3001\u6bd4\u8f83\u5927\u7684\u795e\u7ecf\u7f51\u7edc\u3002\u867d\u7136\u6bd4\u4e0a\u4e00\u7248\u66f4\u5927\u4e00\u4e9b\uff0c\u4f46\u662f\u7cbe\u5ea6\u4e5f\u63d0\u9ad8\u4e86\u3002\u4e0d\u7528\u62c5\u5fc3\uff0c\u5b83\u7684\u901f\u5ea6\u4f9d\u7136\u5f88\u5feb\u3002YOLOv3\u5728320\u00d7320\u8f93\u5165\u56fe\u50cf\u4e0a\u8fd0\u884c\u65f6\u53ea\u970022ms\uff0c\u5e76\u80fd\u8fbe\u523028.2mAP\uff0c\u5176\u7cbe\u5ea6\u548cSSD\u76f8\u5f53\uff0c\u4f46\u901f\u5ea6\u8981\u5feb\u4e0a3\u500d\u3002\u4f7f\u7528\u4e4b\u524d0.5 IOU mAP\u7684\u68c0\u6d4b\u6307\u6807\uff0cYOLOv3\u7684\u6548\u679c\u662f\u76f8\u5f53\u4e0d\u9519\u3002YOLOv3\u4f7f\u7528Titan X GPU\uff0c\u5176\u8017\u65f651ms\u68c0\u6d4b\u7cbe\u5ea6\u8fbe\u523057.9 AP50\uff0c\u4e0eRetinaNet\u76f8\u6bd4\uff0c\u5176\u7cbe\u5ea6\u53ea\u670957.5 AP50\uff0c\u4f46\u5374\u8017\u65f6198ms\uff0c\u76f8\u540c\u6027\u80fd\u7684\u6761\u4ef6\u4e0bYOLOv3\u901f\u5ea6\u6bd4RetinaNet\u5feb3.8\u500d\u3002</p>"},{"location":"zh/modelzoo/yolov3/#_2","title":"\u7ed3\u679c","text":"\u4f7f\u7528\u56fe\u6a21\u5f0f\u5728 Ascend 910(8p) \u4e0a\u6d4b\u8bd5\u7684\u8868\u73b0 Name Scale BatchSize ImageSize Dataset Box mAP (%) Params Recipe Download YOLOv3 Darknet53 16 * 8 640 MS COCO 2017 45.5 61.9M yaml weights \u5728Ascend 910*(8p)\u4e0a\u6d4b\u8bd5\u7684\u8868\u73b0 Name Scale BatchSize ImageSize Dataset Box mAP (%) ms/step Params Recipe Download YOLOv3 Darknet53 16 * 8 640 MS COCO 2017 46.6 396.60 61.9M yaml weights"},{"location":"zh/modelzoo/yolov3/#_3","title":"\u8bf4\u660e","text":"<ul> <li>Box mAP\uff1a\u9a8c\u8bc1\u96c6\u4e0a\u6d4b\u8bd5\u51fa\u7684\u51c6\u786e\u5ea6\u3002</li> <li>\u6211\u4eec\u53c2\u8003\u4e86\u5e38\u7528\u7684\u7b2c\u4e09\u65b9 YOLOv3 \u7684\u5b9e\u73b0\u3002</li> </ul>"},{"location":"zh/modelzoo/yolov3/#_4","title":"\u5feb\u901f\u5165\u95e8","text":"<p>\u8be6\u60c5\u8bf7\u53c2\u9605 MindYOLO \u4e2d\u7684 \u5feb\u901f\u5165\u95e8\u3002</p>"},{"location":"zh/modelzoo/yolov3/#_5","title":"\u8bad\u7ec3","text":""},{"location":"zh/modelzoo/yolov3/#-","title":"- \u9884\u8bad\u7ec3\u6a21\u578b","text":"<p>\u60a8\u53ef\u4ee5\u4ece \u6b64\u5904 \u83b7\u53d6\u9884\u8bad\u7ec3\u6a21\u578b\u3002</p> <p>\u8981\u5c06\u5176\u8f6c\u6362\u4e3a mindyolo \u53ef\u52a0\u8f7d\u7684 ckpt \u6587\u4ef6\uff0c\u8bf7\u5c06\u5176\u653e\u5728\u6839\u76ee\u5f55\u4e2d\uff0c\u7136\u540e\u8fd0\u884c\u4ee5\u4e0b\u8bed\u53e5\uff1a <pre><code>python mindyolo/utils/convert_weight_darknet53.py\n</code></pre></p>"},{"location":"zh/modelzoo/yolov3/#-_1","title":"- \u5206\u5e03\u5f0f\u8bad\u7ec3","text":"<p>\u4f7f\u7528\u9884\u7f6e\u7684\u8bad\u7ec3\u914d\u65b9\u53ef\u4ee5\u8f7b\u677e\u91cd\u73b0\u62a5\u544a\u7684\u7ed3\u679c\u3002\u5982\u9700\u5728\u591a\u53f0Ascend 910\u8bbe\u5907\u4e0a\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u8bf7\u8fd0\u884c <pre><code># \u5728\u591a\u53f0Ascend\u8bbe\u5907\u4e0a\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\nmsrun --worker_num=8 --local_worker_num=8 --bind_core=True --log_dir=./yolov3_log python train.py --config ./configs/yolov3/yolov3.yaml --device_target Ascend --is_parallel True\n</code></pre></p> <p>\u6ce8\u610f: \u66f4\u591a\u5173\u4e8emsrun\u914d\u7f6e\u7684\u4fe1\u606f\uff0c\u8bf7\u53c2\u8003\u8fd9\u91cc\u3002</p> <p>\u6709\u5173\u6240\u6709\u8d85\u53c2\u6570\u7684\u8be6\u7ec6\u8bf4\u660e\uff0c\u8bf7\u53c2\u9605config.py\u3002</p> <p>\u6ce8\u610f\uff1a \u7531\u4e8e\u5168\u5c40batch size\uff08batch_size x \u8bbe\u5907\u6570\uff09\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u8d85\u53c2\u6570\uff0c\u5efa\u8bae\u4fdd\u6301\u5168\u5c40batch size\u4e0d\u53d8\u8fdb\u884c\u590d\u5236\uff0c\u6216\u8005\u5c06\u5b66\u4e60\u7387\u7ebf\u6027\u8c03\u6574\u4e3a\u65b0\u7684\u5168\u5c40batch size\u3002</p>"},{"location":"zh/modelzoo/yolov3/#-_2","title":"- \u5355\u5361\u8bad\u7ec3","text":"<p>\u5982\u679c\u60a8\u60f3\u5728\u8f83\u5c0f\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6216\u5fae\u8c03\u6a21\u578b\u800c\u4e0d\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u8bf7\u8fd0\u884c\uff1a</p> <pre><code># \u5728 CPU/Ascend \u8bbe\u5907\u4e0a\u8fdb\u884c\u5355\u5361\u8bad\u7ec3\npython train.py --config ./configs/yolov3/yolov3.yaml --device_target Ascend\n</code></pre>"},{"location":"zh/modelzoo/yolov3/#_6","title":"\u9a8c\u8bc1\u548c\u6d4b\u8bd5","text":"<p>\u8981\u9a8c\u8bc1\u8bad\u7ec3\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528 <code>test.py</code> \u5e76\u4f7f\u7528 <code>--weight</code> \u4f20\u5165\u6743\u91cd\u8def\u5f84\u3002</p> <pre><code>python test.py --config ./configs/yolov3/yolov3.yaml --device_target Ascend --weight /PATH/TO/WEIGHT.ckpt\n</code></pre>"},{"location":"zh/modelzoo/yolov3/#_7","title":"\u90e8\u7f72","text":"<p>\u8be6\u89c1 \u90e8\u7f72\u3002</p>"},{"location":"zh/modelzoo/yolov3/#_8","title":"\u5f15\u7528","text":"<p>[1] Jocher Glenn. YOLOv3 release v9.1. https://github.com/ultralytics/yolov3/releases/tag/v9.1, 2021. [2] Joseph Redmon and Ali Farhadi. YOLOv3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018.</p>"},{"location":"zh/modelzoo/yolov4/#_1","title":"\u6458\u8981","text":"<p>\u76ee\u524d\u6709\u5f88\u591a\u53ef\u4ee5\u63d0\u9ad8CNN\u51c6\u786e\u6027\u7684\u7b97\u6cd5\u3002\u8fd9\u4e9b\u7b97\u6cd5\u7684\u7ec4\u5408\u5728\u5e9e\u5927\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u3001\u5bf9\u5b9e\u9a8c\u7ed3\u679c\u8fdb\u884c\u7406\u8bba\u9a8c\u8bc1\u90fd\u662f\u975e\u5e38\u5fc5\u8981\u7684\u3002 \u6709\u4e9b\u7b97\u6cd5\u53ea\u5728\u7279\u5b9a\u7684\u6a21\u578b\u4e0a\u6709\u6548\u679c\uff0c\u5e76\u4e14\u53ea\u5bf9\u7279\u5b9a\u7684\u95ee\u9898\u6709\u6548\uff0c\u6216\u8005\u53ea\u5bf9\u5c0f\u89c4\u6a21\u7684\u6570\u636e\u96c6\u6709\u6548\uff1b \u7136\u800c\u6709\u4e9b\u7b97\u6cd5\uff0c\u6bd4\u5982batch-normalization\u548cresidual-connections\uff0c\u5bf9\u5927\u591a\u6570\u7684\u6a21\u578b\u3001\u4efb\u52a1\u548c\u6570\u636e\u96c6\u90fd\u9002\u7528\u3002 \u6211\u4eec\u8ba4\u4e3a\u8fd9\u6837\u901a\u7528\u7684\u7b97\u6cd5\u5305\u62ec\uff1aWeighted-Residual-Connections\uff08WRC), Cross-Stage-Partial-connections\uff08CSP\uff09, Cross mini-Batch Normalization\uff08CmBN\uff09, Self-adversarial-training\uff08SAT\uff09\u4ee5\u53caMish-activation\u3002 \u6211\u4eec\u4f7f\u7528\u4e86\u65b0\u7684\u7b97\u6cd5\uff1aWRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, Dropblock regularization \u548cCIoU loss\u4ee5\u53ca\u5b83\u4eec\u7684\u7ec4\u5408\uff0c \u83b7\u5f97\u4e86\u6700\u4f18\u7684\u6548\u679c\uff1a\u5728MS COCO\u6570\u636e\u96c6\u4e0a\u7684AP\u503c\u4e3a43.5%(65.7% AP50)\uff0c\u5728Tesla V100\u4e0a\u7684\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\u4e3a65FPS\u3002</p>"},{"location":"zh/modelzoo/yolov4/#_2","title":"\u7ed3\u679c","text":"\u4f7f\u7528\u56fe\u6a21\u5f0f\u5728 Ascend 910(8p) \u4e0a\u6d4b\u8bd5\u7684\u8868\u73b0 Name Scale BatchSize ImageSize Dataset Box mAP (%) Params Recipe Download YOLOv4 CSPDarknet53 16 * 8 608 MS COCO 2017 45.4 27.6M yaml weights YOLOv4 CSPDarknet53(silu) 16 * 8 608 MS COCO 2017 45.8 27.6M yaml weights \u5728Ascend 910*(8p)\u4e0a\u6d4b\u8bd5\u7684\u8868\u73b0 Name Scale BatchSize ImageSize Dataset Box mAP (%) ms/step Params Recipe Download YOLOv4 CSPDarknet53 16 * 8 608 MS COCO 2017 46.1 337.25 27.6M yaml weights"},{"location":"zh/modelzoo/yolov4/#_3","title":"\u8bf4\u660e","text":"<ul> <li>Box mAP: \u9a8c\u8bc1\u96c6\u4e0a\u6d4b\u8bd5\u51fa\u7684\u51c6\u786e\u5ea6\u3002</li> </ul>"},{"location":"zh/modelzoo/yolov4/#_4","title":"\u5feb\u901f\u5165\u95e8","text":"<p>\u8be6\u60c5\u8bf7\u53c2\u9605 MindYOLO \u4e2d\u7684 \u5feb\u901f\u5165\u95e8\u3002</p>"},{"location":"zh/modelzoo/yolov4/#_5","title":"\u8bad\u7ec3","text":""},{"location":"zh/modelzoo/yolov4/#-","title":"- \u9884\u8bad\u7ec3\u6a21\u578b","text":"<p>\u60a8\u53ef\u4ee5\u4ece \u6b64\u5904 \u83b7\u53d6\u9884\u8bad\u7ec3\u6a21\u578b\u3002</p> <p>\u8981\u5c06\u5176\u8f6c\u6362\u4e3a mindyolo \u53ef\u52a0\u8f7d\u7684 ckpt \u6587\u4ef6\uff0c\u8bf7\u5c06\u5176\u653e\u5728\u6839\u76ee\u5f55\u4e2d\uff0c\u7136\u540e\u8fd0\u884c\u4ee5\u4e0b\u8bed\u53e5\uff1a <pre><code>python mindyolo/utils/convert_weight_cspdarknet53.py\n</code></pre></p>"},{"location":"zh/modelzoo/yolov4/#-_1","title":"- \u5206\u5e03\u5f0f\u8bad\u7ec3","text":"<p>\u4f7f\u7528\u9884\u7f6e\u7684\u8bad\u7ec3\u914d\u65b9\u53ef\u4ee5\u8f7b\u677e\u91cd\u73b0\u62a5\u544a\u7684\u7ed3\u679c\u3002\u5982\u9700\u5728\u591a\u53f0Ascend 910\u8bbe\u5907\u4e0a\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u8bf7\u8fd0\u884c <pre><code># distributed training on multiple Ascend devices\nmsrun --worker_num=8 --local_worker_num=8 --bind_core=True --log_dir=./yolov4_log python train.py --config ./configs/yolov4/yolov4-silu.yaml --device_target Ascend --is_parallel True --epochs 320\n</code></pre></p> <p>\u6ce8\u610f: \u66f4\u591a\u5173\u4e8emsrun\u914d\u7f6e\u7684\u4fe1\u606f\uff0c\u8bf7\u53c2\u8003\u8fd9\u91cc\u3002</p> <p>\u6709\u5173\u6240\u6709\u8d85\u53c2\u6570\u7684\u8be6\u7ec6\u8bf4\u660e\uff0c\u8bf7\u53c2\u9605config.py\u3002</p>"},{"location":"zh/modelzoo/yolov4/#_6","title":"\u8bf4\u660e","text":"<ul> <li>\u7531\u4e8e\u5168\u5c40batch size\uff08batch_size x \u8bbe\u5907\u6570\uff09\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u8d85\u53c2\u6570\uff0c\u5efa\u8bae\u4fdd\u6301\u5168\u5c40batch size\u4e0d\u53d8\u8fdb\u884c\u590d\u5236\uff0c\u6216\u8005\u5c06\u5b66\u4e60\u7387\u7ebf\u6027\u8c03\u6574\u4e3a\u65b0\u7684\u5168\u5c40batch size\u3002</li> <li>\u5982\u679c\u51fa\u73b0\u4ee5\u4e0b\u8b66\u544a\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf PYTHONWARNINGS='ignore:semaphore_tracker:UserWarning' \u6765\u4fee\u590d\u3002 <pre><code>multiprocessing/semaphore_tracker.py: 144 UserWarning: semaphore_tracker: There appear to be 235 leaked semaphores to clean up at shutdown len(cache))\n</code></pre></li> </ul>"},{"location":"zh/modelzoo/yolov4/#-_2","title":"- \u5355\u5361\u8bad\u7ec3","text":"<p>\u5982\u679c\u60a8\u60f3\u5728\u8f83\u5c0f\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6216\u5fae\u8c03\u6a21\u578b\u800c\u4e0d\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u8bf7\u8fd0\u884c\uff1a</p> <pre><code># \u5728 CPU/Ascend \u8bbe\u5907\u4e0a\u8fdb\u884c\u5355\u5361\u8bad\u7ec3\npython train.py --config ./configs/yolov4/yolov4-silu.yaml --device_target Ascend --epochs 320\n</code></pre>"},{"location":"zh/modelzoo/yolov4/#_7","title":"\u9a8c\u8bc1\u548c\u6d4b\u8bd5","text":"<p>\u8981\u9a8c\u8bc1\u8bad\u7ec3\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528 <code>test.py</code> \u5e76\u4f7f\u7528 <code>--weight</code> \u4f20\u5165\u6743\u91cd\u8def\u5f84\u3002</p> <pre><code>python test.py --config ./configs/yolov4/yolov4-silu.yaml --device_target Ascend --iou_thres 0.6 --weight /PATH/TO/WEIGHT.ckpt\n</code></pre>"},{"location":"zh/modelzoo/yolov4/#_8","title":"\u90e8\u7f72","text":"<p>\u8be6\u89c1 \u90e8\u7f72.</p>"},{"location":"zh/modelzoo/yolov4/#_9","title":"\u5f15\u7528","text":"<p>[1] Alexey Bochkovskiy, Chien-Yao Wang and Ali Farhadi. YOLOv4: Optimal Speed and Accuracy of Object Detection. arXiv preprint arXiv:2004.10934, 2020.</p>"},{"location":"zh/modelzoo/yolov5/#_1","title":"\u6458\u8981","text":"<p>YOLOv5 \u662f\u5728 COCO \u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u7684\u4e00\u7cfb\u5217\u5bf9\u8c61\u68c0\u6d4b\u67b6\u6784\u548c\u6a21\u578b\uff0c\u4ee3\u8868\u4e86 Ultralytics \u5bf9\u672a\u6765\u89c6\u89c9 AI \u65b9\u6cd5\u7684\u5f00\u6e90\u7814\u7a76\uff0c\u878d\u5408\u4e86\u6570\u5343\u5c0f\u65f6\u7684\u7814\u7a76\u548c\u5f00\u53d1\u4e2d\u79ef\u7d2f\u7684\u7ecf\u9a8c\u6559\u8bad\u548c\u6700\u4f73\u5b9e\u8df5\u3002</p>"},{"location":"zh/modelzoo/yolov5/#_2","title":"\u7ed3\u679c","text":"\u4f7f\u7528\u56fe\u6a21\u5f0f\u5728 Ascend 910(8p) \u4e0a\u6d4b\u8bd5\u7684\u8868\u73b0 Name Scale BatchSize ImageSize Dataset Box mAP (%) Params Recipe Download YOLOv5 N 32 * 8 640 MS COCO 2017 27.3 1.9M yaml weights YOLOv5 S 32 * 8 640 MS COCO 2017 37.6 7.2M yaml weights YOLOv5 M 32 * 8 640 MS COCO 2017 44.9 21.2M yaml weights YOLOv5 L 32 * 8 640 MS COCO 2017 48.5 46.5M yaml weights YOLOv5 X 16 * 8 640 MS COCO 2017 50.5 86.7M yaml weights \u5728Ascend 910*(8p)\u4e0a\u6d4b\u8bd5\u7684\u8868\u73b0 Name Scale BatchSize ImageSize Dataset Box mAP (%) ms/step Params Recipe Download YOLOv5 N 32 * 8 640 MS COCO 2017 27.4 736.08 1.9M yaml weights YOLOv5 S 32 * 8 640 MS COCO 2017 37.6 787.34 7.2M yaml weights"},{"location":"zh/modelzoo/yolov5/#_3","title":"\u8bf4\u660e","text":"<ul> <li>Box mAP\uff1a\u9a8c\u8bc1\u96c6\u4e0a\u6d4b\u8bd5\u51fa\u7684\u51c6\u786e\u5ea6\u3002</li> <li>\u6211\u4eec\u53c2\u8003\u4e86\u5e38\u7528\u7684\u7b2c\u4e09\u65b9 YOLOV5 \u91cd\u73b0\u4e86P5\uff08\u5927\u76ee\u6807\uff09\u7cfb\u5217\u6a21\u578b\uff0c\u5e76\u505a\u51fa\u4e86\u5982\u4e0b\u6539\u52a8\uff1a\u4e0e\u5b98\u65b9\u4ee3\u7801\u6709\u6240\u4e0d\u540c\uff0c\u6211\u4eec\u4f7f\u7528\u4e868x NPU(Ascend910)\u8fdb\u884c\u8bad\u7ec3\uff0c\u5355NPU\u7684batch size\u4e3a32\u3002</li> </ul>"},{"location":"zh/modelzoo/yolov5/#_4","title":"\u5feb\u901f\u5165\u95e8","text":"<p>\u8be6\u60c5\u8bf7\u53c2\u9605 MindYOLO \u4e2d\u7684 \u5feb\u901f\u5165\u95e8\u3002</p>"},{"location":"zh/modelzoo/yolov5/#_5","title":"\u8bad\u7ec3","text":""},{"location":"zh/modelzoo/yolov5/#-","title":"- \u5206\u5e03\u5f0f\u8bad\u7ec3","text":"<p>\u4f7f\u7528\u9884\u7f6e\u7684\u8bad\u7ec3\u914d\u65b9\u53ef\u4ee5\u8f7b\u677e\u91cd\u73b0\u62a5\u544a\u7684\u7ed3\u679c\u3002\u5982\u9700\u5728\u591a\u53f0Ascend 910\u8bbe\u5907\u4e0a\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u8bf7\u8fd0\u884c <pre><code># \u5728\u591a\u53f0Ascend\u8bbe\u5907\u4e0a\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\nmsrun --worker_num=8 --local_worker_num=8 --bind_core=True --log_dir=./yolov5_log python train.py --config ./configs/yolov5/yolov5n.yaml --device_target Ascend --is_parallel True\n</code></pre></p> <p>\u6ce8\u610f: \u66f4\u591a\u5173\u4e8emsrun\u914d\u7f6e\u7684\u4fe1\u606f\uff0c\u8bf7\u53c2\u8003\u8fd9\u91cc\u3002</p> <p>\u6709\u5173\u6240\u6709\u8d85\u53c2\u6570\u7684\u8be6\u7ec6\u8bf4\u660e\uff0c\u8bf7\u53c2\u9605config.py\u3002</p> <p>\u6ce8\u610f\uff1a \u7531\u4e8e\u5168\u5c40batch size\uff08batch_size x \u8bbe\u5907\u6570\uff09\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u8d85\u53c2\u6570\uff0c\u5efa\u8bae\u4fdd\u6301\u5168\u5c40batch size\u4e0d\u53d8\u8fdb\u884c\u590d\u5236\uff0c\u6216\u8005\u5c06\u5b66\u4e60\u7387\u7ebf\u6027\u8c03\u6574\u4e3a\u65b0\u7684\u5168\u5c40batch size\u3002</p>"},{"location":"zh/modelzoo/yolov5/#-_1","title":"- \u5355\u5361\u8bad\u7ec3","text":"<p>\u5982\u679c\u60a8\u60f3\u5728\u8f83\u5c0f\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6216\u5fae\u8c03\u6a21\u578b\u800c\u4e0d\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u8bf7\u8fd0\u884c\uff1a</p> <pre><code># \u5728 CPU/Ascend \u8bbe\u5907\u4e0a\u8fdb\u884c\u5355\u5361\u8bad\u7ec3\npython train.py --config ./configs/yolov5/yolov5n.yaml --device_target Ascend\n</code></pre>"},{"location":"zh/modelzoo/yolov5/#_6","title":"\u9a8c\u8bc1\u548c\u6d4b\u8bd5","text":"<p>\u8981\u9a8c\u8bc1\u8bad\u7ec3\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528 <code>test.py</code> \u5e76\u4f7f\u7528 <code>--weight</code> \u4f20\u5165\u6743\u91cd\u8def\u5f84\u3002</p> <pre><code>python test.py --config ./configs/yolov5/yolov5n.yaml --device_target Ascend --weight /PATH/TO/WEIGHT.ckpt\n</code></pre>"},{"location":"zh/modelzoo/yolov5/#_7","title":"\u90e8\u7f72","text":"<p>\u8be6\u89c1 \u90e8\u7f72\u3002</p>"},{"location":"zh/modelzoo/yolov5/#_8","title":"\u5f15\u7528","text":"<p>[1] Jocher Glenn. YOLOv5 release v6.1. https://github.com/ultralytics/yolov5/releases/tag/v6.1, 2022.</p>"},{"location":"zh/modelzoo/yolov7/#_1","title":"\u6458\u8981","text":"<p>YOLOv7\u57285FPS\u5230 160 FPS \u8303\u56f4\u5185\u7684\u901f\u5ea6\u548c\u51c6\u786e\u5ea6\u90fd\u8d85\u8fc7\u4e86\u6240\u6709\u5df2\u77e5\u7684\u7269\u4f53\u68c0\u6d4b\u5668\uff0cYOLOv7 \u5728 5 FPS \u5230 160 FPS \u8303\u56f4\u5185\u7684\u901f\u5ea6\u548c\u51c6\u786e\u5ea6\u90fd\u8d85\u8fc7\u4e86\u6240\u6709\u5df2\u77e5\u7684\u76ee\u6807\u68c0\u6d4b\u5668\uff0c\u5e76\u4e14\u5728 GPU V100 \u4e0a 30 FPS \u6216\u66f4\u9ad8\u7684\u6240\u6709\u5df2\u77e5\u5b9e\u65f6\u76ee\u6807\u68c0\u6d4b\u5668\u4e2d\u5177\u6709\u6700\u9ad8\u7684\u51c6\u786e\u5ea6 56.8% AP\u3002YOLOv7-E6 \u76ee\u6807\u68c0\u6d4b\u5668\uff0856 FPS V100\uff0c55.9% AP\uff09\u6bd4\u57fa\u4e8etransformer-based\u7684\u68c0\u6d4b\u5668 SWINL Cascade-Mask R-CNN\uff089.2 FPS A100\uff0c53.9% AP\uff09\u7684\u901f\u5ea6\u548c\u51c6\u786e\u5ea6\u5206\u522b\u9ad8\u51fa 509% \u548c 2%\uff0c\u4ee5\u53ca\u57fa\u4e8e\u5377\u79ef\u7684\u68c0\u6d4b\u5668 ConvNeXt-XL Cascade-Mask R-CNN (8.6 FPS A100, 55.2% AP) \u901f\u5ea6\u63d0\u9ad8 551%\uff0c\u51c6\u786e\u7387\u63d0\u9ad8 0.7%\uff0c\u4ee5\u53ca YOLOv7 \u7684\u8868\u73b0\u4f18\u4e8e\uff1aYOLOR\u3001YOLOX\u3001Scaled-YOLOv4\u3001YOLOv5\u3001DETR\u3001Deformable DETR  , DINO-5scale-R50, ViT-Adapter-B \u548c\u8bb8\u591a\u5176\u4ed6\u7269\u4f53\u63a2\u6d4b\u5668\u5728\u901f\u5ea6\u548c\u51c6\u786e\u5ea6\u4e0a\u3002 \u6b64\u5916\uff0c\u6211\u4eec\u53ea\u5728 MS COCO \u6570\u636e\u96c6\u4e0a\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3 YOLOv7\uff0c\u800c\u4e0d\u4f7f\u7528\u4efb\u4f55\u5176\u4ed6\u6570\u636e\u96c6\u6216\u9884\u8bad\u7ec3\u7684\u6743\u91cd\u3002</p>"},{"location":"zh/modelzoo/yolov7/#_2","title":"\u7ed3\u679c","text":"\u4f7f\u7528\u56fe\u6a21\u5f0f\u5728 Ascend 910(8p) \u4e0a\u6d4b\u8bd5\u7684\u8868\u73b0 Name Scale BatchSize ImageSize Dataset Box mAP (%) Params Recipe Download YOLOv7 Tiny 16 * 8 640 MS COCO 2017 37.5 6.2M yaml weights YOLOv7 L 16 * 8 640 MS COCO 2017 50.8 36.9M yaml weights YOLOv7 X 12 * 8 640 MS COCO 2017 52.4 71.3M yaml weights \u5728Ascend 910*(8p)\u4e0a\u6d4b\u8bd5\u7684\u8868\u73b0 Name Scale BatchSize ImageSize Dataset Box mAP (%) ms/step Params Recipe Download YOLOv7 Tiny 16 * 8 640 MS COCO 2017 37.5 496.21 6.2M yaml weights"},{"location":"zh/modelzoo/yolov7/#_3","title":"\u8bf4\u660e","text":"<ul> <li>Context\uff1a\u8bad\u7ec3\u4e0a\u4e0b\u6587\uff0c\u8868\u793a\u4e3a{\u8bbe\u5907}x{\u8bbe\u5907\u6570}-{mindspore\u6a21\u5f0f}\uff0c\u5176\u4e2dmindspore\u6a21\u5f0f\u53ef\u4ee5\u662fG-\u56fe\u6a21\u5f0f\u6216F-pynative\u6a21\u5f0f\u3002\u4f8b\u5982\uff0cD910x8-G\u7528\u4e8e\u57288\u5757Ascend 910 NPU\u4e0a\u4f7f\u7528graph\u6a21\u5f0f\u8fdb\u884c\u8bad\u7ec3\u3002</li> <li>Box mAP\uff1a\u9a8c\u8bc1\u96c6\u4e0a\u6d4b\u8bd5\u51fa\u7684\u51c6\u786e\u5ea6\u3002</li> <li>\u6211\u4eec\u53c2\u8003\u4e86\u5e38\u7528\u7684\u7b2c\u4e09\u65b9 YOLOV7 \u91cd\u73b0\u4e86P5\uff08\u5927\u76ee\u6807\uff09\u7cfb\u5217\u6a21\u578b\uff0c\u5e76\u505a\u51fa\u4e86\u5982\u4e0b\u6539\u52a8\uff1a\u4e0e\u5b98\u65b9\u4ee3\u7801\u6709\u6240\u4e0d\u540c\uff0c\u6211\u4eec\u4f7f\u7528\u4e868x NPU(Ascend910)\u8fdb\u884c\u8bad\u7ec3\uff0ctiny/l/x\u5355NPU\u7684batch size\u5206\u522b\u4e3a16/16/12\u3002</li> </ul>"},{"location":"zh/modelzoo/yolov7/#_4","title":"\u5feb\u901f\u5165\u95e8","text":"<p>\u8be6\u60c5\u8bf7\u53c2\u9605 MindYOLO \u4e2d\u7684 \u5feb\u901f\u5165\u95e8\u3002</p>"},{"location":"zh/modelzoo/yolov7/#_5","title":"\u8bad\u7ec3","text":""},{"location":"zh/modelzoo/yolov7/#-","title":"- \u5206\u5e03\u5f0f\u8bad\u7ec3","text":"<p>\u4f7f\u7528\u9884\u7f6e\u7684\u8bad\u7ec3\u914d\u65b9\u53ef\u4ee5\u8f7b\u677e\u91cd\u73b0\u62a5\u544a\u7684\u7ed3\u679c\u3002\u5982\u9700\u5728\u591a\u53f0Ascend 910\u8bbe\u5907\u4e0a\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u8bf7\u8fd0\u884c <pre><code># \u5728\u591a\u53f0Ascend\u8bbe\u5907\u4e0a\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\nmsrun --worker_num=8 --local_worker_num=8 --bind_core=True --log_dir=./yolov7_log python train.py --config ./configs/yolov7/yolov7.yaml --device_target Ascend --is_parallel True\n</code></pre></p> <p>\u6ce8\u610f: \u66f4\u591a\u5173\u4e8emsrun\u914d\u7f6e\u7684\u4fe1\u606f\uff0c\u8bf7\u53c2\u8003\u8fd9\u91cc\u3002</p> <p>\u6709\u5173\u6240\u6709\u8d85\u53c2\u6570\u7684\u8be6\u7ec6\u8bf4\u660e\uff0c\u8bf7\u53c2\u9605config.py\u3002</p> <p>\u6ce8\u610f\uff1a \u7531\u4e8e\u5168\u5c40batch size\uff08batch_size x \u8bbe\u5907\u6570\uff09\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u8d85\u53c2\u6570\uff0c\u5efa\u8bae\u4fdd\u6301\u5168\u5c40batch size\u4e0d\u53d8\u8fdb\u884c\u590d\u5236\uff0c\u6216\u8005\u5c06\u5b66\u4e60\u7387\u7ebf\u6027\u8c03\u6574\u4e3a\u65b0\u7684\u5168\u5c40batch size\u3002</p>"},{"location":"zh/modelzoo/yolov7/#-_1","title":"- \u5355\u5361\u8bad\u7ec3","text":"<p>\u5982\u679c\u60a8\u60f3\u5728\u8f83\u5c0f\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6216\u5fae\u8c03\u6a21\u578b\u800c\u4e0d\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u8bf7\u8fd0\u884c\uff1a</p> <pre><code># \u5728 CPU/Ascend \u8bbe\u5907\u4e0a\u8fdb\u884c\u5355\u5361\u8bad\u7ec3\npython train.py --config ./configs/yolov7/yolov7.yaml --device_target Ascend\n</code></pre>"},{"location":"zh/modelzoo/yolov7/#_6","title":"\u9a8c\u8bc1\u548c\u6d4b\u8bd5","text":"<p>\u8981\u9a8c\u8bc1\u8bad\u7ec3\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528 <code>test.py</code> \u5e76\u4f7f\u7528 <code>--weight</code> \u4f20\u5165\u6743\u91cd\u8def\u5f84\u3002</p> <pre><code>python test.py --config ./configs/yolov7/yolov7.yaml --device_target Ascend --weight /PATH/TO/WEIGHT.ckpt\n</code></pre>"},{"location":"zh/modelzoo/yolov7/#_7","title":"\u90e8\u7f72","text":"<p>\u8be6\u89c1 \u90e8\u7f72\u3002</p>"},{"location":"zh/modelzoo/yolov7/#_8","title":"\u5f15\u7528","text":"<p>[1] Chien-Yao Wang, Alexey Bochkovskiy, and HongYuan Mark Liao. Yolov7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. arXiv preprint arXiv:2207.02696, 2022.</p>"},{"location":"zh/modelzoo/yolov8/#_1","title":"\u6458\u8981","text":"<p>Ultralytics YOLOv8 \u7531 Ultralytics \u5f00\u53d1\uff0c\u662f\u4e00\u6b3e\u5c16\u7aef\u7684\u3001\u6700\u5148\u8fdb\u7684 (SOTA) \u6a21\u578b\uff0c\u5b83\u4ee5\u4e4b\u524d YOLO \u7248\u672c\u7684\u6210\u529f\u4e3a\u57fa\u7840\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u529f\u80fd\u548c\u6539\u8fdb\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\u548c\u7075\u6d3b\u6027\u3002YOLOv8 \u65e8\u5728\u5feb\u901f\u3001\u51c6\u786e\u4e14\u6613\u4e8e\u4f7f\u7528\uff0c\u4f7f\u5176\u6210\u4e3a\u5404\u79cd\u7269\u4f53\u68c0\u6d4b\u3001\u56fe\u50cf\u5206\u5272\u548c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u7684\u7edd\u4f73\u9009\u62e9\u3002</p>"},{"location":"zh/modelzoo/yolov8/#_2","title":"\u7ed3\u679c","text":""},{"location":"zh/modelzoo/yolov8/#_3","title":"\u56fe\u50cf\u68c0\u6d4b","text":"\u4f7f\u7528\u56fe\u6a21\u5f0f\u5728 Ascend 910(8p) \u4e0a\u6d4b\u8bd5\u7684\u8868\u73b0 Name Scale BatchSize ImageSize Dataset Box mAP (%) Params Recipe Download YOLOv8 N 16 * 8 640 MS COCO 2017 37.2 3.2M yaml weights YOLOv8 S 16 * 8 640 MS COCO 2017 44.6 11.2M yaml weights YOLOv8 M 16 * 8 640 MS COCO 2017 50.5 25.9M yaml weights YOLOv8 L 16 * 8 640 MS COCO 2017 52.8 43.7M yaml weights YOLOv8 X 16 * 8 640 MS COCO 2017 53.7 68.2M yaml weights \u5728Ascend 910*(8p)\u4e0a\u6d4b\u8bd5\u7684\u8868\u73b0 Name Scale BatchSize ImageSize Dataset Box mAP (%) ms/step Params Recipe Download YOLOv8 N 16 * 8 640 MS COCO 2017 37.3 373.55 3.2M yaml weights YOLOv8 S 16 * 8 640 MS COCO 2017 44.7 365.53 11.2M yaml weights"},{"location":"zh/modelzoo/yolov8/#_4","title":"\u56fe\u50cf\u5206\u5272","text":"\u4f7f\u7528\u56fe\u6a21\u5f0f\u5728 Ascend 910(8p) \u4e0a\u6d4b\u8bd5\u7684\u8868\u73b0 Name Scale BatchSize ImageSize Dataset Box mAP (%) Mask mAP (%) Params Recipe Download YOLOv8-seg X 16 * 8 640 MS COCO 2017 52.5 42.9 71.8M yaml weights"},{"location":"zh/modelzoo/yolov8/#_5","title":"\u8bf4\u660e","text":"<ul> <li>Box mAP\uff1a\u9a8c\u8bc1\u96c6\u4e0a\u6d4b\u8bd5\u51fa\u7684\u51c6\u786e\u5ea6\u3002</li> <li>\u6211\u4eec\u53c2\u8003\u4e86\u5e38\u7528\u7684\u7b2c\u4e09\u65b9 YOLOV8 \u91cd\u73b0\u4e86P5\uff08\u5927\u76ee\u6807\uff09\u7cfb\u5217\u6a21\u578b\u3002</li> </ul>"},{"location":"zh/modelzoo/yolov8/#_6","title":"\u5feb\u901f\u5165\u95e8","text":"<p>\u8be6\u60c5\u8bf7\u53c2\u9605 MindYOLO \u4e2d\u7684 \u5feb\u901f\u5165\u95e8\u3002</p>"},{"location":"zh/modelzoo/yolov8/#_7","title":"\u8bad\u7ec3","text":""},{"location":"zh/modelzoo/yolov8/#-","title":"- \u5206\u5e03\u5f0f\u8bad\u7ec3","text":"<p>\u4f7f\u7528\u9884\u7f6e\u7684\u8bad\u7ec3\u914d\u65b9\u53ef\u4ee5\u8f7b\u677e\u91cd\u73b0\u62a5\u544a\u7684\u7ed3\u679c\u3002\u5982\u9700\u5728\u591a\u53f0Ascend 910\u8bbe\u5907\u4e0a\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u8bf7\u8fd0\u884c <pre><code># \u5728\u591a\u53f0Ascend\u8bbe\u5907\u4e0a\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\nmsrun --worker_num=8 --local_worker_num=8 --bind_core=True --log_dir=./yolov8_log python train.py --config ./configs/yolov8/yolov8n.yaml --device_target Ascend --is_parallel True\n</code></pre></p> <p>\u6ce8\u610f: \u66f4\u591a\u5173\u4e8emsrun\u914d\u7f6e\u7684\u4fe1\u606f\uff0c\u8bf7\u53c2\u8003\u8fd9\u91cc\u3002</p> <p>\u6709\u5173\u6240\u6709\u8d85\u53c2\u6570\u7684\u8be6\u7ec6\u8bf4\u660e\uff0c\u8bf7\u53c2\u9605config.py\u3002</p> <p>\u6ce8\u610f\uff1a \u7531\u4e8e\u5168\u5c40batch size\uff08batch_size x \u8bbe\u5907\u6570\uff09\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u8d85\u53c2\u6570\uff0c\u5efa\u8bae\u4fdd\u6301\u5168\u5c40batch size\u4e0d\u53d8\u8fdb\u884c\u590d\u5236\uff0c\u6216\u8005\u5c06\u5b66\u4e60\u7387\u7ebf\u6027\u8c03\u6574\u4e3a\u65b0\u7684\u5168\u5c40batch size\u3002</p>"},{"location":"zh/modelzoo/yolov8/#-_1","title":"- \u5355\u5361\u8bad\u7ec3","text":"<p>\u5982\u679c\u60a8\u60f3\u5728\u8f83\u5c0f\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6216\u5fae\u8c03\u6a21\u578b\u800c\u4e0d\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u8bf7\u8fd0\u884c\uff1a</p> <pre><code># \u5728 CPU/Ascend \u8bbe\u5907\u4e0a\u8fdb\u884c\u5355\u5361\u8bad\u7ec3\npython train.py --config ./configs/yolov8/yolov8n.yaml --device_target Ascend\n</code></pre>"},{"location":"zh/modelzoo/yolov8/#_8","title":"\u9a8c\u8bc1\u548c\u6d4b\u8bd5","text":"<p>\u8981\u9a8c\u8bc1\u8bad\u7ec3\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528 <code>test.py</code> \u5e76\u4f7f\u7528 <code>--weight</code> \u4f20\u5165\u6743\u91cd\u8def\u5f84\u3002</p> <pre><code>python test.py --config ./configs/yolov8/yolov8n.yaml --device_target Ascend --weight /PATH/TO/WEIGHT.ckpt\n</code></pre>"},{"location":"zh/modelzoo/yolov8/#_9","title":"\u90e8\u7f72","text":"<p>\u8be6\u89c1 \u90e8\u7f72\u3002</p>"},{"location":"zh/modelzoo/yolov8/#_10","title":"\u5f15\u7528","text":"<p>[1] Jocher Glenn. Ultralytics YOLOv8. https://github.com/ultralytics/ultralytics, 2023.</p>"},{"location":"zh/modelzoo/yolox/#_1","title":"\u6458\u8981","text":"<p>YOLOX \u662f\u4e00\u6b3e\u65b0\u578b\u9ad8\u6027\u80fd\u68c0\u6d4b\u6a21\u578b\uff0c\u5728 YOLO \u7cfb\u5217\u7684\u57fa\u7840\u4e0a\u8fdb\u884c\u4e86\u4e00\u4e9b\u7ecf\u9a8c\u4e30\u5bcc\u7684\u6539\u8fdb\u3002\u6211\u4eec\u5c06 YOLO \u68c0\u6d4b\u5668\u6539\u4e3a\u65e0\u951a\u65b9\u5f0f\uff0c\u5e76\u91c7\u7528\u5176\u4ed6\u5148\u8fdb\u7684\u68c0\u6d4b\u6280\u672f\uff0c\u4f8b\u5982\u89e3\u8026\u5934\u548c\u9886\u5148\u7684\u6807\u7b7e\u5206\u914d\u7b56\u7565 SimOTA\uff0c\u4ee5\u5728\u5927\u89c4\u6a21\u6a21\u578b\u4e2d\u5b9e\u73b0\u6700\u4f73\u6548\u679c\uff1a\u5bf9\u4e8e\u53ea\u6709 0.91M \u53c2\u6570\u548c 1.08G FLOPs \u7684 YOLO-Nano\uff0c\u6211\u4eec\u5728 COCO \u4e0a\u83b7\u5f97\u4e86 25.3% \u7684 AP\uff0c\u6bd4 NanoDet \u9ad8\u51fa 1.8% AP\uff1b\u5bf9\u4e8e\u4e1a\u754c\u4f7f\u7528\u6700\u5e7f\u6cdb\u7684\u68c0\u6d4b\u5668\u4e4b\u4e00 YOLOv3\uff0c\u6211\u4eec\u5c06\u5176\u5728 COCO \u4e0a\u7684 AP \u63d0\u5347\u5230 47.3%\uff0c\u6bd4\u76ee\u524d\u7684\u6700\u4f73\u5b9e\u8df5\u9ad8\u51fa 3.0% AP\uff1b\u5bf9\u4e8e\u53c2\u6570\u91cf\u4e0e YOLOv4-CSP \u5927\u81f4\u76f8\u540c\u7684 YOLOX-L\uff0cYOLOv5-L \u5728 Tesla V100 \u4e0a\u4ee5 68.9 FPS \u7684\u901f\u5ea6\u5728 COCO \u4e0a\u5b9e\u73b0\u4e86 50.0% \u7684 AP\uff0c\u6bd4 YOLOv5-L \u9ad8\u51fa 1.8% \u7684 AP\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4f7f\u7528\u5355\u4e2a YOLOX-L \u6a21\u578b\u5728\u6d41\u5f0f\u611f\u77e5\u6311\u6218\u8d5b\uff08CVPR 2021 \u81ea\u52a8\u9a7e\u9a76\u7814\u8ba8\u4f1a\uff09\u4e0a\u83b7\u5f97\u4e86\u7b2c\u4e00\u540d\u3002</p>"},{"location":"zh/modelzoo/yolox/#_2","title":"\u7ed3\u679c","text":"\u4f7f\u7528\u56fe\u6a21\u5f0f\u5728 Ascend 910(8p) \u4e0a\u6d4b\u8bd5\u7684\u8868\u73b0 Name Scale BatchSize ImageSize Dataset Box mAP (%) Params Recipe Download YOLOX N 8 * 8 416 MS COCO 2017 24.1 0.9M yaml weights YOLOX Tiny 8 * 8 416 MS COCO 2017 33.3 5.1M yaml weights YOLOX S 8 * 8 640 MS COCO 2017 40.7 9.0M yaml weights YOLOX M 8 * 8 640 MS COCO 2017 46.7 25.3M yaml weights YOLOX L 8 * 8 640 MS COCO 2017 49.2 54.2M yaml weights YOLOX X 8 * 8 640 MS COCO 2017 51.6 99.1M yaml weights YOLOX Darknet53 8 * 8 640 MS COCO 2017 47.7 63.7M yaml weights \u5728Ascend 910*(8p)\u4e0a\u6d4b\u8bd5\u7684\u8868\u73b0 Name Scale BatchSize ImageSize Dataset Box mAP (%) ms/step Params Recipe Download YOLOX S 8 * 8 640 MS COCO 2017 41.0 242.15 9.0M yaml weights"},{"location":"zh/modelzoo/yolox/#_3","title":"\u8bf4\u660e","text":"<ul> <li>Box mAP: \u9a8c\u8bc1\u96c6\u4e0a\u6d4b\u8bd5\u51fa\u7684\u51c6\u786e\u5ea6\u3002</li> <li>\u6211\u4eec\u53c2\u8003\u4e86\u5b98\u65b9\u7684 YOLOX \u6765\u91cd\u73b0\u7ed3\u679c.</li> </ul>"},{"location":"zh/modelzoo/yolox/#_4","title":"\u5feb\u901f\u5165\u95e8","text":"<p>\u8be6\u60c5\u8bf7\u53c2\u9605 MindYOLO \u4e2d\u7684 \u5feb\u901f\u5165\u95e8\u3002</p>"},{"location":"zh/modelzoo/yolox/#_5","title":"\u8bad\u7ec3","text":""},{"location":"zh/modelzoo/yolox/#-","title":"- \u5206\u5e03\u5f0f\u8bad\u7ec3","text":"<p>\u4f7f\u7528\u9884\u7f6e\u7684\u8bad\u7ec3\u914d\u65b9\u53ef\u4ee5\u8f7b\u677e\u91cd\u73b0\u62a5\u544a\u7684\u7ed3\u679c\u3002\u5982\u9700\u5728\u591a\u53f0Ascend 910\u8bbe\u5907\u4e0a\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u8bf7\u8fd0\u884c <pre><code># \u5728\u591a\u53f0Ascend\u8bbe\u5907\u4e0a\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\nmsrun --worker_num=8 --local_worker_num=8 --bind_core=True --log_dir=./yolox_log python train.py --config ./configs/yolox/yolox-s.yaml --device_target Ascend --is_parallel True\n</code></pre></p> <p>\u6ce8\u610f: \u66f4\u591a\u5173\u4e8emsrun\u914d\u7f6e\u7684\u4fe1\u606f\uff0c\u8bf7\u53c2\u8003\u8fd9\u91cc\u3002</p> <p>\u6709\u5173\u6240\u6709\u8d85\u53c2\u6570\u7684\u8be6\u7ec6\u8bf4\u660e\uff0c\u8bf7\u53c2\u9605config.py\u3002</p> <p>**\u6ce8\u610f\uff1a**\u7531\u4e8e\u5168\u5c40batch size\uff08batch_size x \u8bbe\u5907\u6570\uff09\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u8d85\u53c2\u6570\uff0c\u5efa\u8bae\u4fdd\u6301\u5168\u5c40batch size\u4e0d\u53d8\u8fdb\u884c\u590d\u5236\uff0c\u6216\u8005\u5c06\u5b66\u4e60\u7387\u7ebf\u6027\u8c03\u6574\u4e3a\u65b0\u7684\u5168\u5c40batch size\u3002</p>"},{"location":"zh/modelzoo/yolox/#-_1","title":"- \u5355\u5361\u8bad\u7ec3","text":"<p>\u5982\u679c\u60a8\u60f3\u5728\u8f83\u5c0f\u7684\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6216\u5fae\u8c03\u6a21\u578b\u800c\u4e0d\u8fdb\u884c\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u8bf7\u8fd0\u884c\uff1a</p> <pre><code># \u5728 CPU/Ascend \u8bbe\u5907\u4e0a\u8fdb\u884c\u5355\u5361\u8bad\u7ec3\npython train.py --config ./configs/yolox/yolox-s.yaml --device_target Ascend\n</code></pre>"},{"location":"zh/modelzoo/yolox/#_6","title":"\u9a8c\u8bc1\u548c\u6d4b\u8bd5","text":"<p>\u8981\u9a8c\u8bc1\u8bad\u7ec3\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528 <code>test.py</code> \u5e76\u4f7f\u7528 <code>--weight</code> \u4f20\u5165\u6743\u91cd\u8def\u5f84\u3002</p> <pre><code>python test.py --config ./configs/yolox/yolox-s.yaml --device_target Ascend --weight /PATH/TO/WEIGHT.ckpt\n</code></pre>"},{"location":"zh/modelzoo/yolox/#_7","title":"\u90e8\u7f72","text":"<p>\u8be6\u89c1 \u90e8\u7f72\u3002</p>"},{"location":"zh/modelzoo/yolox/#_8","title":"\u5f15\u7528","text":"<p>[1] Zheng Ge. YOLOX: Exceeding YOLO Series in 2021. https://arxiv.org/abs/2107.08430, 2021.</p>"},{"location":"zh/notes/changelog/","title":"\u66f4\u65b0\u65e5\u5fd7","text":"<p>\u5373\u5c06\u5230\u6765</p>"},{"location":"zh/notes/code_of_conduct/","title":"\u884c\u4e3a\u51c6\u5219","text":"<p>\u5373\u5c06\u5230\u6765</p>"},{"location":"zh/notes/contributing/","title":"MindYOLO \u8d21\u732e\u6307\u5357","text":""},{"location":"zh/notes/contributing/#_1","title":"\u8d21\u732e\u8005\u8bb8\u53ef\u534f\u8bae","text":"<p>\u9996\u6b21\u5411 MindYOLO \u793e\u533a\u63d0\u4ea4\u4ee3\u7801\u524d\uff0c\u9700\u7b7e\u7f72 CLA\u3002</p> <p>\u4e2a\u4eba\u8d21\u732e\u8005\u8bf7\u53c2\u8003 ICLA \u5728\u7ebf\u6587\u6863 \u4e86\u89e3\u8be6\u7ec6\u4fe1\u606f\u3002</p>"},{"location":"zh/notes/contributing/#_2","title":"\u5165\u95e8\u6307\u5357","text":"<ul> <li>\u5728 Github \u4e0a Fork \u4ee3\u7801\u5e93\u3002</li> <li>\u9605\u8bfb README.md\u3002</li> </ul>"},{"location":"zh/notes/contributing/#_3","title":"\u8d21\u732e\u6d41\u7a0b","text":""},{"location":"zh/notes/contributing/#_4","title":"\u4ee3\u7801\u98ce\u683c","text":"<p>\u8bf7\u9075\u5faa\u6b64\u98ce\u683c\uff0c\u4ee5\u4fbf MindYOLO \u6613\u4e8e\u5ba1\u67e5\u3001\u7ef4\u62a4\u548c\u5f00\u53d1\u3002</p> <ul> <li>\u7f16\u7801\u6307\u5357</li> </ul> <p>MindYOLO \u793e\u533a\u4f7f\u7528 Python PEP 8 \u7f16\u7801\u98ce\u683c \u5efa\u8bae\u7684 Python \u7f16\u7801\u98ce\u683c\u548c Google C++ \u7f16\u7801\u6307\u5357 \u5efa\u8bae\u7684 C++ \u7f16\u7801\u98ce\u683c\u3002 CppLint\u3001CppCheck\u3001CMakeLint\u3001CodeSpell\u3001Lizard\u3001ShellCheck \u548c PyLint \u7528\u4e8e\u68c0\u67e5\u4ee3\u7801\u683c\u5f0f\uff0c\u5efa\u8bae\u5728 IDE \u4e2d\u5b89\u88c5\u8fd9\u4e9b\u63d2\u4ef6\u3002</p> <ul> <li>\u5355\u5143\u6d4b\u8bd5\u6307\u5357</li> </ul> <p>MindYOLO \u793e\u533a\u4f7f\u7528 pytest \u5efa\u8bae\u7684 Python \u5355\u5143\u6d4b\u8bd5\u98ce\u683c\u548c Googletest Primer \u5efa\u8bae\u7684 C++ \u5355\u5143\u6d4b\u8bd5\u98ce\u683c\u3002\u6d4b\u8bd5\u7528\u4f8b\u7684\u8bbe\u8ba1\u610f\u56fe\u5e94\u8be5\u901a\u8fc7\u5176\u6ce8\u91ca\u540d\u79f0\u6765\u4f53\u73b0\u3002</p> <ul> <li>\u91cd\u6784\u6307\u5357</li> </ul> <p>\u6211\u4eec\u9f13\u52b1\u5f00\u53d1\u4eba\u5458\u91cd\u6784\u6211\u4eec\u7684\u4ee3\u7801\u4ee5\u6d88\u9664 \u4ee3\u7801\u5f02\u5473\u3002\u6240\u6709\u4ee3\u7801\u90fd\u5e94\u7b26\u5408\u7f16\u7801\u98ce\u683c\u548c\u6d4b\u8bd5\u98ce\u683c\u7684\u9700\u6c42\uff0c\u91cd\u6784\u4ee3\u7801\u4e5f\u4e0d\u4f8b\u5916\u3002Lizard \u5bf9 nloc\uff08\u65e0\u6ce8\u91ca\u7684\u4ee3\u7801\u884c\u6570\uff09\u7684\u9608\u503c\u4e3a 100\uff0c\u5bf9 cnc\uff08\u5faa\u73af\u590d\u6742\u5ea6\u6570\uff09\u7684\u9608\u503c\u4e3a 20\uff0c\u5f53\u60a8\u6536\u5230 Lizard \u8b66\u544a\u65f6\uff0c\u60a8\u5fc5\u987b\u91cd\u6784\u8981\u5408\u5e76\u7684\u4ee3\u7801\u3002</p> <ul> <li>\u6587\u6863\u6307\u5357</li> </ul> <p>\u6211\u4eec\u4f7f\u7528 MarkdownLint \u68c0\u67e5 markdown \u6587\u6863\u7684\u683c\u5f0f\u3002MindYOLO CI \u6839\u636e\u9ed8\u8ba4\u914d\u7f6e\u4fee\u6539\u4e86\u4ee5\u4e0b\u89c4\u5219\u3002</p> <ul> <li>MD007\uff08\u65e0\u5e8f\u5217\u8868\u7f29\u8fdb\uff09\uff1aindent**\u53c2\u6570\u8bbe\u7f6e\u4e3a**4\uff0c\u8868\u793a\u65e0\u5e8f\u5217\u8868\u4e2d\u7684\u6240\u6709\u5185\u5bb9\u90fd\u9700\u8981\u4f7f\u7528\u56db\u4e2a\u7a7a\u683c\u8fdb\u884c\u7f29\u8fdb\u3002</li> <li>MD009\uff08\u884c\u672b\u7a7a\u683c\uff09\uff1abr_spaces**\u53c2\u6570\u8bbe\u7f6e\u4e3a**2\uff0c\u8868\u793a\u884c\u672b\u53ef\u4ee5\u67090\u4e2a\u62162\u4e2a\u7a7a\u683c\u3002</li> <li>MD029\uff08\u6709\u5e8f\u5217\u8868\u7684\u5e8f\u53f7\uff09\uff1astyle**\u53c2\u6570\u8bbe\u7f6e\u4e3a**ordered\uff0c\u8868\u793a\u6709\u5e8f\u5217\u8868\u7684\u5e8f\u53f7\u6309\u5347\u5e8f\u6392\u5217\u3002</li> </ul> <p>\u5177\u4f53\u8bf7\u53c2\u89c1RULES\u3002</p>"},{"location":"zh/notes/contributing/#fork-pull","title":"Fork-Pull\u5f00\u53d1\u6a21\u5f0f","text":"<ul> <li>Fork MindYOLO\u4ed3\u5e93</li> </ul> <p>\u5728\u5411MindYOLO\u9879\u76ee\u63d0\u4ea4\u4ee3\u7801\u4e4b\u524d\uff0c\u8bf7\u786e\u4fdd\u8be5\u9879\u76ee\u5df2\u7ecffork\u5230\u4f60\u81ea\u5df1\u7684\u4ed3\u5e93\u3002\u8fd9\u610f\u5473\u7740MindYOLO \u4ed3\u5e93\u548c\u4f60\u81ea\u5df1\u7684\u4ed3\u5e93\u4e4b\u95f4\u4f1a\u5e76\u884c\u5f00\u53d1\uff0c\u6240\u4ee5\u8981\u5c0f\u5fc3\u907f\u514d\u4e24\u8005\u4e0d\u4e00\u81f4\u3002</p> <ul> <li>\u514b\u9686\u8fdc\u7a0b\u4ed3\u5e93</li> </ul> <p>\u5982\u679c\u8981\u5c06\u4ee3\u7801\u4e0b\u8f7d\u5230\u672c\u5730\u673a\u5668\uff0c<code>git</code> \u662f\u6700\u597d\u7684\u65b9\u5f0f\uff1a</p> <pre><code># \u5bf9\u4e8e GitHub\ngit clone https://github.com/{insert_your_forked_repo}/mindyolo.git\ngit remote add upper https://github.com/mindspore-lab/mindyolo.git\n</code></pre> <ul> <li>\u672c\u5730\u5f00\u53d1\u4ee3\u7801</li> </ul> <p>\u4e3a\u907f\u514d\u591a\u4e2a\u5206\u652f\u4e4b\u95f4\u4e0d\u4e00\u81f4\uff0c<code>SUGGESTED</code> \u5efa\u8bae\u7b7e\u51fa\u5230\u65b0\u5206\u652f\uff1a</p> <pre><code>git checkout -b {new_branch_name} origin/master\n</code></pre> <p>\u4ee5 master \u5206\u652f\u4e3a\u4f8b\uff0cMindYOLO \u53ef\u80fd\u4f1a\u6839\u636e\u9700\u8981\u521b\u5efa\u7248\u672c\u5206\u652f\u548c\u4e0b\u6e38\u5f00\u53d1\u5206\u652f\uff0c\u8bf7\u5148\u4fee\u590d\u4e0a\u6e38\u7684 bug\u3002 \u7136\u540e\u4f60\u53ef\u4ee5\u4efb\u610f\u66f4\u6539\u4ee3\u7801\u3002</p> <ul> <li>\u5c06\u4ee3\u7801\u63a8\u9001\u5230\u8fdc\u7a0b\u4ed3\u5e93</li> </ul> <p>\u66f4\u65b0\u4ee3\u7801\u540e\uff0c\u5e94\u4ee5\u6b63\u5f0f\u65b9\u5f0f\u63a8\u9001\u66f4\u65b0\uff1a</p> <pre><code>git add .\ngit status # \u68c0\u67e5\u66f4\u65b0\u72b6\u6001\ngit commit -m \"\u60a8\u7684\u63d0\u4ea4\u6807\u9898\"\ngit commit -s --amend #\u6dfb\u52a0\u63d0\u4ea4\u7684\u5177\u4f53\u63cf\u8ff0\ngit push origin {new_branch_name}\n</code></pre> <ul> <li>\u5c06\u8bf7\u6c42\u62c9\u53d6\u5230 MindYOLO \u4ed3\u5e93</li> </ul> <p>\u6700\u540e\u4e00\u6b65\uff0c\u60a8\u9700\u8981\u5c06\u65b0\u5206\u652f\u4e0e MindYOLO <code>master</code> \u5206\u652f\u8fdb\u884c\u6bd4\u8f83\u3002\u5b8c\u6210\u62c9\u53d6\u8bf7\u6c42\u540e\uff0cJenkins CI \u5c06\u81ea\u52a8\u8bbe\u7f6e\u4e3a\u6784\u5efa\u6d4b\u8bd5\u3002\u60a8\u7684\u62c9\u53d6\u8bf7\u6c42\u5e94\u5c3d\u5feb\u5408\u5e76\u5230\u4e0a\u6e38\u4e3b\u5206\u652f\u4e2d\uff0c\u4ee5\u964d\u4f4e\u5408\u5e76\u98ce\u9669\u3002</p>"},{"location":"zh/notes/contributing/#_5","title":"\u62a5\u544a\u95ee\u9898","text":"<p>\u4e3a\u9879\u76ee\u505a\u51fa\u8d21\u732e\u7684\u4e00\u79cd\u597d\u65b9\u6cd5\u662f\u5728\u9047\u5230\u95ee\u9898\u65f6\u53d1\u9001\u8be6\u7ec6\u62a5\u544a\u3002\u6211\u4eec\u59cb\u7ec8\u6b23\u8d4f\u5199\u5f97\u597d\u3001\u8be6\u5c3d\u7684\u9519\u8bef\u62a5\u544a\uff0c\u5e76\u4f1a\u4e3a\u6b64\u611f\u8c22\u60a8\uff01</p> <p>\u62a5\u544a\u95ee\u9898\u65f6\uff0c\u8bf7\u53c2\u8003\u4ee5\u4e0b\u683c\u5f0f\uff1a</p> <ul> <li>\u60a8\u4f7f\u7528\u7684\u662f\u54ea\u4e2a\u7248\u672c\u7684\u73af\u5883\uff08MindSpore\u3001os\u3001python\u3001MindYOLO \u7b49\uff09\uff1f</li> <li>\u8fd9\u662f\u9519\u8bef\u62a5\u544a\u8fd8\u662f\u529f\u80fd\u8bf7\u6c42\uff1f</li> <li>\u4ec0\u4e48\u7c7b\u578b\u7684\u95ee\u9898\uff0c\u8bf7\u6dfb\u52a0\u6807\u7b7e\u4ee5\u5728\u95ee\u9898\u4eea\u8868\u677f\u4e0a\u7a81\u51fa\u663e\u793a\u5b83\u3002</li> <li>\u53d1\u751f\u4e86\u4ec0\u4e48\uff1f</li> <li>\u60a8\u671f\u671b\u53d1\u751f\u4ec0\u4e48\uff1f</li> <li>\u5982\u4f55\u91cd\u73b0\u5b83\uff1f\uff08\u5c3d\u53ef\u80fd\u7b80\u77ed\u548c\u51c6\u786e\uff09</li> <li>\u7ed9\u5ba1\u9605\u8005\u7684\u7279\u522b\u8bf4\u660e\uff1f</li> </ul> <p>\u95ee\u9898\u54a8\u8be2\uff1a</p> <ul> <li>\u5982\u679c\u60a8\u53d1\u73b0\u4e00\u4e2a\u672a\u5173\u95ed\u7684\u95ee\u9898\uff0c\u800c\u8fd9\u6b63\u662f\u60a8\u8981\u89e3\u51b3\u7684\u95ee\u9898\uff0c \u8bf7\u5728\u8be5\u95ee\u9898\u4e0a\u53d1\u8868\u4e00\u4e9b\u8bc4\u8bba\uff0c\u544a\u8bc9\u5176\u4ed6\u4eba\u60a8\u5c06\u8d1f\u8d23\u8be5\u95ee\u9898\u3002</li> <li>\u5982\u679c\u95ee\u9898\u6253\u5f00\u4e86\u4e00\u6bb5\u65f6\u95f4\uff0c \u5efa\u8bae\u8d21\u732e\u8005\u5728\u89e3\u51b3\u8be5\u95ee\u9898\u4e4b\u524d\u8fdb\u884c\u9884\u68c0\u67e5\u3002</li> <li>\u5982\u679c\u60a8\u89e3\u51b3\u4e86\u81ea\u5df1\u62a5\u544a\u7684\u95ee\u9898\uff0c \u4e5f\u9700\u8981\u5728\u5173\u95ed\u8be5\u95ee\u9898\u4e4b\u524d\u901a\u77e5\u5176\u4ed6\u4eba\u3002</li> <li>\u5982\u679c\u60a8\u5e0c\u671b\u95ee\u9898\u5c3d\u5feb\u5f97\u5230\u56de\u590d\uff0c \u8bf7\u5c1d\u8bd5\u4e3a\u5176\u6dfb\u52a0\u6807\u7b7e\uff0c\u60a8\u53ef\u4ee5\u5728 \u6807\u7b7e\u5217\u8868 \u4e0a\u627e\u5230\u5404\u79cd\u6807\u7b7e</li> </ul>"},{"location":"zh/notes/contributing/#pr","title":"\u63d0\u51fa PR","text":"<ul> <li> <p>\u5728 GitHub \u4e0a\u4ee5 issue \u5f62\u5f0f\u63d0\u51fa\u60a8\u7684\u60f3\u6cd5</p> </li> <li> <p>\u5982\u679c\u662f\u9700\u8981\u5927\u91cf\u8bbe\u8ba1\u7ec6\u8282\u7684\u65b0\u529f\u80fd\uff0c\u8fd8\u5e94\u63d0\u4ea4\u8bbe\u8ba1\u63d0\u6848\u3002</p> </li> <li> <p>\u5728\u95ee\u9898\u8ba8\u8bba\u548c\u8bbe\u8ba1\u63d0\u6848\u5ba1\u67e5\u4e2d\u8fbe\u6210\u5171\u8bc6\u540e\uff0c\u5b8c\u6210\u5206\u53c9\u4ed3\u5e93\u7684\u5f00\u53d1\u5e76\u63d0\u4ea4 PR\u3002</p> </li> <li> <p>\u4efb\u4f55 PR \u90fd\u5fc5\u987b\u6536\u5230\u6765\u81ea\u6279\u51c6\u8005\u7684 2+ LGTM \u624d\u80fd\u88ab\u5141\u8bb8\u3002\u8bf7\u6ce8\u610f\uff0c\u6279\u51c6\u8005\u4e0d\u5f97\u5728\u81ea\u5df1\u7684 PR \u4e0a\u6dfb\u52a0 LGTM\u3002</p> </li> <li> <p>PR \u7ecf\u8fc7\u5145\u5206\u8ba8\u8bba\u540e\uff0c\u5c06\u6839\u636e\u8ba8\u8bba\u7ed3\u679c\u8fdb\u884c\u5408\u5e76\u3001\u653e\u5f03\u6216\u62d2\u7edd\u3002</p> </li> </ul> <p>PR \u5efa\u8bae\uff1a</p> <ul> <li>\u5e94\u907f\u514d\u4efb\u4f55\u4e0d\u76f8\u5173\u7684\u66f4\u6539\u3002</li> <li>\u786e\u4fdd\u60a8\u7684\u63d0\u4ea4\u5386\u53f2\u8bb0\u5f55\u6709\u5e8f\u3002</li> <li>\u59cb\u7ec8\u8ba9\u60a8\u7684\u5206\u652f\u4e0e\u4e3b\u5206\u652f\u4fdd\u6301\u4e00\u81f4\u3002</li> <li>\u5bf9\u4e8e\u9519\u8bef\u4fee\u590d PR\uff0c\u8bf7\u786e\u4fdd\u6240\u6709\u76f8\u5173\u95ee\u9898\u90fd\u5df2\u94fe\u63a5\u3002</li> </ul>"},{"location":"zh/notes/faq/","title":"\u5e38\u89c1\u95ee\u9898","text":"<p>\u5373\u5c06\u5230\u6765</p>"},{"location":"zh/reference/data/","title":"\u6570\u636e","text":""},{"location":"zh/reference/data/#_2","title":"\u6570\u636e\u52a0\u8f7d","text":""},{"location":"zh/reference/data/#_3","title":"\u6570\u636e\u96c6","text":""},{"location":"zh/reference/models/","title":"\u6a21\u578b","text":""},{"location":"zh/reference/models/#_2","title":"\u521b\u5efa\u6a21\u578b","text":""},{"location":"zh/tutorials/configuration/","title":"\u914d\u7f6e","text":"<p>MindYOLO\u5957\u4ef6\u540c\u65f6\u652f\u6301yaml\u6587\u4ef6\u53c2\u6570\u548c\u547d\u4ee4\u884c\u53c2\u6570\u89e3\u6790\uff0c\u5e76\u5c06\u76f8\u5bf9\u56fa\u5b9a\u3001\u4e0e\u6a21\u578b\u5f3a\u76f8\u5173\u3001\u8f83\u4e3a\u590d\u6742\u6216\u8005\u542b\u6709\u5d4c\u5957\u7ed3\u6784\u7684\u53c2\u6570\u7f16\u5199\u6210yaml\u6587\u4ef6\uff0c\u9700\u6839\u636e\u5b9e\u9645\u5e94\u7528\u573a\u666f\u66f4\u6539\u6216\u8005\u8f83\u4e3a\u7b80\u5355\u7684\u53c2\u6570\u5219\u901a\u8fc7\u547d\u4ee4\u884c\u4f20\u5165\u3002</p> <p>\u4e0b\u9762\u4ee5yolov3\u4e3a\u4f8b\uff0c\u89e3\u91ca\u5982\u4f55\u914d\u7f6e\u76f8\u5e94\u7684\u53c2\u6570\u3002</p>"},{"location":"zh/tutorials/configuration/#_2","title":"\u53c2\u6570\u7ee7\u627f\u5173\u7cfb","text":"<p>\u53c2\u6570\u4f18\u5148\u7ea7\u7531\u9ad8\u5230\u4f4e\u5982\u4e0b\uff0c\u51fa\u73b0\u540c\u540d\u53c2\u6570\u65f6\uff0c\u4f4e\u4f18\u5148\u7ea7\u53c2\u6570\u4f1a\u88ab\u9ad8\u4f18\u5148\u7ea7\u53c2\u6570\u8986\u76d6</p> <ul> <li>\u7528\u6237\u547d\u4ee4\u884c\u4f20\u5165\u53c2\u6570</li> <li>python\u6267\u884cpy\u6587\u4ef6\u4e2dparser\u7684\u9ed8\u8ba4\u53c2\u6570</li> <li>\u547d\u4ee4\u884c\u4f20\u5165config\u53c2\u6570\u5bf9\u5e94\u7684yaml\u6587\u4ef6\u53c2\u6570</li> <li>\u547d\u4ee4\u884c\u4f20\u5165config\u53c2\u6570\u5bf9\u5e94\u7684yaml\u6587\u4ef6\u4e2d__BASE__\u53c2\u6570\u4e2d\u5305\u542b\u7684yaml\u6587\u4ef6\u53c2\u6570\uff0c\u4f8b\u5982yolov3.yaml\u542b\u6709\u5982\u4e0b\u53c2\u6570\uff1a <pre><code>__BASE__: [\n'../coco.yaml',\n'./hyp.scratch.yaml',\n]\n</code></pre></li> </ul>"},{"location":"zh/tutorials/configuration/#_3","title":"\u57fa\u7840\u53c2\u6570","text":""},{"location":"zh/tutorials/configuration/#_4","title":"\u53c2\u6570\u8bf4\u660e","text":"<ul> <li>device_target: \u6240\u7528\u8bbe\u5907\uff0cAscend/CPU</li> <li>save_dir: \u8fd0\u884c\u7ed3\u679c\u4fdd\u5b58\u8def\u5f84\uff0c\u9ed8\u8ba4\u4e3a./runs</li> <li>log_interval: \u6253\u5370\u65e5\u5fd7step\u95f4\u9694\uff0c\u9ed8\u8ba4\u4e3a100</li> <li>is_parallel: \u662f\u5426\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u9ed8\u8ba4\u4e3aFalse</li> <li>ms_mode: \u4f7f\u7528\u9759\u6001\u56fe\u6a21\u5f0f(0)\u6216\u52a8\u6001\u56fe\u6a21\u5f0f(1)\uff0c\u9ed8\u8ba4\u4e3a0\u3002</li> <li>config: yaml\u914d\u7f6e\u6587\u4ef6\u8def\u5f84</li> <li>per_batch_size: \u6bcf\u5f20\u5361batch size\uff0c\u9ed8\u8ba4\u4e3a32</li> <li>epochs: \u8bad\u7ec3epoch\u6570\uff0c\u9ed8\u8ba4\u4e3a300</li> <li>...</li> </ul>"},{"location":"zh/tutorials/configuration/#parse","title":"parse\u53c2\u6570\u8bbe\u7f6e","text":"<p>\u8be5\u90e8\u5206\u53c2\u6570\u901a\u5e38\u7531\u547d\u4ee4\u884c\u4f20\u5165\uff0c\u793a\u4f8b\u5982\u4e0b\uff1a</p> <pre><code>msrun --worker_num=8 --local_worker_num=8 --bind_core=True --log_dir=./yolov7_log python train.py --config ./configs/yolov7/yolov7.yaml  --is_parallel True --log_interval 50\n</code></pre>"},{"location":"zh/tutorials/configuration/#_5","title":"\u6570\u636e\u96c6","text":""},{"location":"zh/tutorials/configuration/#_6","title":"\u53c2\u6570\u8bf4\u660e","text":"<ul> <li>dataset_name: \u6570\u636e\u96c6\u540d\u79f0</li> <li>train_set: \u8bad\u7ec3\u96c6\u6240\u5728\u8def\u5f84</li> <li>val_set: \u9a8c\u8bc1\u96c6\u6240\u5728\u8def\u5f84</li> <li>test_set: \u6d4b\u8bd5\u96c6\u6240\u5728\u8def\u5f84</li> <li>nc: \u6570\u636e\u96c6\u7c7b\u522b\u6570</li> <li>names: \u7c7b\u522b\u540d\u79f0</li> <li>...</li> </ul>"},{"location":"zh/tutorials/configuration/#yaml","title":"yaml\u6587\u4ef6\u6837\u4f8b","text":"<p>\u8be5\u90e8\u5206\u53c2\u6570\u5728configs/coco.yaml\u4e2d\u5b9a\u4e49\uff0c\u901a\u5e38\u9700\u4fee\u6539\u5176\u4e2d\u7684\u6570\u636e\u96c6\u8def\u5f84</p> <pre><code>data:\ndataset_name: coco\n\ntrain_set: ./coco/train2017.txt  # 118287 images\nval_set: ./coco/val2017.txt  # 5000 images\ntest_set: ./coco/test-dev2017.txt  # 20288 of 40670 images, submit to https://competitions.codalab.org/competitions/20794\n\nnc: 80\n\n# class names\nnames: [ 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n'hair drier', 'toothbrush' ]\n</code></pre>"},{"location":"zh/tutorials/configuration/#_7","title":"\u6570\u636e\u589e\u5f3a","text":""},{"location":"zh/tutorials/configuration/#_8","title":"\u53c2\u6570\u8bf4\u660e","text":"<ul> <li>num_parallel_workers: \u8bfb\u53d6\u6570\u636e\u7684\u5de5\u4f5c\u8fdb\u7a0b\u6570</li> <li>train_transformers: \u8bad\u7ec3\u8fc7\u7a0b\u6570\u636e\u589e\u5f3a</li> <li>test_transformers: \u9a8c\u8bc1\u8fc7\u7a0b\u6570\u636e\u589e\u5f3a</li> <li>...</li> </ul>"},{"location":"zh/tutorials/configuration/#yaml_1","title":"yaml\u6587\u4ef6\u6837\u4f8b","text":"<p>\u8be5\u90e8\u5206\u53c2\u6570\u5728configs/yolov3/hyp.scratch.yaml\u4e2d\u5b9a\u4e49\uff0c\u5176\u4e2dtrain_transformers\u548ctest_transformers\u5747\u4e3a\u7531\u5b57\u5178\u7ec4\u6210\u7684\u5217\u8868\uff0c\u5404\u5b57\u5178\u5305\u542b\u6570\u636e\u589e\u5f3a\u64cd\u4f5c\u540d\u79f0\u3001\u53d1\u751f\u6982\u7387\u53ca\u8be5\u589e\u5f3a\u65b9\u6cd5\u76f8\u5173\u7684\u53c2\u6570</p> <pre><code>data:\nnum_parallel_workers: 8\n\ntrain_transforms:\n- { func_name: mosaic, prob: 1.0, mosaic9_prob: 0.0, translate: 0.1, scale: 0.9 }\n- { func_name: mixup, prob: 0.1, alpha: 8.0, beta: 8.0, needed_mosaic: True }\n- { func_name: hsv_augment, prob: 1.0, hgain: 0.015, sgain: 0.7, vgain: 0.4 }\n- { func_name: label_norm, xyxy2xywh_: True }\n- { func_name: albumentations }\n- { func_name: fliplr, prob: 0.5 }\n- { func_name: label_pad, padding_size: 160, padding_value: -1 }\n- { func_name: image_norm, scale: 255. }\n- { func_name: image_transpose, bgr2rgb: True, hwc2chw: True }\n\ntest_transforms:\n- { func_name: letterbox, scaleup: False }\n- { func_name: label_norm, xyxy2xywh_: True }\n- { func_name: label_pad, padding_size: 160, padding_value: -1 }\n- { func_name: image_norm, scale: 255. }\n- { func_name: image_transpose, bgr2rgb: True, hwc2chw: True }\n</code></pre>"},{"location":"zh/tutorials/configuration/#_9","title":"\u6a21\u578b","text":""},{"location":"zh/tutorials/configuration/#_10","title":"\u53c2\u6570\u8bf4\u660e","text":"<ul> <li>model_name: \u6a21\u578b\u540d\u79f0</li> <li>depth_multiple: \u6a21\u578b\u6df1\u5ea6\u56e0\u5b50</li> <li>width_multiple: \u6a21\u578b\u5bbd\u5ea6\u56e0\u5b50</li> <li>stride: \u7279\u5f81\u56fe\u4e0b\u91c7\u6837\u500d\u6570</li> <li>anchors: \u9884\u8bbe\u951a\u6846</li> <li>backbone: \u6a21\u578b\u9aa8\u5e72\u7f51\u7edc</li> <li>head: \u6a21\u578b\u68c0\u6d4b\u5934</li> </ul>"},{"location":"zh/tutorials/configuration/#yaml_2","title":"yaml\u6587\u4ef6\u6837\u4f8b","text":"<p>\u8be5\u90e8\u5206\u53c2\u6570\u5728configs/yolov3/yolov3.yaml\u4e2d\u5b9a\u4e49\uff0c\u6839\u636ebackbon\u548chead\u53c2\u6570\u8fdb\u884c\u7f51\u7edc\u6784\u5efa\uff0c\u53c2\u6570\u4ee5\u5d4c\u5957\u5217\u8868\u7684\u5f62\u5f0f\u5448\u73b0\uff0c\u6bcf\u884c\u4ee3\u8868\u4e00\u5c42\u6a21\u5757\uff0c\u5305\u542b4\u4e2a\u53c2\u6570\uff0c\u5206\u522b\u662f \u8f93\u5165\u5c42\u7f16\u53f7(-1\u4ee3\u8868\u4e0a\u4e00\u5c42)\u3001\u6a21\u5757\u91cd\u590d\u6b21\u6570\u3001\u6a21\u5757\u540d\u79f0\u548c\u6a21\u5757\u76f8\u5e94\u53c2\u6570\u3002\u7528\u6237\u4e5f\u53ef\u4ee5\u4e0d\u501f\u52a9yaml\u6587\u4ef6\u800c\u76f4\u63a5\u5728py\u6587\u4ef6\u4e2d\u5b9a\u4e49\u548c\u6ce8\u518c\u7f51\u7edc\u3002</p> <pre><code>network:\nmodel_name: yolov3\n\ndepth_multiple: 1.0  # model depth multiple\nwidth_multiple: 1.0  # layer channel multiple\nstride: [8, 16, 32]\nanchors:\n- [10,13, 16,30, 33,23]  # P3/8\n- [30,61, 62,45, 59,119]  # P4/16\n- [116,90, 156,198, 373,326]  # P5/32\n\n# darknet53 backbone\nbackbone:\n# [from, number, module, args]\n[[-1, 1, ConvNormAct, [32, 3, 1]],  # 0\n[-1, 1, ConvNormAct, [64, 3, 2]],  # 1-P1/2\n[-1, 1, Bottleneck, [64]],\n[-1, 1, ConvNormAct, [128, 3, 2]],  # 3-P2/4\n[-1, 2, Bottleneck, [128]],\n[-1, 1, ConvNormAct, [256, 3, 2]],  # 5-P3/8\n[-1, 8, Bottleneck, [256]],\n[-1, 1, ConvNormAct, [512, 3, 2]],  # 7-P4/16\n[-1, 8, Bottleneck, [512]],\n[-1, 1, ConvNormAct, [1024, 3, 2]],  # 9-P5/32\n[-1, 4, Bottleneck, [1024]],  # 10\n]\n\n# YOLOv3 head\nhead:\n[[-1, 1, Bottleneck, [1024, False]],\n[-1, 1, ConvNormAct, [512, 1, 1]],\n[-1, 1, ConvNormAct, [1024, 3, 1]],\n[-1, 1, ConvNormAct, [512, 1, 1]],\n[-1, 1, ConvNormAct, [1024, 3, 1]],  # 15 (P5/32-large)\n\n[-2, 1, ConvNormAct, [256, 1, 1]],\n[-1, 1, Upsample, [None, 2, 'nearest']],\n[[-1, 8], 1, Concat, [1]],  # cat backbone P4\n[-1, 1, Bottleneck, [512, False]],\n[-1, 1, Bottleneck, [512, False]],\n[-1, 1, ConvNormAct, [256, 1, 1]],\n[-1, 1, ConvNormAct, [512, 3, 1]],  # 22 (P4/16-medium)\n\n[-2, 1, ConvNormAct, [128, 1, 1]],\n[-1, 1, Upsample, [None, 2, 'nearest']],\n[[-1, 6], 1, Concat, [1]],  # cat backbone P3\n[-1, 1, Bottleneck, [256, False]],\n[-1, 2, Bottleneck, [256, False]],  # 27 (P3/8-small)\n\n[[27, 22, 15], 1, YOLOv3Head, [nc, anchors, stride]],   # Detect(P3, P4, P5)\n]\n</code></pre>"},{"location":"zh/tutorials/configuration/#_11","title":"\u635f\u5931\u51fd\u6570","text":""},{"location":"zh/tutorials/configuration/#_12","title":"\u53c2\u6570\u8bf4\u660e","text":"<ul> <li>name: \u635f\u5931\u51fd\u6570\u540d\u79f0</li> <li>box: box\u635f\u5931\u6743\u91cd</li> <li>cls: class\u635f\u5931\u6743\u91cd</li> <li>cls_pw: class\u635f\u5931\u6b63\u6837\u672c\u6743\u91cd</li> <li>obj: object\u635f\u5931\u6743\u91cd</li> <li>obj_pw: object\u635f\u5931\u6b63\u6837\u672c\u6743\u91cd</li> <li>fl_gamma: focal loss gamma</li> <li>anchor_t: anchor shape\u6bd4\u4f8b\u9608\u503c</li> <li>label_smoothing: \u6807\u7b7e\u5e73\u6ed1\u503c</li> </ul>"},{"location":"zh/tutorials/configuration/#yaml_3","title":"yaml\u6587\u4ef6\u6837\u4f8b","text":"<p>\u8be5\u90e8\u5206\u53c2\u6570\u5728configs/yolov3/hyp.scratch.yaml\u4e2d\u5b9a\u4e49</p> <pre><code>loss:\nname: YOLOv7Loss\nbox: 0.05  # box loss gain\ncls: 0.5  # cls loss gain\ncls_pw: 1.0  # cls BCELoss positive_weight\nobj: 1.0  # obj loss gain (scale with pixels)\nobj_pw: 1.0  # obj BCELoss positive_weight\nfl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\nanchor_t: 4.0  # anchor-multiple threshold\nlabel_smoothing: 0.0 # label smoothing epsilon\n</code></pre>"},{"location":"zh/tutorials/configuration/#_13","title":"\u4f18\u5316\u5668","text":""},{"location":"zh/tutorials/configuration/#_14","title":"\u53c2\u6570\u8bf4\u660e","text":"<ul> <li>optimizer: \u4f18\u5316\u5668\u540d\u79f0\u3002</li> <li>lr_init: \u5b66\u4e60\u7387\u521d\u59cb\u503c</li> <li>warmup_epochs: warmup epoch\u6570</li> <li>warmup_momentum: warmup momentum\u521d\u59cb\u503c</li> <li>warmup_bias_lr: warmup bias\u5b66\u4e60\u7387\u521d\u59cb\u503c</li> <li>min_warmup_step: \u6700\u5c0fwarmup step\u6570</li> <li>group_param: \u53c2\u6570\u5206\u7ec4\u7b56\u7565</li> <li>gp_weight_decay: \u5206\u7ec4\u53c2\u6570\u6743\u91cd\u8870\u51cf\u7cfb\u6570</li> <li>start_factor: \u521d\u59cb\u5b66\u4e60\u7387\u56e0\u6570</li> <li>end_factor: \u7ed3\u675f\u5b66\u4e60\u7387\u56e0\u6570</li> <li>momentum\uff1a\u79fb\u52a8\u5e73\u5747\u7684\u52a8\u91cf</li> <li>loss_scale\uff1aloss\u7f29\u653e\u7cfb\u6570</li> <li>nesterov\uff1a\u662f\u5426\u4f7f\u7528Nesterov Accelerated Gradient (NAG)\u7b97\u6cd5\u66f4\u65b0\u68af\u5ea6\u3002</li> </ul>"},{"location":"zh/tutorials/configuration/#yaml_4","title":"yaml\u6587\u4ef6\u6837\u4f8b","text":"<p>\u8be5\u90e8\u5206\u53c2\u6570\u5728configs/yolov3/hyp.scratch.yaml\u4e2d\u5b9a\u4e49\uff0c\u5982\u4e0b\u793a\u4f8b\u4e2d\u7ecf\u8fc7warmup\u9636\u6bb5\u540e\u7684\u521d\u59cb\u5b66\u4e60\u7387\u4e3alr_init * start_factor = 0.01 * 1.0 = 0.01, \u6700\u7ec8\u5b66\u4e60\u7387\u4e3alr_init * end_factor = 0.01 * 0.01 = 0.0001</p> <pre><code>optimizer:\noptimizer: momentum\nlr_init: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\nmomentum: 0.937  # SGD momentum/Adam beta1\nnesterov: True # update gradients with NAG(Nesterov Accelerated Gradient) algorithm\nloss_scale: 1.0 # loss scale for optimizer\nwarmup_epochs: 3  # warmup epochs (fractions ok)\nwarmup_momentum: 0.8  # warmup initial momentum\nwarmup_bias_lr: 0.1  # warmup initial bias lr\nmin_warmup_step: 1000 # minimum warmup step\ngroup_param: yolov7 # group param strategy\ngp_weight_decay: 0.0005  # group param weight decay 5e-4\nstart_factor: 1.0\nend_factor: 0.01\n</code></pre>"},{"location":"zh/tutorials/data_augmentation/","title":"\u6570\u636e\u589e\u5f3a","text":""},{"location":"zh/tutorials/data_augmentation/#_2","title":"\u5957\u4ef6\u81ea\u5e26\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u6e05\u5355","text":"\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u540d \u6982\u8981\u89e3\u91ca mosaic \u968f\u673a\u9009\u62e9mosaic4\u548cmosaic9 mosaic4 4\u5206\u683c\u62fc\u63a5 mosaic9 9\u5206\u683c\u62fc\u63a5 mixup \u5bf9\u4e24\u4e2a\u56fe\u50cf\u8fdb\u884c\u7ebf\u6027\u6df7\u5408 pastein \u526a\u8d34\u589e\u5f3a random_perspective \u968f\u673a\u900f\u89c6\u53d8\u6362 hsv_augment \u968f\u673a\u989c\u8272\u53d8\u6362 fliplr \u6c34\u5e73\u7ffb\u8f6c flipud \u5782\u76f4\u7ffb\u8f6c letterbox \u7f29\u653e\u548c\u586b\u5145 label_norm \u6807\u7b7e\u5f52\u4e00\u5316 \u5750\u6807\u5f52\u4e00\u5316\u52300-1\u5230\u8303\u56f4 label_pad \u5c06\u6807\u7b7e\u4fe1\u606f\u586b\u5145\u4e3a\u56fa\u5b9a\u5927\u5c0f\u7684\u6570\u7ec4 image_norm \u56fe\u50cf\u6570\u636e\u6807\u51c6\u5316 image_transpose \u901a\u9053\u8f6c\u7f6e\u548c\u7ef4\u5ea6\u8f6c\u7f6e albumentations albumentations\u6570\u636e\u589e\u5f3a <p>\u8fd9\u4e9b\u6570\u636e\u589e\u5f3a\u51fd\u6570\u5b9a\u4e49\u5728 mindyolo/data/dataset.py \u4e2d\u3002</p>"},{"location":"zh/tutorials/data_augmentation/#_3","title":"\u4f7f\u7528\u65b9\u6cd5","text":"<p>MindYOLO\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u901a\u8fc7\u5728yaml\u6587\u4ef6\u91cc\u914d\u7f6e\u3002\u4f8b\u5982\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u6dfb\u52a0\u4e00\u4e2a\u6570\u636e\u589e\u5f3a\uff0c\u9700\u8981\u5728yaml\u6587\u4ef6data.train_transforms\u5b57\u6bb5\u4e0b\u6dfb\u52a0\u4e00\u4e2a\u5b57\u5178\u5217\u8868\uff0c\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u81ea\u4e0a\u800c\u4e0b\u4f9d\u6b21\u7f57\u5217\u3002</p> <p>\u4e00\u4e2a\u5178\u578b\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u914d\u7f6e\u5b57\u5178\u91cc\u5fc5\u987b\u6709func_name\uff0c\u8868\u793a\u5e94\u7528\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u540d\uff0c\u800c\u540e\u7f57\u5217\u8be5\u65b9\u6cd5\u9700\u8981\u8bbe\u7f6e\u7684\u53c2\u6570\uff0c\u82e5\u6ca1\u6709\u5728\u6570\u636e\u589e\u5f3a\u914d\u7f6e\u5b57\u5178\u4e2d\u914d\u7f6e\u53c2\u6570\u9879\uff0c\u5219\u4f1a\u9009\u62e9\u8be5\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u9ed8\u8ba4\u7684\u6570\u503c\u3002</p> <p>\u6570\u636e\u589e\u5f3a\u901a\u7528\u914d\u7f6e\u5b57\u5178\uff1a <pre><code>- {func_name: \u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u540d1, args11=x11, args12=x12, ..., args1n=x1n}\n- {func_name: \u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u540d2, args21=x21, args22=x22, ..., args2n=x2n}\n...\n- {func_name: \u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u540dn, argsn1=xn1, argsn2=xn2, ..., argsnn=xnn}\n</code></pre></p> <p>\u4ee5YOLOv7\u8bad\u7ec3\u6570\u636e\u589e\u5f3a\u793a\u4f8b\uff1a <pre><code># \u6587\u4ef6\u76ee\u5f55\uff1aconfigs/yolov7/hyp.scratch.tiny.yaml (https://github.com/mindspore-lab/mindyolo/blob/master/configs/yolov7/hyp.scratch.tiny.yaml)\ntrain_transforms:\n- {func_name: mosaic, prob: 1.0, mosaic9_prob: 0.2, translate: 0.1, scale: 0.5}\n- {func_name: mixup, prob: 0.05, alpha: 8.0, beta: 8.0, needed_mosaic: True}\n- {func_name: hsv_augment, prob: 1.0, hgain: 0.015, sgain: 0.7, vgain: 0.4}\n- {func_name: pastein, prob: 0.05, num_sample: 30}\n- {func_name: label_norm, xyxy2xywh_: True}\n- {func_name: fliplr, prob: 0.5}\n- {func_name: label_pad, padding_size: 160, padding_value: -1}\n- {func_name: image_norm, scale: 255.}\n- {func_name: image_transpose, bgr2rgb: True, hwc2chw: True}\n</code></pre> \u6ce8\u610f\uff1afunc_name\u8868\u793a\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u540d\uff0cprob\uff0cmosaic9_prob\uff0ctranslate\uff0cscale\u4e3a\u8be5\u65b9\u6cd5\u53c2\u6570\u3002 \u5176\u4e2dprob\u4e3a\u6240\u6709\u65b9\u6cd5\u5747\u6709\u7684\u53c2\u6570\uff0c\u8868\u793a\u8be5\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u7684\u6267\u884c\u6982\u7387\uff0c\u9ed8\u8ba4\u503c\u4e3a1</p> <p>\u4e0a\u8ff0yaml\u6587\u4ef6\u6267\u884c\u7684\u5177\u4f53\u64cd\u4f5c\u5982\u4e0b\uff1a</p> <ul> <li> <p><code>mosaic</code>\uff1a\u4ee51.0\u7684\u6982\u7387\u5bf9\u8f93\u5165\u7684\u56fe\u7247\u8fdb\u884cmosaic\u64cd\u4f5c\uff0c\u5373\u5c064\u5f20\u4e0d\u540c\u7684\u56fe\u7247\u62fc\u63a5\u6210\u4e00\u5f20\u56fe\u7247\u3002mosaic9_prob\u8868\u793a\u4f7f\u75289\u5bab\u683c\u65b9\u5f0f\u8fdb\u884c\u62fc\u63a5\u7684\u6982\u7387\uff0ctranslate\u548cscale\u5206\u522b\u8868\u793a\u968f\u673a\u5e73\u79fb\u548c\u7f29\u653e\u7684\u7a0b\u5ea6\u3002 \u5982\u56fe\u6240\u793a\uff1a </p> </li> <li> <p><code>mixup</code>\uff1a\u4ee50.05\u7684\u6982\u7387\u5bf9\u8f93\u5165\u7684\u56fe\u7247\u8fdb\u884cmixup\u64cd\u4f5c\uff0c\u5373\u5c06\u4e24\u5f20\u4e0d\u540c\u7684\u56fe\u7247\u8fdb\u884c\u6df7\u5408\u3002\u5176\u4e2dalpha\u548cbeta\u8868\u793a\u6df7\u5408\u7cfb\u6570\uff0cneeded_mosaic\u8868\u793a\u662f\u5426\u9700\u8981\u4f7f\u7528mosaic\u8fdb\u884c\u6df7\u5408\u3002</p> </li> <li> <p><code>hsv_augment</code>: HSV\u589e\u5f3a, \u4ee51.0\u7684\u6982\u7387\u5bf9\u8f93\u5165\u7684\u56fe\u7247\u8fdb\u884cHSV\u989c\u8272\u7a7a\u95f4\u7684\u8c03\u6574\uff0c\u589e\u52a0\u6570\u636e\u591a\u6837\u6027\u3002\u5176\u4e2dhgain\u3001sgain\u548cvgain\u5206\u522b\u8868\u793a\u5bf9H\u3001S\u3001V\u901a\u9053\u7684\u8c03\u6574\u7a0b\u5ea6\u3002</p> </li> <li> <p><code>pastein</code>\uff1a\u4ee50.05\u7684\u6982\u7387\u5728\u8f93\u5165\u7684\u56fe\u7247\u4e2d\u968f\u673a\u8d34\u5165\u4e00\u4e9b\u6837\u672c\u3002\u5176\u4e2dnum_sample\u8868\u793a\u968f\u673a\u8d34\u5165\u7684\u6837\u672c\u6570\u91cf\u3002</p> </li> <li> <p><code>label_norm</code>\uff1a\u5c06\u8f93\u5165\u7684\u6807\u7b7e\u4ece(x1, y1, x2, y2)\u7684\u683c\u5f0f\u8f6c\u6362\u4e3a(x, y, w, h)\u7684\u683c\u5f0f\u3002</p> </li> <li> <p><code>fliplr</code>\uff1a\u4ee50.5\u7684\u6982\u7387\u5bf9\u8f93\u5165\u7684\u56fe\u7247\u8fdb\u884c\u6c34\u5e73\u7ffb\u8f6c\uff0c\u589e\u52a0\u6570\u636e\u591a\u6837\u6027\u3002</p> </li> <li> <p><code>label_pad</code>\uff1a\u5bf9\u8f93\u5165\u7684\u6807\u7b7e\u8fdb\u884c\u586b\u5145\uff0c\u4f7f\u5f97\u6bcf\u4e2a\u56fe\u7247\u90fd\u6709\u76f8\u540c\u6570\u91cf\u7684\u6807\u7b7e\u3002padding_size\u8868\u793a\u586b\u5145\u540e\u6807\u7b7e\u7684\u6570\u91cf\uff0cpadding_value\u8868\u793a\u586b\u5145\u7684\u503c\u3002</p> </li> <li> <p><code>image_norm</code>\uff1a\u5c06\u8f93\u5165\u7684\u56fe\u7247\u50cf\u7d20\u503c\u4ece[0, 255]\u8303\u56f4\u5185\u7f29\u653e\u5230[0, 1]\u8303\u56f4\u5185\u3002</p> </li> <li> <p><code>image_transpose</code>\uff1a\u5c06\u8f93\u5165\u7684\u56fe\u7247\u4eceBGR\u683c\u5f0f\u8f6c\u6362\u4e3aRGB\u683c\u5f0f\uff0c\u5e76\u5c06\u56fe\u7247\u7684\u901a\u9053\u6570\u4eceHWC\u683c\u5f0f\u8f6c\u6362\u4e3aCHW\u683c\u5f0f\u3002</p> </li> </ul> <p>\u6d4b\u8bd5\u6570\u636e\u589e\u5f3a\u9700\u8981\u7528test_transforms\u5b57\u6bb5\u6807\u6ce8\uff0c\u914d\u7f6e\u65b9\u6cd5\u540c\u8bad\u7ec3\u3002</p>"},{"location":"zh/tutorials/data_augmentation/#_4","title":"\u81ea\u5b9a\u4e49\u6570\u636e\u589e\u5f3a","text":"<p>\u7f16\u5199\u6307\u5357\uff1a</p> <ul> <li>\u5728mindyolo/data/dataset.py\u6587\u4ef6COCODataset\u7c7b\u4e2d\u6dfb\u52a0\u81ea\u5b9a\u4e49\u6570\u636e\u589e\u5f3a\u65b9\u6cd5   </li> <li>\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u7684\u8f93\u5165\u901a\u5e38\u5305\u542b\u56fe\u7247\u3001\u6807\u7b7e\u548c\u81ea\u5b9a\u4e49\u53c2\u6570\u3002  </li> <li>\u7f16\u5199\u51fd\u6570\u4f53\u5185\u5bb9\uff0c\u81ea\u5b9a\u4e49\u8f93\u51fa</li> </ul> <p>\u4e00\u4e2a\u5178\u578b\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff1a <pre><code>#\u5728mindyolo/data/dataset.py COCODataset \u6dfb\u52a0\u5b50\u65b9\u6cd5\n    def data_trans_func(self, image, labels, args1=x1, args2=x2, ..., argsn=xn):\n        # \u6570\u636e\u589e\u5f3a\u903b\u8f91\n        ......\n        return image, labels\n</code></pre> \u81ea\u5b9a\u4e49\u4e00\u4e2a\u529f\u80fd\u4e3a\u65cb\u8f6c\u7684\u6570\u636e\u589e\u5f3a\u51fd\u6570 <pre><code>#mindyolo/data/dataset.py\n    def rotate(self, image, labels, angle):\n        # rotate image\n        image = np.rot90(image, angle // 90)\n        if len(labels):\n            if angle == 90:\n                labels[:, 0], labels[:, 1] = 1 - labels[:, 1], labels[:, 0]\n            elif angle == 180:\n                labels[:, 0], labels[:, 1] = 1 - labels[:, 0], 1 - labels[:, 1]\n            elif angle == 270:\n                labels[:, 0], labels[:, 1] = labels[:, 1], 1 - labels[:, 0]\n        return image, labels\n</code></pre></p> <p>\u4f7f\u7528\u6307\u5357\uff1a - \u5728\u6a21\u578b\u7684yaml\u6587\u4ef6\u4e2d\uff0c\u4ee5\u5b57\u5178\u7684\u5f62\u5f0f\u5b9a\u4e49\u6b64\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002\u4e0e\u4e0a\u6587\u6240\u8ff0\u7528\u6cd5\u4e00\u81f4 <pre><code>    - {func_name: rotate, angle: 90}\n</code></pre></p> <p>\u6548\u679c\u5c55\u793a\uff1a</p> <p> </p>"},{"location":"zh/tutorials/deployment/","title":"\u90e8\u7f72","text":""},{"location":"zh/tutorials/deployment/#_2","title":"\u4f9d\u8d56","text":"<pre><code>pip install -r requirement.txt\n</code></pre>"},{"location":"zh/tutorials/deployment/#mindspore-lite","title":"MindSpore Lite\u73af\u5883\u51c6\u5907","text":"<p>\u53c2\u8003\uff1aLite\u73af\u5883\u914d\u7f6e     \u6ce8\u610f\uff1aMindSpore Lite\u9002\u914d\u7684python\u73af\u5883\u4e3a3.7\uff0c\u8bf7\u5728\u5b89\u88c5Lite\u524d\u51c6\u5907\u597dpython3.7\u7684\u73af\u5883 </p> <ol> <li> <p>\u6839\u636e\u73af\u5883\uff0c\u4e0b\u8f7d\u914d\u5957\u7684tar.gz\u5305\u548cwhl\u5305</p> </li> <li> <p>\u89e3\u538btar.gz\u5305\u5e76\u5b89\u88c5\u5bf9\u5e94\u7248\u672c\u7684whl\u5305    <pre><code>tar -zxvf mindspore_lite-2.0.0a0-cp37-cp37m-{os}_{platform}_64.tar.gz\npip install mindspore_lite-2.0.0a0-cp37-cp37m-{os}_{platform}_64.whl\n</code></pre></p> </li> <li>\u914d\u7f6eLite\u7684\u73af\u5883\u53d8\u91cf    LITE_HOME\u4e3atar.gz\u89e3\u538b\u51fa\u7684\u6587\u4ef6\u5939\u8def\u5f84\uff0c\u63a8\u8350\u4f7f\u7528\u7edd\u5bf9\u8def\u5f84    <pre><code>export LITE_HOME=/path/to/mindspore-lite-{version}-{os}-{platform}\nexport LD_LIBRARY_PATH=$LITE_HOME/runtime/lib:$LITE_HOME/tools/converter/lib:$LD_LIBRARY_PATH\nexport PATH=$LITE_HOME/tools/converter/converter:$LITE_HOME/tools/benchmark:$PATH\n</code></pre></li> </ol>"},{"location":"zh/tutorials/deployment/#_3","title":"\u5feb\u901f\u5f00\u59cb","text":""},{"location":"zh/tutorials/deployment/#_4","title":"\u6a21\u578b\u8f6c\u6362","text":"<p>ckpt\u6a21\u578b\u8f6c\u4e3amindir\u6a21\u578b\uff0c\u6b64\u6b65\u9aa4\u53ef\u5728CPU/Ascend910\u4e0a\u8fd0\u884c    <pre><code>python ./deploy/export.py --config ./path_to_config/model.yaml --weight ./path_to_ckpt/weight.ckpt --per_batch_size 1 --file_format MINDIR --device_target [CPU/Ascend]\ne.g.\n# \u5728CPU\u4e0a\u8fd0\u884c\npython ./deploy/export.py --config ./configs/yolov5/yolov5n.yaml --weight yolov5n_300e_mAP273-9b16bd7b.ckpt --per_batch_size 1 --file_format MINDIR --device_target CPU\n# \u5728Ascend\u4e0a\u8fd0\u884c\npython ./deploy/export.py --config ./configs/yolov5/yolov5n.yaml --weight yolov5n_300e_mAP273-9b16bd7b.ckpt --per_batch_size 1 --file_format MINDIR --device_target Ascend\n</code></pre></p>"},{"location":"zh/tutorials/deployment/#_5","title":"\u811a\u672c\u8bf4\u660e","text":"<ul> <li>predict.py \u652f\u6301\u5355\u5f20\u56fe\u7247\u63a8\u7406</li> <li>test.py \u652f\u6301COCO\u6570\u636e\u96c6\u63a8\u7406</li> <li>\u6ce8\u610f\uff1a\u5f53\u524d\u53ea\u652f\u6301\u5728Ascend 310\u4e0a\u63a8\u7406</li> </ul>"},{"location":"zh/tutorials/deployment/#mindx","title":"MindX\u90e8\u7f72","text":""},{"location":"zh/tutorials/deployment/#_6","title":"\u73af\u5883\u914d\u7f6e","text":"<p>\u53c2\u8003\uff1aMindX\u73af\u5883\u51c6\u5907  \u6ce8\u610f\uff1aMindX\u76ee\u524d\u652f\u6301\u7684python\u7248\u672c\u4e3a3.9\uff0c\u8bf7\u5728\u5b89\u88c5MindX\u524d\uff0c\u51c6\u5907\u597dpython3.9\u7684\u73af\u5883 </p> <ol> <li> <p>\u5728MindX\u5b98\u7f51\u83b7\u53d6\u73af\u5883\u5b89\u88c5\u5305\uff0c\u76ee\u524d\u652f\u63013.0.0\u7248\u672cMindX\u63a8\u7406</p> </li> <li> <p>\u8df3\u8f6c\u81f3\u4e0b\u8f7d\u9875\u9762\u4e0b\u8f7dAscend-mindxsdk-mxmanufacture_{version}_linux-{arch}.run</p> </li> <li> <p>\u5c06\u5b89\u88c5\u5305\u653e\u7f6e\u4e8eAscend310\u673a\u5668\u76ee\u5f55\u4e2d\u5e76\u89e3\u538b</p> </li> <li> <p>\u5982\u4e0d\u662froot\u7528\u6237\uff0c\u9700\u589e\u52a0\u5bf9\u5957\u4ef6\u5305\u7684\u53ef\u6267\u884c\u6743\u9650\uff1a <pre><code>chmod +x Ascend-mindxsdk-mxmanufacture_{version}_linux-{arch}.run\n</code></pre></p> </li> <li>\u8fdb\u5165\u5f00\u53d1\u5957\u4ef6\u5305\u7684\u4e0a\u4f20\u8def\u5f84\uff0c\u5b89\u88c5mxManufacture\u5f00\u53d1\u5957\u4ef6\u5305\u3002 <pre><code>./Ascend-mindxsdk-mxmanufacture_{version}_linux-{arch}.run --install\n</code></pre> \u5b89\u88c5\u5b8c\u6210\u540e\uff0c\u82e5\u51fa\u73b0\u5982\u4e0b\u56de\u663e\uff0c\u8868\u793a\u8f6f\u4ef6\u6210\u529f\u5b89\u88c5\u3002 <pre><code>The installation is successfully\n</code></pre> \u5b89\u88c5\u5b8c\u6210\u540e\uff0cmxManufacture\u8f6f\u4ef6\u76ee\u5f55\u7ed3\u6784\u5982\u4e0b\u6240\u793a\uff1a <pre><code>.\n\u251c\u2500\u2500 bin\n\u251c\u2500\u2500 config\n\u251c\u2500\u2500 filelist.txt\n\u251c\u2500\u2500 include\n\u251c\u2500\u2500 lib\n\u251c\u2500\u2500 opensource\n\u251c\u2500\u2500 operators\n\u251c\u2500\u2500 python\n\u251c\u2500\u2500 samples\n\u251c\u2500\u2500 set_env.sh\n\u251c\u2500\u2500 toolkit\n\u2514\u2500\u2500 version.info\n</code></pre></li> <li>\u8fdb\u5165mxmanufacture\u7684\u5b89\u88c5\u76ee\u5f55\uff0c\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u4f7fMindX SDK\u73af\u5883\u53d8\u91cf\u751f\u6548\u3002 <pre><code>source set_env.sh\n</code></pre></li> <li>\u8fdb\u5165./mxVision-3.0.0/python/\uff0c\u5b89\u88c5mindx-3.0.0-py3-none-any.whl <pre><code>pip install mindx-3.0.0-py3-none-any.whl\n</code></pre></li> </ol>"},{"location":"zh/tutorials/deployment/#_7","title":"\u6a21\u578b\u8f6c\u6362","text":"<ol> <li> <p>ckpt\u6a21\u578b\u8f6c\u4e3aair\u6a21\u578b\uff0c\u6b64\u6b65\u9aa4\u9700\u8981\u5728Ascend910\u4e0a\u64cd\u4f5c    <pre><code>python ./deploy/export.py --config ./path_to_config/model.yaml --weight ./path_to_ckpt/weight.ckpt --per_batch_size 1 --file_format AIR\ne.g.\npython ./deploy/export.py --config ./configs/yolov5/yolov5n.yaml --weight yolov5n_300e_mAP273-9b16bd7b.ckpt --per_batch_size 1 --file_format AIR\n</code></pre> yolov7\u9700\u8981\u57282.0\u7248\u672c\u4ee5\u4e0a\u7684Ascend910\u673a\u5668\u8fd0\u884cexport</p> </li> <li> <p>air\u6a21\u578b\u8f6c\u4e3aom\u6a21\u578b\uff0c\u4f7f\u7528atc\u8f6c\u6362\u5de5\u5177\uff0c\u6b64\u6b65\u9aa4\u9700\u5b89\u88c5MindX\u73af\u5883\uff0c\u5728Ascend310\u4e0a\u8fd0\u884c    <pre><code>atc --model=./path_to_air/weight.air --framework=1 --output=yolo  --soc_version=Ascend310\n</code></pre></p> </li> </ol>"},{"location":"zh/tutorials/deployment/#mindx-test","title":"MindX Test","text":"<p>\u5bf9COCO\u6570\u636e\u63a8\u7406\uff1a    <pre><code>python ./deploy/test.py --model_type MindX --model_path ./path_to_om/weight.om --config ./path_to_config/yolo.yaml\ne.g.\npython ./deploy/test.py --model_type MindX --model_path ./yolov5n.om --config ./configs/yolov5/yolov5n.yaml\n</code></pre></p>"},{"location":"zh/tutorials/deployment/#mindx-predict","title":"MindX Predict","text":"<p>\u5bf9\u5355\u5f20\u56fe\u7247\u63a8\u7406\uff1a    <pre><code>python ./deploy/predict.py --model_type MindX --model_path ./path_to_om/weight.om --config ./path_to_config/yolo.yaml --image_path ./path_to_image/image.jpg\ne.g.\npython ./deploy/predict.py --model_type MindX --model_path ./yolov5n.om --config ./configs/yolov5/yolov5n.yaml --image_path ./coco/image/val2017/image.jpg\n</code></pre></p>"},{"location":"zh/tutorials/deployment/#mindir","title":"MindIR\u90e8\u7f72","text":""},{"location":"zh/tutorials/deployment/#_8","title":"\u73af\u5883\u8981\u6c42","text":"<p>mindspore&gt;=2.1</p>"},{"location":"zh/tutorials/deployment/#_9","title":"\u6ce8\u610f\u4e8b\u9879","text":"<ol> <li> <p>\u5f53\u524d\u4ec5\u652f\u6301Predict</p> </li> <li> <p>\u7406\u8bba\u4e0a\u4e5f\u53ef\u5728Ascend910\u4e0a\u8fd0\u884c\uff0c\u672a\u6d4b\u8bd5</p> </li> </ol>"},{"location":"zh/tutorials/deployment/#_10","title":"\u6a21\u578b\u8f6c\u6362","text":"<p>ckpt\u6a21\u578b\u8f6c\u4e3amindir\u6a21\u578b\uff0c\u6b64\u6b65\u9aa4\u53ef\u5728CPU\u4e0a\u8fd0\u884c    <pre><code>python ./deploy/export.py --config ./path_to_config/model.yaml --weight ./path_to_ckpt/weight.ckpt --per_batch_size 1 --file_format MINDIR --device_target CPU\ne.g.\n# \u5728CPU\u4e0a\u8fd0\u884c\npython ./deploy/export.py --config ./configs/yolov5/yolov5n.yaml --weight yolov5n_300e_mAP273-9b16bd7b.ckpt --per_batch_size 1 --file_format MINDIR --device_target CPU\n</code></pre></p>"},{"location":"zh/tutorials/deployment/#mindir-test","title":"MindIR Test","text":"<p>\u656c\u8bf7\u671f\u5f85</p>"},{"location":"zh/tutorials/deployment/#mindir-predict","title":"MindIR Predict","text":"<p>\u5bf9\u5355\u5f20\u56fe\u7247\u63a8\u7406\uff1a    <pre><code>python ./deploy/predict.py --model_type MindIR --model_path ./path_to_mindir/weight.mindir --config ./path_to_conifg/yolo.yaml --image_path ./path_to_image/image.jpg\ne.g.\npython deploy/predict.py --model_type MindIR --model_path ./yolov5n.mindir --config ./configs/yolov5/yolov5n.yaml --image_path ./coco/image/val2017/image.jpg\n</code></pre></p>"},{"location":"zh/tutorials/deployment/#onnx","title":"ONNX\u90e8\u7f72","text":"<p>\u6ce8\u610f: \u4ec5\u90e8\u5206\u6a21\u578b\u652f\u6301\u5bfc\u51faONNX\u5e76\u4f7f\u7528ONNXRuntime\u8fdb\u884c\u90e8\u7f72</p>"},{"location":"zh/tutorials/deployment/#_11","title":"\u73af\u5883\u914d\u7f6e","text":"<pre><code>pip install onnx&gt;=1.9.0\npip install onnxruntime&gt;=1.8.0\n</code></pre>"},{"location":"zh/tutorials/deployment/#_12","title":"\u6ce8\u610f\u4e8b\u9879","text":"<ol> <li> <p>\u5f53\u524d\u5e76\u975e\u6240\u6709mindyolo\u5747\u652f\u6301ONNX\u5bfc\u51fa\u548c\u63a8\u7406\uff08\u4ec5\u4ee5YoloV3\u4e3a\u4f8b\uff09</p> </li> <li> <p>\u5f53\u524d\u4ec5\u652f\u6301Predict\u529f\u80fd</p> </li> <li> <p>\u5bfc\u51faONNX\u9700\u8981\u8c03\u6574nn.SiLU\u7b97\u5b50\uff0c\u91c7\u7528sigmoid\u7b97\u5b50\u5e95\u5c42\u5b9e\u73b0  </p> </li> </ol> <p>\u4f8b\u5982\uff1a\u6dfb\u52a0\u5982\u4e0b\u81ea\u5b9a\u4e49\u5c42\u5e76\u66ff\u6362mindyolo\u4e2d\u6240\u6709\u7684nn.SiLU <pre><code>class EdgeSiLU(nn.Cell):\n\"\"\"\n    SiLU activation function: x * sigmoid(x). To support for onnx export with nn.SiLU.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    def construct(self, x):\n        return x * ops.sigmoid(x)\n</code></pre></p>"},{"location":"zh/tutorials/deployment/#_13","title":"\u6a21\u578b\u8f6c\u6362","text":"<p>ckpt\u6a21\u578b\u8f6c\u4e3aONNX\u6a21\u578b\uff0c\u6b64\u6b65\u9aa4\u4ee5\u53caTest\u6b65\u9aa4\u5747\u4ec5\u652f\u6301CPU\u4e0a\u8fd0\u884c    <pre><code>python ./deploy/export.py --config ./path_to_config/model.yaml --weight ./path_to_ckpt/weight.ckpt --per_batch_size 1 --file_format ONNX --device_target [CPU]\ne.g.\n# \u5728CPU\u4e0a\u8fd0\u884c\npython ./deploy/export.py --config ./configs/yolov3/yolov3.yaml --weight yolov3-darknet53_300e_mAP455-adfb27af.ckpt --per_batch_size 1 --file_format ONNX --device_target CPU\n</code></pre></p>"},{"location":"zh/tutorials/deployment/#onnx-test","title":"ONNX Test","text":"<p>\u656c\u8bf7\u671f\u5f85</p>"},{"location":"zh/tutorials/deployment/#onnxruntime-predict","title":"ONNXRuntime Predict","text":"<p>\u5bf9\u5355\u5f20\u56fe\u7247\u63a8\u7406\uff1a    <pre><code>python ./deploy/predict.py --model_type ONNX --model_path ./path_to_onnx_model/model.onnx --config ./path_to_config/yolo.yaml --image_path ./path_to_image/image.jpg\ne.g.\npython ./deploy/predict.py --model_type ONNX --model_path ./yolov3.onnx --config ./configs/yolov3/yolov3.yaml --image_path ./coco/image/val2017/image.jpg\n</code></pre></p>"},{"location":"zh/tutorials/deployment/#_14","title":"\u6807\u51c6\u548c\u652f\u6301\u7684\u6a21\u578b\u5e93","text":"<ul> <li> YOLOv8</li> <li> YOLOv7</li> <li> YOLOX</li> <li> YOLOv5</li> <li> YOLOv4</li> <li> YOLOv3</li> </ul> Name Scale Context ImageSize Dataset Box mAP (%) Params FLOPs Recipe Download YOLOv8 N D310x1-G 640 MS COCO 2017 37.2 3.2M 8.7G yaml ckpt mindir YOLOv8 S D310x1-G 640 MS COCO 2017 44.6 11.2M 28.6G yaml ckpt mindir YOLOv8 M D310x1-G 640 MS COCO 2017 50.5 25.9M 78.9G yaml ckpt mindir YOLOv8 L D310x1-G 640 MS COCO 2017 52.8 43.7M 165.2G yaml ckpt mindir YOLOv8 X D310x1-G 640 MS COCO 2017 53.7 68.2M 257.8G yaml ckpt mindir YOLOv7 Tiny D310x1-G 640 MS COCO 2017 37.5 6.2M 13.8G yaml ckpt mindir YOLOv7 L D310x1-G 640 MS COCO 2017 50.8 36.9M 104.7G yaml ckpt mindir YOLOv7 X D310x1-G 640 MS COCO 2017 52.4 71.3M 189.9G yaml ckpt mindir YOLOv5 N D310x1-G 640 MS COCO 2017 27.3 1.9M 4.5G yaml ckpt mindir YOLOv5 S D310x1-G 640 MS COCO 2017 37.6 7.2M 16.5G yaml ckpt mindir YOLOv5 M D310x1-G 640 MS COCO 2017 44.9 21.2M 49.0G yaml ckpt mindir YOLOv5 L D310x1-G 640 MS COCO 2017 48.5 46.5M 109.1G yaml ckpt mindir YOLOv5 X D310x1-G 640 MS COCO 2017 50.5 86.7M 205.7G yaml ckpt mindir YOLOv4 CSPDarknet53 D310x1-G 608 MS COCO 2017 45.4 27.6M 52G yaml ckpt mindir YOLOv4 CSPDarknet53(silu) D310x1-G 640 MS COCO 2017 45.8 27.6M 52G yaml ckpt mindir YOLOv3 Darknet53 D310x1-G 640 MS COCO 2017 45.5 61.9M 156.4G yaml ckpt mindir YOLOX N D310x1-G 416 MS COCO 2017 24.1 0.9M 1.1G yaml ckpt mindir YOLOX Tiny D310x1-G 416 MS COCO 2017 33.3 5.1M 6.5G yaml ckpt mindir YOLOX S D310x1-G 640 MS COCO 2017 40.7 9.0M 26.8G yaml ckpt mindir YOLOX M D310x1-G 640 MS COCO 2017 46.7 25.3M 73.8G yaml ckpt mindir YOLOX L D310x1-G 640 MS COCO 2017 49.2 54.2M 155.6G yaml ckpt mindir YOLOX X D310x1-G 640 MS COCO 2017 51.6 99.1M 281.9G yaml ckpt mindir YOLOX Darknet53 D310x1-G 640 MS COCO 2017 47.7 63.7M 185.3G yaml ckpt mindir"},{"location":"zh/tutorials/finetune/","title":"\u5fae\u8c03","text":""},{"location":"zh/tutorials/finetune/#finetune","title":"\u81ea\u5b9a\u4e49\u6570\u636e\u96c6finetune\u6d41\u7a0b","text":"<p>\u672c\u6587\u4ee5\u5b89\u5168\u5e3d\u4f69\u6234\u68c0\u6d4b\u6570\u636e\u96c6(SHWD)\u4e3a\u4f8b\uff0c\u4ecb\u7ecd\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u5728MindYOLO\u4e0a\u8fdb\u884cfinetune\u7684\u4e3b\u8981\u6d41\u7a0b\u3002</p>"},{"location":"zh/tutorials/finetune/#_2","title":"\u6570\u636e\u96c6\u683c\u5f0f\u8f6c\u6362","text":"<p>SHWD\u6570\u636e\u96c6\u91c7\u7528voc\u683c\u5f0f\u7684\u6570\u636e\u6807\u6ce8\uff0c\u5176\u6587\u4ef6\u76ee\u5f55\u5982\u4e0b\u6240\u793a\uff1a <pre><code>             ROOT_DIR\n                \u251c\u2500\u2500 Annotations\n                \u2502        \u251c\u2500\u2500 000000.xml\n                \u2502        \u2514\u2500\u2500 000002.xml\n                \u251c\u2500\u2500 ImageSets\n                \u2502       \u2514\u2500\u2500 Main\n                \u2502             \u251c\u2500\u2500 test.txt\n                \u2502             \u251c\u2500\u2500 train.txt\n                \u2502             \u251c\u2500\u2500 trainval.txt\n                \u2502             \u2514\u2500\u2500 val.txt\n                \u2514\u2500\u2500 JPEGImages\n                        \u251c\u2500\u2500 000000.jpg\n                        \u2514\u2500\u2500 000002.jpg\n</code></pre> Annotations\u6587\u4ef6\u5939\u4e0b\u7684xml\u6587\u4ef6\u4e3a\u6bcf\u5f20\u56fe\u7247\u7684\u6807\u6ce8\u4fe1\u606f\uff0c\u4e3b\u8981\u5185\u5bb9\u5982\u4e0b\uff1a <pre><code>&lt;annotation&gt;\n  &lt;folder&gt;JPEGImages&lt;/folder&gt;\n  &lt;filename&gt;000377.jpg&lt;/filename&gt;\n  &lt;path&gt;F:\\baidu\\VOC2028\\JPEGImages\\000377.jpg&lt;/path&gt;\n  &lt;source&gt;\n    &lt;database&gt;Unknown&lt;/database&gt;\n  &lt;/source&gt;\n  &lt;size&gt;\n    &lt;width&gt;750&lt;/width&gt;\n    &lt;height&gt;558&lt;/height&gt;\n    &lt;depth&gt;3&lt;/depth&gt;\n  &lt;/size&gt;\n  &lt;segmented&gt;0&lt;/segmented&gt;\n  &lt;object&gt;\n    &lt;name&gt;hat&lt;/name&gt;\n    &lt;pose&gt;Unspecified&lt;/pose&gt;\n    &lt;truncated&gt;0&lt;/truncated&gt;\n    &lt;difficult&gt;0&lt;/difficult&gt;\n    &lt;bndbox&gt;\n      &lt;xmin&gt;142&lt;/xmin&gt;\n      &lt;ymin&gt;388&lt;/ymin&gt;\n      &lt;xmax&gt;177&lt;/xmax&gt;\n      &lt;ymax&gt;426&lt;/ymax&gt;\n    &lt;/bndbox&gt;\n  &lt;/object&gt;\n</code></pre> \u5176\u4e2d\u5305\u542b\u591a\u4e2aobject, object\u4e2d\u7684name\u4e3a\u7c7b\u522b\u540d\u79f0\uff0cxmin, ymin, xmax, ymax\u5219\u4e3a\u68c0\u6d4b\u6846\u5de6\u4e0a\u89d2\u548c\u53f3\u4e0b\u89d2\u7684\u5750\u6807\u3002</p> <p>MindYOLO\u652f\u6301\u7684\u6570\u636e\u96c6\u683c\u5f0f\u4e3aYOLO\u683c\u5f0f\uff0c\u8be6\u60c5\u53ef\u53c2\u8003\u6570\u636e\u51c6\u5907</p> <p>\u7531\u4e8eMindYOLO\u5728\u9a8c\u8bc1\u9636\u6bb5\u9009\u7528\u56fe\u7247\u540d\u79f0\u4f5c\u4e3aimage_id\uff0c\u56e0\u6b64\u56fe\u7247\u540d\u79f0\u53ea\u80fd\u4e3a\u6570\u503c\u7c7b\u578b\uff0c\u800c\u4e0d\u80fd\u4e3a\u5b57\u7b26\u4e32\u7c7b\u578b\uff0c\u8fd8\u9700\u8981\u5bf9\u56fe\u7247\u8fdb\u884c\u6539\u540d\u3002\u5bf9SHWD\u6570\u636e\u96c6\u683c\u5f0f\u7684\u8f6c\u6362\u5305\u542b\u5982\u4e0b\u6b65\u9aa4\uff1a * \u5c06\u56fe\u7247\u590d\u5236\u5230\u76f8\u5e94\u7684\u8def\u5f84\u4e0b\u5e76\u6539\u540d * \u5728\u6839\u76ee\u5f55\u4e0b\u76f8\u5e94\u7684txt\u6587\u4ef6\u4e2d\u5199\u5165\u8be5\u56fe\u7247\u7684\u76f8\u5bf9\u8def\u5f84 * \u89e3\u6790xml\u6587\u4ef6\uff0c\u5728\u76f8\u5e94\u8def\u5f84\u4e0b\u751f\u6210\u5bf9\u5e94\u7684txt\u6807\u6ce8\u6587\u4ef6 * \u9a8c\u8bc1\u96c6\u8fd8\u9700\u751f\u6210\u6700\u7ec8\u7684json\u6587\u4ef6</p> <p>\u8be6\u7ec6\u5b9e\u73b0\u53ef\u53c2\u8003convert_shwd2yolo.py\uff0c\u8fd0\u884c\u65b9\u5f0f\u5982\u4e0b\uff1a</p> <p><pre><code>python examples/finetune_SHWD/convert_shwd2yolo.py --root_dir /path_to_shwd/SHWD\n</code></pre> \u8fd0\u884c\u4ee5\u4e0a\u547d\u4ee4\u5c06\u5728\u4e0d\u6539\u53d8\u539f\u6570\u636e\u96c6\u7684\u524d\u63d0\u4e0b\uff0c\u5728\u540c\u7ea7\u76ee\u5f55\u751f\u6210yolo\u683c\u5f0f\u7684SHWD\u6570\u636e\u96c6\u3002</p>"},{"location":"zh/tutorials/finetune/#yaml","title":"\u7f16\u5199yaml\u914d\u7f6e\u6587\u4ef6","text":"<p>\u914d\u7f6e\u6587\u4ef6\u4e3b\u8981\u5305\u542b\u6570\u636e\u96c6\u3001\u6570\u636e\u589e\u5f3a\u3001loss\u3001optimizer\u3001\u6a21\u578b\u7ed3\u6784\u6d89\u53ca\u7684\u76f8\u5e94\u53c2\u6570\uff0c\u7531\u4e8eMindYOLO\u63d0\u4f9byaml\u6587\u4ef6\u7ee7\u627f\u673a\u5236\uff0c\u53ef\u53ea\u5c06\u9700\u8981\u8c03\u6574\u7684\u53c2\u6570\u7f16\u5199\u4e3ayolov7-tiny_shwd.yaml\uff0c\u5e76\u7ee7\u627fMindYOLO\u63d0\u4f9b\u7684\u539f\u751fyaml\u6587\u4ef6\u5373\u53ef\uff0c\u5176\u5185\u5bb9\u5982\u4e0b\uff1a <pre><code>__BASE__: [\n  '../../configs/yolov7/yolov7-tiny.yaml',\n]\n\nper_batch_size: 16 # \u5355\u5361batchsize\uff0c\u603b\u7684batchsize=per_batch_size * device_num\nimg_size: 640 # image sizes\nweight: ./yolov7-tiny_pretrain.ckpt\nstrict_load: False # \u662f\u5426\u6309\u4e25\u683c\u52a0\u8f7dckpt\u5185\u53c2\u6570\uff0c\u9ed8\u8ba4True\uff0c\u82e5\u8bbe\u6210False\uff0c\u5f53\u5206\u7c7b\u6570\u4e0d\u4e00\u81f4\uff0c\u4e22\u6389\u6700\u540e\u4e00\u5c42\u5206\u7c7b\u5668\u7684weight\nlog_interval: 10 # \u6bcflog_interval\u6b21\u8fed\u4ee3\u6253\u5370\u4e00\u6b21loss\u7ed3\u679c\n\ndata:\n  dataset_name: shwd\n  train_set: ./SHWD/train.txt # \u5b9e\u9645\u8bad\u7ec3\u6570\u636e\u8def\u5f84\n  val_set: ./SHWD/val.txt\n  test_set: ./SHWD/val.txt\n  nc: 2 # \u5206\u7c7b\u6570\n  # class names\n  names: [ 'person',  'hat' ] # \u6bcf\u4e00\u7c7b\u7684\u540d\u5b57\n\noptimizer:\n  lr_init: 0.001  # initial learning rate\n</code></pre> * <code>__BASE__</code>\u4e3a\u4e00\u4e2a\u5217\u8868\uff0c\u8868\u793a\u7ee7\u627f\u7684yaml\u6587\u4ef6\u6240\u5728\u8def\u5f84\uff0c\u53ef\u4ee5\u7ee7\u627f\u591a\u4e2ayaml\u6587\u4ef6 * per_batch_size\u548cimg_size\u5206\u522b\u8868\u793a\u5355\u5361\u4e0a\u7684batch_size\u548c\u6570\u636e\u5904\u7406\u56fe\u7247\u91c7\u7528\u7684\u56fe\u7247\u5c3a\u5bf8 * weight\u4e3a\u4e0a\u8ff0\u63d0\u5230\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6587\u4ef6\u8def\u5f84\uff0cstrict_load\u8868\u793a\u4e22\u5f03shape\u4e0d\u4e00\u81f4\u7684\u53c2\u6570 * log_interval\u8868\u793a\u65e5\u5fd7\u6253\u5370\u95f4\u9694 * data\u5b57\u6bb5\u4e0b\u5168\u90e8\u4e3a\u6570\u636e\u96c6\u76f8\u5173\u53c2\u6570\uff0c\u5176\u4e2ddataset_name\u4e3a\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u540d\u79f0\uff0ctrain_set\u3001val_set\u3001test_set\u5206\u522b\u4e3a\u4fdd\u5b58\u8bad\u7ec3\u96c6\u3001\u9a8c\u8bc1\u96c6\u3001\u6d4b\u8bd5\u96c6\u56fe\u7247\u8def\u5f84\u7684txt\u6587\u4ef6\u8def\u5f84\uff0cnc\u4e3a\u7c7b\u522b\u6570\u91cf\uff0cnames\u4e3a\u7c7b\u522b\u540d\u79f0 * optimizer\u5b57\u6bb5\u4e0b\u7684lr_init\u4e3a\u7ecf\u8fc7warm_up\u4e4b\u540e\u7684\u521d\u59cb\u5316\u5b66\u4e60\u7387\uff0c\u6b64\u5904\u76f8\u6bd4\u9ed8\u8ba4\u53c2\u6570\u7f29\u5c0f\u4e8610\u500d</p> <p>\u53c2\u6570\u7ee7\u627f\u5173\u7cfb\u548c\u53c2\u6570\u8bf4\u660e\u53ef\u53c2\u8003configuration\u3002</p>"},{"location":"zh/tutorials/finetune/#_3","title":"\u4e0b\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b","text":"<p>\u53ef\u9009\u7528MindYOLO\u63d0\u4f9b\u7684\u6a21\u578b\u4ed3\u5e93\u4f5c\u4e3a\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u5728COCO\u6570\u636e\u96c6\u4e0a\u5df2\u7ecf\u6709\u8f83\u597d\u7684\u7cbe\u5ea6\u8868\u73b0\uff0c\u76f8\u6bd4\u4ece\u5934\u8bad\u7ec3\uff0c\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\u4e00\u822c\u4f1a\u62e5\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u4ee5\u53ca\u66f4\u9ad8\u7684\u6700\u7ec8\u7cbe\u5ea6\uff0c\u5e76\u4e14\u5927\u6982\u7387\u80fd\u907f\u514d\u521d\u59cb\u5316\u4e0d\u5f53\u5bfc\u81f4\u7684\u68af\u5ea6\u6d88\u5931\u3001\u68af\u5ea6\u7206\u70b8\u7b49\u95ee\u9898\u3002</p> <p>\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u7c7b\u522b\u6570\u901a\u5e38\u4e0eCOCO\u6570\u636e\u96c6\u4e0d\u4e00\u81f4\uff0cMindYOLO\u4e2d\u5404\u6a21\u578b\u7684\u68c0\u6d4b\u5934head\u7ed3\u6784\u8ddf\u6570\u636e\u96c6\u7c7b\u522b\u6570\u6709\u5173\uff0c\u76f4\u63a5\u5c06\u9884\u8bad\u7ec3\u6a21\u578b\u5bfc\u5165\u53ef\u80fd\u4f1a\u56e0\u4e3ashape\u4e0d\u4e00\u81f4\u800c\u5bfc\u5165\u5931\u8d25\uff0c\u53ef\u4ee5\u5728yaml\u914d\u7f6e\u6587\u4ef6\u4e2d\u8bbe\u7f6estrict_load\u53c2\u6570\u4e3aFalse\uff0cMindYOLO\u5c06\u81ea\u52a8\u820d\u5f03shape\u4e0d\u4e00\u81f4\u7684\u53c2\u6570\uff0c\u5e76\u629b\u51fa\u8be5module\u53c2\u6570\u5e76\u672a\u5bfc\u5165\u7684\u544a\u8b66</p>"},{"location":"zh/tutorials/finetune/#finetune_1","title":"\u6a21\u578b\u5fae\u8c03(Finetune)","text":"<p>\u6a21\u578b\u5fae\u8c03\u8fc7\u7a0b\u4e2d\uff0c\u53ef\u9996\u5148\u6309\u7167\u9ed8\u8ba4\u914d\u7f6e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5982\u6548\u679c\u4e0d\u4f73\uff0c\u53ef\u8003\u8651\u8c03\u6574\u4ee5\u4e0b\u53c2\u6570\uff1a * \u5b66\u4e60\u7387\u53ef\u8c03\u5c0f\u4e00\u4e9b\uff0c\u9632\u6b62loss\u96be\u4ee5\u6536\u655b * per_batch_size\u53ef\u6839\u636e\u5b9e\u9645\u663e\u5b58\u5360\u7528\u8c03\u6574\uff0c\u901a\u5e38per_batch_size\u8d8a\u5927\uff0c\u68af\u5ea6\u8ba1\u7b97\u8d8a\u7cbe\u786e * epochs\u53ef\u6839\u636eloss\u662f\u5426\u6536\u655b\u8fdb\u884c\u8c03\u6574 * anchor\u53ef\u6839\u636e\u5b9e\u9645\u7269\u4f53\u5927\u5c0f\u8fdb\u884c\u8c03\u6574</p> <p>\u7531\u4e8eSHWD\u8bad\u7ec3\u96c6\u53ea\u6709\u7ea66000\u5f20\u56fe\u7247\uff0c\u9009\u7528yolov7-tiny\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u3002 * \u5728\u591a\u5361NPU\u4e0a\u8fdb\u884c\u5206\u5e03\u5f0f\u6a21\u578b\u8bad\u7ec3\uff0c\u4ee58\u5361\u4e3a\u4f8b:</p> <pre><code>msrun --worker_num=8 --local_worker_num=8 --bind_core=True --log_dir=./yolov7-tiny_log python train.py --config ./examples/finetune_SHWD/yolov7-tiny_shwd.yaml --is_parallel True\n</code></pre> <ul> <li>\u5728\u5355\u5361NPU/CPU\u4e0a\u8bad\u7ec3\u6a21\u578b\uff1a</li> </ul> <p><pre><code>python train.py --config ./examples/finetune_SHWD/yolov7-tiny_shwd.yaml </code></pre> \u6ce8\u610f\uff1a\u76f4\u63a5\u7528yolov7-tiny\u9ed8\u8ba4\u53c2\u6570\u5728SHWD\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u53ef\u53d6\u5f97AP50 87.0\u7684\u7cbe\u5ea6\u3002\u5c06lr_init\u53c2\u6570\u75310.01\u6539\u4e3a0.001\uff0c\u5373\u53ef\u5b9e\u73b0ap50\u4e3a89.2\u7684\u7cbe\u5ea6\u7ed3\u679c\u3002</p>"},{"location":"zh/tutorials/finetune/#_4","title":"\u53ef\u89c6\u5316\u63a8\u7406","text":"<p>\u4f7f\u7528/demo/predict.py\u5373\u53ef\u7528\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u8fdb\u884c\u53ef\u89c6\u5316\u63a8\u7406\uff0c\u8fd0\u884c\u65b9\u5f0f\u5982\u4e0b\uff1a</p> <p><pre><code>python demo/predict.py --config ./examples/finetune_SHWD/yolov7-tiny_shwd.yaml --weight=/path_to_ckpt/WEIGHT.ckpt --image_path /path_to_image/IMAGE.jpg\n</code></pre> \u63a8\u7406\u6548\u679c\u5982\u4e0b\uff1a</p>"},{"location":"zh/tutorials/modelarts/","title":"MindYOLO ModelArts\u8bad\u7ec3\u5feb\u901f\u5165\u95e8","text":"<p>\u672c\u6587\u4e3b\u8981\u4ecb\u7ecdMindYOLO\u501f\u52a9ModelArts\u5e73\u53f0\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002 ModelArts\u76f8\u5173\u6559\u7a0b\u53c2\u8003\u5e2e\u52a9\u4e2d\u5fc3</p>"},{"location":"zh/tutorials/modelarts/#_1","title":"\u51c6\u5907\u6570\u636e\u53ca\u4ee3\u7801","text":"<p>\u4f7f\u7528OBS\u670d\u52a1\u4e0a\u4f20\u6570\u636e\u96c6\uff0c\u76f8\u5173\u64cd\u4f5c\u6559\u7a0b\u89c1OBS\u7528\u6237\u6307\u5357\uff0c\u83b7\u53d6\u672c\u8d26\u6237\u7684AK\uff0c\u670d\u52a1\u5668\u5730\u5740\u8bf7\u54a8\u8be2\u5bf9\u5e94\u5e73\u53f0\u7ba1\u7406\u5458\u6216\u8d26\u53f7\u8d1f\u8d23\u4eba\uff0c\u5982AK\u4e0d\u5728\u7528\u6237\u6307\u5357\u6307\u5b9a\u4f4d\u7f6e\uff0c\u4e5f\u8bf7\u54a8\u8be2\u5e73\u53f0\u7ba1\u7406\u5458\u6216\u8d26\u53f7\u8d1f\u8d23\u4eba\u3002 \u64cd\u4f5c\uff1a 1. \u767b\u5f55obs browser+  2. \u521b\u5efa\u6876 -&gt; \u65b0\u5efa\u6587\u4ef6\u5939\uff08\u5982\uff1acoco\uff09  3. \u4e0a\u4f20\u6570\u636e\u6587\u4ef6\uff0c\u8bf7\u5c06\u6570\u636e\u6587\u4ef6\u7edf\u4e00\u5355\u72ec\u653e\u7f6e\u5728\u4e00\u4e2a\u6587\u4ef6\u5939\u5185\uff08\u5373\u7528\u4f8b\u4e2d\u7684coco\uff09\uff0c\u4ee3\u7801\u4e2d\u4f1a\u5bf9obs\u6876\u5185\u6570\u636e\u505a\u62f7\u8d1d\uff0c\u62f7\u8d1d\u5185\u5bb9\u4e3a\u6b64\u6587\u4ef6\u5939\uff08\u5982\uff1acoco\uff09\u4e0b\u6240\u6709\u7684\u6587\u4ef6\u3002\u5982\u672a\u65b0\u5efa\u6587\u4ef6\u5939\uff0c\u5c31\u65e0\u6cd5\u9009\u62e9\u5b8c\u6574\u6570\u636e\u96c6\u3002 </p>"},{"location":"zh/tutorials/modelarts/#_2","title":"\u51c6\u5907\u4ee3\u7801","text":"<p>\u540c\u6837\u4f7f\u7528OBS\u670d\u52a1\u4e0a\u4f20\u8bad\u7ec3\u4ee3\u7801\u3002 \u64cd\u4f5c\uff1a\u521b\u5efa\u6876 -&gt; \u65b0\u5efa\u6587\u4ef6\u5939\uff08\u5982\uff1amindyolo\uff09-&gt; \u4e0a\u4f20\u4ee3\u7801\u6587\u4ef6\uff0c\u5728mindyolo\u540c\u5c42\u7ea7\u4e0b\u521b\u5efaoutput\u6587\u4ef6\u5939\u7528\u4e8e\u5b58\u653e\u8bad\u7ec3\u8bb0\u5f55\uff0c\u521b\u5efalog\u6587\u4ef6\u5939\u7528\u4e8e\u5b58\u653e\u65e5\u5fd7\u3002  </p>"},{"location":"zh/tutorials/modelarts/#_3","title":"\u65b0\u5efa\u7b97\u6cd5","text":"<ol> <li>\u5728\u9009\u9879\u5361\u4e2d\u9009\u62e9\u7b97\u6cd5\u7ba1\u7406-&gt;\u521b\u5efa\u3002 </li> <li>\u81ea\u5b9a\u4e49\u7b97\u6cd5\u540d\u79f0\uff0c\u9884\u5236\u6846\u67b6\u9009\u62e9Ascend-Powered-Engine\uff0cmaster\u5206\u652f\u8bf7\u9009\u62e9MindSpore-2.0\u7248\u672c\u955c\u50cf\uff0cr0.1\u5206\u652f\u8bf7\u9009\u62e9MindSpore-1.8.1\u7248\u672c\u955c\u50cf\uff0c\u8bbe\u7f6e\u4ee3\u7801\u76ee\u5f55\u3001\u542f\u52a8\u6587\u4ef6\u3001\u8f93\u5165\u3001\u8f93\u51fa\u4ee5\u53ca\u8d85\u53c2\u3002 </li> </ol> <ul> <li>\u5982\u9700\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u91cd\uff0c\u53ef\u5728\u9009\u62e9\u6a21\u578b\u4e2d\u9009\u62e9\u5df2\u4e0a\u4f20\u7684\u6a21\u578b\u6587\u4ef6\uff0c\u5e76\u5728\u8fd0\u884c\u53c2\u6570\u4e2d\u589e\u52a0ckpt_dir\u53c2\u6570 </li> <li>\u542f\u52a8\u6587\u4ef6\u4e3atrain.py</li> <li>\u8fd0\u884c\u8d85\u53c2\u9700\u6dfb\u52a0enable_modelarts\uff0c\u503c\u4e3aTrue</li> <li>\u8fd0\u884c\u8d85\u53c2config\u8def\u5f84\u53c2\u8003\u8bad\u7ec3\u4f5c\u4e1a\u4e2d\u8fd0\u884c\u73af\u5883\u9884\u89c8\u7684\u76ee\u5f55\uff0c\u5982/home/ma-user/modelarts/user-job-dir/mindyolo/configs/yolov5/yolov5n.yaml</li> <li>\u5982\u6d89\u53ca\u5206\u5e03\u5f0f\u8bad\u7ec3\u573a\u666f\uff0c\u9700\u589e\u52a0\u8d85\u53c2is_parallel\uff0c\u5e76\u5728\u5206\u5e03\u5f0f\u8fd0\u884c\u65f6\u8bbe\u7f6e\u4e3aTrue\uff0c\u5355\u5361\u65f6\u4e3aFalse</li> </ul>"},{"location":"zh/tutorials/modelarts/#_4","title":"\u65b0\u5efa\u4f5c\u4e1a","text":"<ol> <li>\u5728ModelArts\u670d\u52a1\u4e2d\u9009\u62e9\uff1a\u8bad\u7ec3\u7ba1\u7406 -&gt; \u8bad\u7ec3\u4f5c\u4e1a -&gt; \u521b\u5efa\u8bad\u7ec3\u4f5c\u4e1a\uff0c\u8bbe\u7f6e\u4f5c\u4e1a\u540d\u79f0\uff0c\u9009\u62e9\u4e0d\u7eb3\u5165\u5b9e\u9a8c\uff1b\u521b\u5efa\u65b9\u5f0f-&gt;\u6211\u7684\u7b97\u6cd5\u9009\u62e9\u521a\u624d\u65b0\u5efa\u7684\u7b97\u6cd5\uff1b  </li> <li>\u8bad\u7ec3\u8f93\u5165-&gt;\u6570\u636e\u5b58\u50a8\u4f4d\u7f6e\uff0c\u9009\u62e9\u521a\u624d\u521b\u5efa\u7684obs\u6570\u636e\u6876\uff08\u793a\u4f8b\u4e2d\u4e3acoco\uff09\uff0c\u8bad\u7ec3\u8f93\u51fa\u9009\u62e9\u51c6\u5907\u4ee3\u7801\u65f6\u7684output\u6587\u4ef6\u5939\uff0c\u5e76\u6839\u636e\u8fd0\u884c\u73af\u5883\u9884\u89c8\u8bbe\u7f6e\u597dconfig\u8d85\u53c2\u503c; </li> <li>\u9009\u62e9\u8d44\u6e90\u6c60\u3001\u89c4\u683c\u3001\u8ba1\u7b97\u8282\u70b9\uff0c\u4f5c\u4e1a\u65e5\u5fd7\u8def\u5f84\u9009\u62e9\u521b\u5efa\u4ee3\u7801\u65f6\u7684log\u6587\u4ef6\u5939  </li> <li>\u63d0\u4ea4\u8bad\u7ec3\uff0c\u6392\u961f\u540e\u4f1a\u8fdb\u5165\u8fd0\u884c\u4e2d</li> </ol>"},{"location":"zh/tutorials/modelarts/#_5","title":"\u4fee\u6539\u4f5c\u4e1a","text":"<p>\u5728\u8bad\u7ec3\u4f5c\u4e1a\u9875\u9762\u9009\u62e9\u91cd\u5efa\uff0c\u53ef\u4fee\u6539\u9009\u62e9\u7684\u4f5c\u4e1a\u914d\u7f6e</p>"},{"location":"zh/tutorials/quick_start/","title":"\u5feb\u901f\u5f00\u59cb","text":""},{"location":"zh/tutorials/quick_start/#mindyolo","title":"MindYOLO \u5feb\u901f\u5165\u95e8","text":"<p>\u672c\u6587\u7b80\u8981\u4ecb\u7ecdMindYOLO\u4e2d\u5185\u7f6e\u7684\u547d\u4ee4\u884c\u5de5\u5177\u7684\u4f7f\u7528\u65b9\u6cd5\u3002</p>"},{"location":"zh/tutorials/quick_start/#_2","title":"\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u63a8\u7406","text":"<ol> <li>\u4ece\u6a21\u578b\u4ed3\u5e93\u4e2d\u9009\u62e9\u4e00\u4e2a\u6a21\u578b\u53ca\u5176\u914d\u7f6e\u6587\u4ef6\uff0c\u4f8b\u5982\uff0c <code>./configs/yolov7/yolov7.yaml</code>.</li> <li>\u4ece\u6a21\u578b\u4ed3\u5e93\u4e2d\u4e0b\u8f7d\u76f8\u5e94\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u6743\u91cd\u6587\u4ef6\u3002</li> <li>\u4f7f\u7528\u5185\u7f6e\u914d\u7f6e\u8fdb\u884c\u63a8\u7406\uff0c\u8bf7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a</li> </ol> <pre><code># NPU (\u9ed8\u8ba4)\npython demo/predict.py --config ./configs/yolov7/yolov7.yaml --weight=/path_to_ckpt/WEIGHT.ckpt --image_path /path_to_image/IMAGE.jpg\n</code></pre> <p>\u6709\u5173\u547d\u4ee4\u884c\u53c2\u6570\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605<code>demo/predict.py -h</code>\uff0c\u6216\u67e5\u770b\u5176\u6e90\u4ee3\u7801\u3002</p> <ul> <li>\u8981\u5728CPU\u4e0a\u8fd0\u884c\uff0c\u8bf7\u5c06device_target\u7684\u503c\u4fee\u6539\u4e3aCPU.</li> <li>\u7ed3\u679c\u5c06\u4fdd\u5b58\u5728<code>./detect_results</code>\u76ee\u5f55\u4e0b</li> </ul>"},{"location":"zh/tutorials/quick_start/#_3","title":"\u4f7f\u7528\u547d\u4ee4\u884c\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30","text":"<ul> <li>\u6309\u7167YOLO\u683c\u5f0f\u51c6\u5907\u60a8\u7684\u6570\u636e\u96c6\u3002\u5982\u679c\u4f7f\u7528COCO\u6570\u636e\u96c6\uff08YOLO\u683c\u5f0f\uff09\u8fdb\u884c\u8bad\u7ec3\uff0c\u8bf7\u4eceyolov5\u6216darknet\u51c6\u5907\u6570\u636e\u96c6.</li> </ul> <pre><code>  coco/\n    {train,val}2017.txt\n    annotations/\n      instances_{train,val}2017.json\n    images/\n      {train,val}2017/\n          00000001.jpg\n          ...\n          # image files that are mentioned in the corresponding train/val2017.txt\n    labels/\n      {train,val}2017/\n          00000001.txt\n          ...\n          # label files that are mentioned in the corresponding train/val2017.txt\n</code></pre> <ul> <li>\u5728\u591a\u5361NPU\u4e0a\u8fdb\u884c\u5206\u5e03\u5f0f\u6a21\u578b\u8bad\u7ec3\uff0c\u4ee58\u5361\u4e3a\u4f8b:</li> </ul> <pre><code>msrun --worker_num=8 --local_worker_num=8 --bind_core=True --log_dir=./yolov7_log python train.py --config ./configs/yolov7/yolov7.yaml  --is_parallel True\n</code></pre> <ul> <li>\u5728\u5355\u5361NPU/CPU\u4e0a\u8bad\u7ec3\u6a21\u578b\uff1a</li> </ul> <pre><code>python train.py --config ./configs/yolov7/yolov7.yaml </code></pre> <ul> <li>\u5728\u5355\u5361NPU/CPU\u4e0a\u8bc4\u4f30\u6a21\u578b\u7684\u7cbe\u5ea6\uff1a</li> </ul> <p><pre><code>python test.py --config ./configs/yolov7/yolov7.yaml --weight /path_to_ckpt/WEIGHT.ckpt\n</code></pre> * \u5728\u591a\u5361NPU\u4e0a\u8fdb\u884c\u5206\u5e03\u5f0f\u8bc4\u4f30\u6a21\u578b\u7684\u7cbe\u5ea6\uff1a</p> <pre><code>msrun --worker_num=8 --local_worker_num=8 --bind_core=True --log_dir=./yolov7_log python test.py --config ./configs/yolov7/yolov7.yaml --weight /path_to_ckpt/WEIGHT.ckpt --is_parallel True\n</code></pre> <p>\u6ce8\u610f\uff1a</p> <p>(1) \u9ed8\u8ba4\u8d85\u53c2\u4e3a8\u5361\u8bad\u7ec3\uff0c\u5355\u5361\u60c5\u51b5\u9700\u8c03\u6574\u90e8\u5206\u53c2\u6570\u3002</p> <p>(2) \u9ed8\u8ba4\u8bbe\u5907\u4e3aAscend\uff0c\u60a8\u53ef\u4ee5\u6307\u5b9a'device_target'\u7684\u503c\u4e3aAscend/CPU\u3002</p> <p>(3) \u6709\u5173\u66f4\u591a\u9009\u9879\uff0c\u8bf7\u53c2\u9605 <code>train/test.py -h</code>\u3002</p> <p>(4) \u5728CloudBrain\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u8bf7\u5728\u8fd9\u91cc\u67e5\u770b</p>"},{"location":"zh/tutorials/quick_start/#_4","title":"\u90e8\u7f72","text":"<p>\u8bf7\u5728\u90e8\u7f72\u67e5\u770b.</p>"},{"location":"zh/tutorials/quick_start/#mindyolo-api","title":"\u5728\u4ee3\u7801\u4e2d\u4f7f\u7528MindYOLO API","text":"<p>\u656c\u8bf7\u671f\u5f85</p>"}]}